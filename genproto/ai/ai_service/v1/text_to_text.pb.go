// Code generated by protoc-gen-go. DO NOT EDIT.
// versions:
// 	protoc-gen-go v1.36.9
// 	protoc        v6.32.1
// source: malonaz/ai/ai_service/v1/text_to_text.proto

//go:build !protoopaque

package v1

import (
	_ "buf.build/gen/go/bufbuild/protovalidate/protocolbuffers/go/buf/validate"
	v1 "github.com/malonaz/core/genproto/ai/v1"
	_ "google.golang.org/genproto/googleapis/api/annotations"
	protoreflect "google.golang.org/protobuf/reflect/protoreflect"
	protoimpl "google.golang.org/protobuf/runtime/protoimpl"
	reflect "reflect"
	unsafe "unsafe"
)

const (
	// Verify that this generated code is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(20 - protoimpl.MinVersion)
	// Verify that runtime/protoimpl is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(protoimpl.MaxVersion - 20)
)

// Reason why generation stopped.
type TextToTextStopReason int32

const (
	// Used to detect an unset field.
	TextToTextStopReason_TEXT_TO_TEXT_STOP_REASON_UNSPECIFIED TextToTextStopReason = 0
	// The model reached a natural stopping point.
	TextToTextStopReason_TEXT_TO_TEXT_STOP_REASON_END_TURN TextToTextStopReason = 1
	// The model reached the maximum token limit specified in the request.
	TextToTextStopReason_TEXT_TO_TEXT_STOP_REASON_MAX_TOKENS TextToTextStopReason = 2
	// The model requested to call one or more tools.
	TextToTextStopReason_TEXT_TO_TEXT_STOP_REASON_TOOL_CALL TextToTextStopReason = 3
	// one of your provided custom `stop_sequences` was generated
	TextToTextStopReason_TEXT_TO_TEXT_STOP_REASON_STOP_SEQUENCE TextToTextStopReason = 4
	// The model paused its turn to allow the user to interject.
	TextToTextStopReason_TEXT_TO_TEXT_STOP_REASON_PAUSE_TURN TextToTextStopReason = 5
	// The model refused to respond due to safety or policy reasons.
	TextToTextStopReason_TEXT_TO_TEXT_STOP_REASON_REFUSAL TextToTextStopReason = 6
)

// Enum value maps for TextToTextStopReason.
var (
	TextToTextStopReason_name = map[int32]string{
		0: "TEXT_TO_TEXT_STOP_REASON_UNSPECIFIED",
		1: "TEXT_TO_TEXT_STOP_REASON_END_TURN",
		2: "TEXT_TO_TEXT_STOP_REASON_MAX_TOKENS",
		3: "TEXT_TO_TEXT_STOP_REASON_TOOL_CALL",
		4: "TEXT_TO_TEXT_STOP_REASON_STOP_SEQUENCE",
		5: "TEXT_TO_TEXT_STOP_REASON_PAUSE_TURN",
		6: "TEXT_TO_TEXT_STOP_REASON_REFUSAL",
	}
	TextToTextStopReason_value = map[string]int32{
		"TEXT_TO_TEXT_STOP_REASON_UNSPECIFIED":   0,
		"TEXT_TO_TEXT_STOP_REASON_END_TURN":      1,
		"TEXT_TO_TEXT_STOP_REASON_MAX_TOKENS":    2,
		"TEXT_TO_TEXT_STOP_REASON_TOOL_CALL":     3,
		"TEXT_TO_TEXT_STOP_REASON_STOP_SEQUENCE": 4,
		"TEXT_TO_TEXT_STOP_REASON_PAUSE_TURN":    5,
		"TEXT_TO_TEXT_STOP_REASON_REFUSAL":       6,
	}
)

func (x TextToTextStopReason) Enum() *TextToTextStopReason {
	p := new(TextToTextStopReason)
	*p = x
	return p
}

func (x TextToTextStopReason) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (TextToTextStopReason) Descriptor() protoreflect.EnumDescriptor {
	return file_malonaz_ai_ai_service_v1_text_to_text_proto_enumTypes[0].Descriptor()
}

func (TextToTextStopReason) Type() protoreflect.EnumType {
	return &file_malonaz_ai_ai_service_v1_text_to_text_proto_enumTypes[0]
}

func (x TextToTextStopReason) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Configuration for text to text generation.
type TextToTextConfiguration struct {
	state protoimpl.MessageState `protogen:"hybrid.v1"`
	// Maximum number of tokens to generate. Includes reasoning tokens.
	MaxTokens int32 `protobuf:"varint,1,opt,name=max_tokens,json=maxTokens,proto3" json:"max_tokens,omitempty"`
	// Sampling temperature (0.0 to 2.0).
	Temperature float64 `protobuf:"fixed64,2,opt,name=temperature,proto3" json:"temperature,omitempty"`
	// Controls which tool(s) the model should use.
	ToolChoice *v1.ToolChoice `protobuf:"bytes,3,opt,name=tool_choice,json=toolChoice,proto3" json:"tool_choice,omitempty"`
	// Represents the level of reasoning effort for AI model responses.
	// The reasoning effort parameter guides the model on how many reasoning tokens
	// to generate before creating a response to the prompt. Higher effort levels
	// result in more thorough reasoning at the cost of speed and token usage.
	ReasoningEffort v1.ReasoningEffort `protobuf:"varint,4,opt,name=reasoning_effort,json=reasoningEffort,proto3,enum=malonaz.ai.v1.ReasoningEffort" json:"reasoning_effort,omitempty"`
	// If true, we attempt to clean the output to extract a json object.
	// Fails the request if a json object cannot be found.
	ExtractJsonObject bool `protobuf:"varint,5,opt,name=extract_json_object,json=extractJsonObject,proto3" json:"extract_json_object,omitempty"`
	// If true, we stream partial tool calls.
	StreamPartialToolCalls bool `protobuf:"varint,6,opt,name=stream_partial_tool_calls,json=streamPartialToolCalls,proto3" json:"stream_partial_tool_calls,omitempty"`
	// Image generation configuration.
	ImageConfig   *ImageGenerationConfig `protobuf:"bytes,7,opt,name=image_config,json=imageConfig,proto3" json:"image_config,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *TextToTextConfiguration) Reset() {
	*x = TextToTextConfiguration{}
	mi := &file_malonaz_ai_ai_service_v1_text_to_text_proto_msgTypes[0]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TextToTextConfiguration) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TextToTextConfiguration) ProtoMessage() {}

func (x *TextToTextConfiguration) ProtoReflect() protoreflect.Message {
	mi := &file_malonaz_ai_ai_service_v1_text_to_text_proto_msgTypes[0]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

func (x *TextToTextConfiguration) GetMaxTokens() int32 {
	if x != nil {
		return x.MaxTokens
	}
	return 0
}

func (x *TextToTextConfiguration) GetTemperature() float64 {
	if x != nil {
		return x.Temperature
	}
	return 0
}

func (x *TextToTextConfiguration) GetToolChoice() *v1.ToolChoice {
	if x != nil {
		return x.ToolChoice
	}
	return nil
}

func (x *TextToTextConfiguration) GetReasoningEffort() v1.ReasoningEffort {
	if x != nil {
		return x.ReasoningEffort
	}
	return v1.ReasoningEffort(0)
}

func (x *TextToTextConfiguration) GetExtractJsonObject() bool {
	if x != nil {
		return x.ExtractJsonObject
	}
	return false
}

func (x *TextToTextConfiguration) GetStreamPartialToolCalls() bool {
	if x != nil {
		return x.StreamPartialToolCalls
	}
	return false
}

func (x *TextToTextConfiguration) GetImageConfig() *ImageGenerationConfig {
	if x != nil {
		return x.ImageConfig
	}
	return nil
}

func (x *TextToTextConfiguration) SetMaxTokens(v int32) {
	x.MaxTokens = v
}

func (x *TextToTextConfiguration) SetTemperature(v float64) {
	x.Temperature = v
}

func (x *TextToTextConfiguration) SetToolChoice(v *v1.ToolChoice) {
	x.ToolChoice = v
}

func (x *TextToTextConfiguration) SetReasoningEffort(v v1.ReasoningEffort) {
	x.ReasoningEffort = v
}

func (x *TextToTextConfiguration) SetExtractJsonObject(v bool) {
	x.ExtractJsonObject = v
}

func (x *TextToTextConfiguration) SetStreamPartialToolCalls(v bool) {
	x.StreamPartialToolCalls = v
}

func (x *TextToTextConfiguration) SetImageConfig(v *ImageGenerationConfig) {
	x.ImageConfig = v
}

func (x *TextToTextConfiguration) HasToolChoice() bool {
	if x == nil {
		return false
	}
	return x.ToolChoice != nil
}

func (x *TextToTextConfiguration) HasImageConfig() bool {
	if x == nil {
		return false
	}
	return x.ImageConfig != nil
}

func (x *TextToTextConfiguration) ClearToolChoice() {
	x.ToolChoice = nil
}

func (x *TextToTextConfiguration) ClearImageConfig() {
	x.ImageConfig = nil
}

type TextToTextConfiguration_builder struct {
	_ [0]func() // Prevents comparability and use of unkeyed literals for the builder.

	// Maximum number of tokens to generate. Includes reasoning tokens.
	MaxTokens int32
	// Sampling temperature (0.0 to 2.0).
	Temperature float64
	// Controls which tool(s) the model should use.
	ToolChoice *v1.ToolChoice
	// Represents the level of reasoning effort for AI model responses.
	// The reasoning effort parameter guides the model on how many reasoning tokens
	// to generate before creating a response to the prompt. Higher effort levels
	// result in more thorough reasoning at the cost of speed and token usage.
	ReasoningEffort v1.ReasoningEffort
	// If true, we attempt to clean the output to extract a json object.
	// Fails the request if a json object cannot be found.
	ExtractJsonObject bool
	// If true, we stream partial tool calls.
	StreamPartialToolCalls bool
	// Image generation configuration.
	ImageConfig *ImageGenerationConfig
}

func (b0 TextToTextConfiguration_builder) Build() *TextToTextConfiguration {
	m0 := &TextToTextConfiguration{}
	b, x := &b0, m0
	_, _ = b, x
	x.MaxTokens = b.MaxTokens
	x.Temperature = b.Temperature
	x.ToolChoice = b.ToolChoice
	x.ReasoningEffort = b.ReasoningEffort
	x.ExtractJsonObject = b.ExtractJsonObject
	x.StreamPartialToolCalls = b.StreamPartialToolCalls
	x.ImageConfig = b.ImageConfig
	return m0
}

// Configuration for image generation in text-to-text requests.
// Used with models that support native image generation (e.g., Gemini 2.5 Flash Image, Gemini 3 Pro Image).
type ImageGenerationConfig struct {
	state protoimpl.MessageState `protogen:"hybrid.v1"`
	// Aspect ratio of generated images.
	// Defaults to matching input image size, or "1:1" if no input image.
	AspectRatio string `protobuf:"bytes,1,opt,name=aspect_ratio,json=aspectRatio,proto3" json:"aspect_ratio,omitempty"`
	// Resolution of generated images. Only supported by Gemini 3 Pro Image.
	// Defaults to "1K".
	ImageSize     string `protobuf:"bytes,2,opt,name=image_size,json=imageSize,proto3" json:"image_size,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ImageGenerationConfig) Reset() {
	*x = ImageGenerationConfig{}
	mi := &file_malonaz_ai_ai_service_v1_text_to_text_proto_msgTypes[1]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ImageGenerationConfig) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ImageGenerationConfig) ProtoMessage() {}

func (x *ImageGenerationConfig) ProtoReflect() protoreflect.Message {
	mi := &file_malonaz_ai_ai_service_v1_text_to_text_proto_msgTypes[1]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

func (x *ImageGenerationConfig) GetAspectRatio() string {
	if x != nil {
		return x.AspectRatio
	}
	return ""
}

func (x *ImageGenerationConfig) GetImageSize() string {
	if x != nil {
		return x.ImageSize
	}
	return ""
}

func (x *ImageGenerationConfig) SetAspectRatio(v string) {
	x.AspectRatio = v
}

func (x *ImageGenerationConfig) SetImageSize(v string) {
	x.ImageSize = v
}

type ImageGenerationConfig_builder struct {
	_ [0]func() // Prevents comparability and use of unkeyed literals for the builder.

	// Aspect ratio of generated images.
	// Defaults to matching input image size, or "1:1" if no input image.
	AspectRatio string
	// Resolution of generated images. Only supported by Gemini 3 Pro Image.
	// Defaults to "1K".
	ImageSize string
}

func (b0 ImageGenerationConfig_builder) Build() *ImageGenerationConfig {
	m0 := &ImageGenerationConfig{}
	b, x := &b0, m0
	_, _ = b, x
	x.AspectRatio = b.AspectRatio
	x.ImageSize = b.ImageSize
	return m0
}

// Request message for AiService.TextToText.
type TextToTextRequest struct {
	state protoimpl.MessageState `protogen:"hybrid.v1"`
	// The resource name of the model used.
	// Format: providers/{provider}/models/{model}
	Model string `protobuf:"bytes,1,opt,name=model,proto3" json:"model,omitempty"`
	// The conversation messages.
	Messages []*v1.Message `protobuf:"bytes,2,rep,name=messages,proto3" json:"messages,omitempty"`
	// Tools available for the model to call.
	Tools []*v1.Tool `protobuf:"bytes,3,rep,name=tools,proto3" json:"tools,omitempty"`
	// Additional configuration.
	Configuration *TextToTextConfiguration `protobuf:"bytes,4,opt,name=configuration,proto3" json:"configuration,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *TextToTextRequest) Reset() {
	*x = TextToTextRequest{}
	mi := &file_malonaz_ai_ai_service_v1_text_to_text_proto_msgTypes[2]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TextToTextRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TextToTextRequest) ProtoMessage() {}

func (x *TextToTextRequest) ProtoReflect() protoreflect.Message {
	mi := &file_malonaz_ai_ai_service_v1_text_to_text_proto_msgTypes[2]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

func (x *TextToTextRequest) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *TextToTextRequest) GetMessages() []*v1.Message {
	if x != nil {
		return x.Messages
	}
	return nil
}

func (x *TextToTextRequest) GetTools() []*v1.Tool {
	if x != nil {
		return x.Tools
	}
	return nil
}

func (x *TextToTextRequest) GetConfiguration() *TextToTextConfiguration {
	if x != nil {
		return x.Configuration
	}
	return nil
}

func (x *TextToTextRequest) SetModel(v string) {
	x.Model = v
}

func (x *TextToTextRequest) SetMessages(v []*v1.Message) {
	x.Messages = v
}

func (x *TextToTextRequest) SetTools(v []*v1.Tool) {
	x.Tools = v
}

func (x *TextToTextRequest) SetConfiguration(v *TextToTextConfiguration) {
	x.Configuration = v
}

func (x *TextToTextRequest) HasConfiguration() bool {
	if x == nil {
		return false
	}
	return x.Configuration != nil
}

func (x *TextToTextRequest) ClearConfiguration() {
	x.Configuration = nil
}

type TextToTextRequest_builder struct {
	_ [0]func() // Prevents comparability and use of unkeyed literals for the builder.

	// The resource name of the model used.
	// Format: providers/{provider}/models/{model}
	Model string
	// The conversation messages.
	Messages []*v1.Message
	// Tools available for the model to call.
	Tools []*v1.Tool
	// Additional configuration.
	Configuration *TextToTextConfiguration
}

func (b0 TextToTextRequest_builder) Build() *TextToTextRequest {
	m0 := &TextToTextRequest{}
	b, x := &b0, m0
	_, _ = b, x
	x.Model = b.Model
	x.Messages = b.Messages
	x.Tools = b.Tools
	x.Configuration = b.Configuration
	return m0
}

// Response message for AiService.TextToText.
type TextToTextResponse struct {
	state protoimpl.MessageState `protogen:"hybrid.v1"`
	// The generated message.
	Message *v1.Message `protobuf:"bytes,1,opt,name=message,proto3" json:"message,omitempty"`
	// Reason why generation stopped.
	StopReason TextToTextStopReason `protobuf:"varint,2,opt,name=stop_reason,json=stopReason,proto3,enum=malonaz.ai.ai_service.v1.TextToTextStopReason" json:"stop_reason,omitempty"`
	// Model usage metrics.
	ModelUsage *v1.ModelUsage `protobuf:"bytes,3,opt,name=model_usage,json=modelUsage,proto3" json:"model_usage,omitempty"`
	// Generation metrics.
	GenerationMetrics *v1.GenerationMetrics `protobuf:"bytes,4,opt,name=generation_metrics,json=generationMetrics,proto3" json:"generation_metrics,omitempty"`
	unknownFields     protoimpl.UnknownFields
	sizeCache         protoimpl.SizeCache
}

func (x *TextToTextResponse) Reset() {
	*x = TextToTextResponse{}
	mi := &file_malonaz_ai_ai_service_v1_text_to_text_proto_msgTypes[3]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TextToTextResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TextToTextResponse) ProtoMessage() {}

func (x *TextToTextResponse) ProtoReflect() protoreflect.Message {
	mi := &file_malonaz_ai_ai_service_v1_text_to_text_proto_msgTypes[3]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

func (x *TextToTextResponse) GetMessage() *v1.Message {
	if x != nil {
		return x.Message
	}
	return nil
}

func (x *TextToTextResponse) GetStopReason() TextToTextStopReason {
	if x != nil {
		return x.StopReason
	}
	return TextToTextStopReason_TEXT_TO_TEXT_STOP_REASON_UNSPECIFIED
}

func (x *TextToTextResponse) GetModelUsage() *v1.ModelUsage {
	if x != nil {
		return x.ModelUsage
	}
	return nil
}

func (x *TextToTextResponse) GetGenerationMetrics() *v1.GenerationMetrics {
	if x != nil {
		return x.GenerationMetrics
	}
	return nil
}

func (x *TextToTextResponse) SetMessage(v *v1.Message) {
	x.Message = v
}

func (x *TextToTextResponse) SetStopReason(v TextToTextStopReason) {
	x.StopReason = v
}

func (x *TextToTextResponse) SetModelUsage(v *v1.ModelUsage) {
	x.ModelUsage = v
}

func (x *TextToTextResponse) SetGenerationMetrics(v *v1.GenerationMetrics) {
	x.GenerationMetrics = v
}

func (x *TextToTextResponse) HasMessage() bool {
	if x == nil {
		return false
	}
	return x.Message != nil
}

func (x *TextToTextResponse) HasModelUsage() bool {
	if x == nil {
		return false
	}
	return x.ModelUsage != nil
}

func (x *TextToTextResponse) HasGenerationMetrics() bool {
	if x == nil {
		return false
	}
	return x.GenerationMetrics != nil
}

func (x *TextToTextResponse) ClearMessage() {
	x.Message = nil
}

func (x *TextToTextResponse) ClearModelUsage() {
	x.ModelUsage = nil
}

func (x *TextToTextResponse) ClearGenerationMetrics() {
	x.GenerationMetrics = nil
}

type TextToTextResponse_builder struct {
	_ [0]func() // Prevents comparability and use of unkeyed literals for the builder.

	// The generated message.
	Message *v1.Message
	// Reason why generation stopped.
	StopReason TextToTextStopReason
	// Model usage metrics.
	ModelUsage *v1.ModelUsage
	// Generation metrics.
	GenerationMetrics *v1.GenerationMetrics
}

func (b0 TextToTextResponse_builder) Build() *TextToTextResponse {
	m0 := &TextToTextResponse{}
	b, x := &b0, m0
	_, _ = b, x
	x.Message = b.Message
	x.StopReason = b.StopReason
	x.ModelUsage = b.ModelUsage
	x.GenerationMetrics = b.GenerationMetrics
	return m0
}

// Request message for AiService.TextToTextStream.
type TextToTextStreamRequest struct {
	state protoimpl.MessageState `protogen:"hybrid.v1"`
	// The resource name of the model used.
	// Format: providers/{provider}/models/{model}
	Model string `protobuf:"bytes,1,opt,name=model,proto3" json:"model,omitempty"`
	// The conversation messages.
	Messages []*v1.Message `protobuf:"bytes,2,rep,name=messages,proto3" json:"messages,omitempty"`
	// Tools available for the model to call.
	Tools []*v1.Tool `protobuf:"bytes,3,rep,name=tools,proto3" json:"tools,omitempty"`
	// For the model to use a tool.
	ToolChoice string `protobuf:"bytes,4,opt,name=tool_choice,json=toolChoice,proto3" json:"tool_choice,omitempty"`
	// Additional configuration.
	Configuration *TextToTextConfiguration `protobuf:"bytes,5,opt,name=configuration,proto3" json:"configuration,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *TextToTextStreamRequest) Reset() {
	*x = TextToTextStreamRequest{}
	mi := &file_malonaz_ai_ai_service_v1_text_to_text_proto_msgTypes[4]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TextToTextStreamRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TextToTextStreamRequest) ProtoMessage() {}

func (x *TextToTextStreamRequest) ProtoReflect() protoreflect.Message {
	mi := &file_malonaz_ai_ai_service_v1_text_to_text_proto_msgTypes[4]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

func (x *TextToTextStreamRequest) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *TextToTextStreamRequest) GetMessages() []*v1.Message {
	if x != nil {
		return x.Messages
	}
	return nil
}

func (x *TextToTextStreamRequest) GetTools() []*v1.Tool {
	if x != nil {
		return x.Tools
	}
	return nil
}

func (x *TextToTextStreamRequest) GetToolChoice() string {
	if x != nil {
		return x.ToolChoice
	}
	return ""
}

func (x *TextToTextStreamRequest) GetConfiguration() *TextToTextConfiguration {
	if x != nil {
		return x.Configuration
	}
	return nil
}

func (x *TextToTextStreamRequest) SetModel(v string) {
	x.Model = v
}

func (x *TextToTextStreamRequest) SetMessages(v []*v1.Message) {
	x.Messages = v
}

func (x *TextToTextStreamRequest) SetTools(v []*v1.Tool) {
	x.Tools = v
}

func (x *TextToTextStreamRequest) SetToolChoice(v string) {
	x.ToolChoice = v
}

func (x *TextToTextStreamRequest) SetConfiguration(v *TextToTextConfiguration) {
	x.Configuration = v
}

func (x *TextToTextStreamRequest) HasConfiguration() bool {
	if x == nil {
		return false
	}
	return x.Configuration != nil
}

func (x *TextToTextStreamRequest) ClearConfiguration() {
	x.Configuration = nil
}

type TextToTextStreamRequest_builder struct {
	_ [0]func() // Prevents comparability and use of unkeyed literals for the builder.

	// The resource name of the model used.
	// Format: providers/{provider}/models/{model}
	Model string
	// The conversation messages.
	Messages []*v1.Message
	// Tools available for the model to call.
	Tools []*v1.Tool
	// For the model to use a tool.
	ToolChoice string
	// Additional configuration.
	Configuration *TextToTextConfiguration
}

func (b0 TextToTextStreamRequest_builder) Build() *TextToTextStreamRequest {
	m0 := &TextToTextStreamRequest{}
	b, x := &b0, m0
	_, _ = b, x
	x.Model = b.Model
	x.Messages = b.Messages
	x.Tools = b.Tools
	x.ToolChoice = b.ToolChoice
	x.Configuration = b.Configuration
	return m0
}

// Response message for AiService.TextToTextStream.
type TextToTextStreamResponse struct {
	state protoimpl.MessageState `protogen:"hybrid.v1"`
	// Content of this response.
	//
	// Types that are valid to be assigned to Content:
	//
	//	*TextToTextStreamResponse_ContentChunk
	//	*TextToTextStreamResponse_ReasoningChunk
	//	*TextToTextStreamResponse_StopReason
	//	*TextToTextStreamResponse_ToolCall
	//	*TextToTextStreamResponse_PartialToolCall
	//	*TextToTextStreamResponse_ModelUsage
	//	*TextToTextStreamResponse_GenerationMetrics
	//	*TextToTextStreamResponse_Image
	Content       isTextToTextStreamResponse_Content `protobuf_oneof:"content"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *TextToTextStreamResponse) Reset() {
	*x = TextToTextStreamResponse{}
	mi := &file_malonaz_ai_ai_service_v1_text_to_text_proto_msgTypes[5]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TextToTextStreamResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TextToTextStreamResponse) ProtoMessage() {}

func (x *TextToTextStreamResponse) ProtoReflect() protoreflect.Message {
	mi := &file_malonaz_ai_ai_service_v1_text_to_text_proto_msgTypes[5]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

func (x *TextToTextStreamResponse) GetContent() isTextToTextStreamResponse_Content {
	if x != nil {
		return x.Content
	}
	return nil
}

func (x *TextToTextStreamResponse) GetContentChunk() string {
	if x != nil {
		if x, ok := x.Content.(*TextToTextStreamResponse_ContentChunk); ok {
			return x.ContentChunk
		}
	}
	return ""
}

func (x *TextToTextStreamResponse) GetReasoningChunk() string {
	if x != nil {
		if x, ok := x.Content.(*TextToTextStreamResponse_ReasoningChunk); ok {
			return x.ReasoningChunk
		}
	}
	return ""
}

func (x *TextToTextStreamResponse) GetStopReason() TextToTextStopReason {
	if x != nil {
		if x, ok := x.Content.(*TextToTextStreamResponse_StopReason); ok {
			return x.StopReason
		}
	}
	return TextToTextStopReason_TEXT_TO_TEXT_STOP_REASON_UNSPECIFIED
}

func (x *TextToTextStreamResponse) GetToolCall() *v1.ToolCall {
	if x != nil {
		if x, ok := x.Content.(*TextToTextStreamResponse_ToolCall); ok {
			return x.ToolCall
		}
	}
	return nil
}

func (x *TextToTextStreamResponse) GetPartialToolCall() *v1.ToolCall {
	if x != nil {
		if x, ok := x.Content.(*TextToTextStreamResponse_PartialToolCall); ok {
			return x.PartialToolCall
		}
	}
	return nil
}

func (x *TextToTextStreamResponse) GetModelUsage() *v1.ModelUsage {
	if x != nil {
		if x, ok := x.Content.(*TextToTextStreamResponse_ModelUsage); ok {
			return x.ModelUsage
		}
	}
	return nil
}

func (x *TextToTextStreamResponse) GetGenerationMetrics() *v1.GenerationMetrics {
	if x != nil {
		if x, ok := x.Content.(*TextToTextStreamResponse_GenerationMetrics); ok {
			return x.GenerationMetrics
		}
	}
	return nil
}

func (x *TextToTextStreamResponse) GetImage() *v1.Image {
	if x != nil {
		if x, ok := x.Content.(*TextToTextStreamResponse_Image); ok {
			return x.Image
		}
	}
	return nil
}

func (x *TextToTextStreamResponse) SetContentChunk(v string) {
	x.Content = &TextToTextStreamResponse_ContentChunk{v}
}

func (x *TextToTextStreamResponse) SetReasoningChunk(v string) {
	x.Content = &TextToTextStreamResponse_ReasoningChunk{v}
}

func (x *TextToTextStreamResponse) SetStopReason(v TextToTextStopReason) {
	x.Content = &TextToTextStreamResponse_StopReason{v}
}

func (x *TextToTextStreamResponse) SetToolCall(v *v1.ToolCall) {
	if v == nil {
		x.Content = nil
		return
	}
	x.Content = &TextToTextStreamResponse_ToolCall{v}
}

func (x *TextToTextStreamResponse) SetPartialToolCall(v *v1.ToolCall) {
	if v == nil {
		x.Content = nil
		return
	}
	x.Content = &TextToTextStreamResponse_PartialToolCall{v}
}

func (x *TextToTextStreamResponse) SetModelUsage(v *v1.ModelUsage) {
	if v == nil {
		x.Content = nil
		return
	}
	x.Content = &TextToTextStreamResponse_ModelUsage{v}
}

func (x *TextToTextStreamResponse) SetGenerationMetrics(v *v1.GenerationMetrics) {
	if v == nil {
		x.Content = nil
		return
	}
	x.Content = &TextToTextStreamResponse_GenerationMetrics{v}
}

func (x *TextToTextStreamResponse) SetImage(v *v1.Image) {
	if v == nil {
		x.Content = nil
		return
	}
	x.Content = &TextToTextStreamResponse_Image{v}
}

func (x *TextToTextStreamResponse) HasContent() bool {
	if x == nil {
		return false
	}
	return x.Content != nil
}

func (x *TextToTextStreamResponse) HasContentChunk() bool {
	if x == nil {
		return false
	}
	_, ok := x.Content.(*TextToTextStreamResponse_ContentChunk)
	return ok
}

func (x *TextToTextStreamResponse) HasReasoningChunk() bool {
	if x == nil {
		return false
	}
	_, ok := x.Content.(*TextToTextStreamResponse_ReasoningChunk)
	return ok
}

func (x *TextToTextStreamResponse) HasStopReason() bool {
	if x == nil {
		return false
	}
	_, ok := x.Content.(*TextToTextStreamResponse_StopReason)
	return ok
}

func (x *TextToTextStreamResponse) HasToolCall() bool {
	if x == nil {
		return false
	}
	_, ok := x.Content.(*TextToTextStreamResponse_ToolCall)
	return ok
}

func (x *TextToTextStreamResponse) HasPartialToolCall() bool {
	if x == nil {
		return false
	}
	_, ok := x.Content.(*TextToTextStreamResponse_PartialToolCall)
	return ok
}

func (x *TextToTextStreamResponse) HasModelUsage() bool {
	if x == nil {
		return false
	}
	_, ok := x.Content.(*TextToTextStreamResponse_ModelUsage)
	return ok
}

func (x *TextToTextStreamResponse) HasGenerationMetrics() bool {
	if x == nil {
		return false
	}
	_, ok := x.Content.(*TextToTextStreamResponse_GenerationMetrics)
	return ok
}

func (x *TextToTextStreamResponse) HasImage() bool {
	if x == nil {
		return false
	}
	_, ok := x.Content.(*TextToTextStreamResponse_Image)
	return ok
}

func (x *TextToTextStreamResponse) ClearContent() {
	x.Content = nil
}

func (x *TextToTextStreamResponse) ClearContentChunk() {
	if _, ok := x.Content.(*TextToTextStreamResponse_ContentChunk); ok {
		x.Content = nil
	}
}

func (x *TextToTextStreamResponse) ClearReasoningChunk() {
	if _, ok := x.Content.(*TextToTextStreamResponse_ReasoningChunk); ok {
		x.Content = nil
	}
}

func (x *TextToTextStreamResponse) ClearStopReason() {
	if _, ok := x.Content.(*TextToTextStreamResponse_StopReason); ok {
		x.Content = nil
	}
}

func (x *TextToTextStreamResponse) ClearToolCall() {
	if _, ok := x.Content.(*TextToTextStreamResponse_ToolCall); ok {
		x.Content = nil
	}
}

func (x *TextToTextStreamResponse) ClearPartialToolCall() {
	if _, ok := x.Content.(*TextToTextStreamResponse_PartialToolCall); ok {
		x.Content = nil
	}
}

func (x *TextToTextStreamResponse) ClearModelUsage() {
	if _, ok := x.Content.(*TextToTextStreamResponse_ModelUsage); ok {
		x.Content = nil
	}
}

func (x *TextToTextStreamResponse) ClearGenerationMetrics() {
	if _, ok := x.Content.(*TextToTextStreamResponse_GenerationMetrics); ok {
		x.Content = nil
	}
}

func (x *TextToTextStreamResponse) ClearImage() {
	if _, ok := x.Content.(*TextToTextStreamResponse_Image); ok {
		x.Content = nil
	}
}

const TextToTextStreamResponse_Content_not_set_case case_TextToTextStreamResponse_Content = 0
const TextToTextStreamResponse_ContentChunk_case case_TextToTextStreamResponse_Content = 1
const TextToTextStreamResponse_ReasoningChunk_case case_TextToTextStreamResponse_Content = 2
const TextToTextStreamResponse_StopReason_case case_TextToTextStreamResponse_Content = 3
const TextToTextStreamResponse_ToolCall_case case_TextToTextStreamResponse_Content = 4
const TextToTextStreamResponse_PartialToolCall_case case_TextToTextStreamResponse_Content = 5
const TextToTextStreamResponse_ModelUsage_case case_TextToTextStreamResponse_Content = 6
const TextToTextStreamResponse_GenerationMetrics_case case_TextToTextStreamResponse_Content = 7
const TextToTextStreamResponse_Image_case case_TextToTextStreamResponse_Content = 8

func (x *TextToTextStreamResponse) WhichContent() case_TextToTextStreamResponse_Content {
	if x == nil {
		return TextToTextStreamResponse_Content_not_set_case
	}
	switch x.Content.(type) {
	case *TextToTextStreamResponse_ContentChunk:
		return TextToTextStreamResponse_ContentChunk_case
	case *TextToTextStreamResponse_ReasoningChunk:
		return TextToTextStreamResponse_ReasoningChunk_case
	case *TextToTextStreamResponse_StopReason:
		return TextToTextStreamResponse_StopReason_case
	case *TextToTextStreamResponse_ToolCall:
		return TextToTextStreamResponse_ToolCall_case
	case *TextToTextStreamResponse_PartialToolCall:
		return TextToTextStreamResponse_PartialToolCall_case
	case *TextToTextStreamResponse_ModelUsage:
		return TextToTextStreamResponse_ModelUsage_case
	case *TextToTextStreamResponse_GenerationMetrics:
		return TextToTextStreamResponse_GenerationMetrics_case
	case *TextToTextStreamResponse_Image:
		return TextToTextStreamResponse_Image_case
	default:
		return TextToTextStreamResponse_Content_not_set_case
	}
}

type TextToTextStreamResponse_builder struct {
	_ [0]func() // Prevents comparability and use of unkeyed literals for the builder.

	// Content of this response.

	// Fields of oneof Content:
	// A chunk of the generated message content.
	ContentChunk *string
	// Reasoning content chunk (if model supports reasoning).
	ReasoningChunk *string
	// Reason why generation stopped.
	StopReason *TextToTextStopReason
	// Tool calls requested by the assistant (sent when complete).
	ToolCall *v1.ToolCall
	// Tool calls requested by the assistant (sent when complete).
	PartialToolCall *v1.ToolCall
	// Model usage event (sent last).
	ModelUsage *v1.ModelUsage
	// Generation metrics (sent last).
	GenerationMetrics *v1.GenerationMetrics
	// Images generated.
	Image *v1.Image
	// -- end of Content
}

func (b0 TextToTextStreamResponse_builder) Build() *TextToTextStreamResponse {
	m0 := &TextToTextStreamResponse{}
	b, x := &b0, m0
	_, _ = b, x
	if b.ContentChunk != nil {
		x.Content = &TextToTextStreamResponse_ContentChunk{*b.ContentChunk}
	}
	if b.ReasoningChunk != nil {
		x.Content = &TextToTextStreamResponse_ReasoningChunk{*b.ReasoningChunk}
	}
	if b.StopReason != nil {
		x.Content = &TextToTextStreamResponse_StopReason{*b.StopReason}
	}
	if b.ToolCall != nil {
		x.Content = &TextToTextStreamResponse_ToolCall{b.ToolCall}
	}
	if b.PartialToolCall != nil {
		x.Content = &TextToTextStreamResponse_PartialToolCall{b.PartialToolCall}
	}
	if b.ModelUsage != nil {
		x.Content = &TextToTextStreamResponse_ModelUsage{b.ModelUsage}
	}
	if b.GenerationMetrics != nil {
		x.Content = &TextToTextStreamResponse_GenerationMetrics{b.GenerationMetrics}
	}
	if b.Image != nil {
		x.Content = &TextToTextStreamResponse_Image{b.Image}
	}
	return m0
}

type case_TextToTextStreamResponse_Content protoreflect.FieldNumber

func (x case_TextToTextStreamResponse_Content) String() string {
	md := file_malonaz_ai_ai_service_v1_text_to_text_proto_msgTypes[5].Descriptor()
	if x == 0 {
		return "not set"
	}
	return protoimpl.X.MessageFieldStringOf(md, protoreflect.FieldNumber(x))
}

type isTextToTextStreamResponse_Content interface {
	isTextToTextStreamResponse_Content()
}

type TextToTextStreamResponse_ContentChunk struct {
	// A chunk of the generated message content.
	ContentChunk string `protobuf:"bytes,1,opt,name=content_chunk,json=contentChunk,proto3,oneof"`
}

type TextToTextStreamResponse_ReasoningChunk struct {
	// Reasoning content chunk (if model supports reasoning).
	ReasoningChunk string `protobuf:"bytes,2,opt,name=reasoning_chunk,json=reasoningChunk,proto3,oneof"`
}

type TextToTextStreamResponse_StopReason struct {
	// Reason why generation stopped.
	StopReason TextToTextStopReason `protobuf:"varint,3,opt,name=stop_reason,json=stopReason,proto3,enum=malonaz.ai.ai_service.v1.TextToTextStopReason,oneof"`
}

type TextToTextStreamResponse_ToolCall struct {
	// Tool calls requested by the assistant (sent when complete).
	ToolCall *v1.ToolCall `protobuf:"bytes,4,opt,name=tool_call,json=toolCall,proto3,oneof"`
}

type TextToTextStreamResponse_PartialToolCall struct {
	// Tool calls requested by the assistant (sent when complete).
	PartialToolCall *v1.ToolCall `protobuf:"bytes,5,opt,name=partial_tool_call,json=partialToolCall,proto3,oneof"`
}

type TextToTextStreamResponse_ModelUsage struct {
	// Model usage event (sent last).
	ModelUsage *v1.ModelUsage `protobuf:"bytes,6,opt,name=model_usage,json=modelUsage,proto3,oneof"`
}

type TextToTextStreamResponse_GenerationMetrics struct {
	// Generation metrics (sent last).
	GenerationMetrics *v1.GenerationMetrics `protobuf:"bytes,7,opt,name=generation_metrics,json=generationMetrics,proto3,oneof"`
}

type TextToTextStreamResponse_Image struct {
	// Images generated.
	Image *v1.Image `protobuf:"bytes,8,opt,name=image,proto3,oneof"`
}

func (*TextToTextStreamResponse_ContentChunk) isTextToTextStreamResponse_Content() {}

func (*TextToTextStreamResponse_ReasoningChunk) isTextToTextStreamResponse_Content() {}

func (*TextToTextStreamResponse_StopReason) isTextToTextStreamResponse_Content() {}

func (*TextToTextStreamResponse_ToolCall) isTextToTextStreamResponse_Content() {}

func (*TextToTextStreamResponse_PartialToolCall) isTextToTextStreamResponse_Content() {}

func (*TextToTextStreamResponse_ModelUsage) isTextToTextStreamResponse_Content() {}

func (*TextToTextStreamResponse_GenerationMetrics) isTextToTextStreamResponse_Content() {}

func (*TextToTextStreamResponse_Image) isTextToTextStreamResponse_Content() {}

var File_malonaz_ai_ai_service_v1_text_to_text_proto protoreflect.FileDescriptor

const file_malonaz_ai_ai_service_v1_text_to_text_proto_rawDesc = "" +
	"\n" +
	"+malonaz/ai/ai_service/v1/text_to_text.proto\x12\x18malonaz.ai.ai_service.v1\x1a\x1bbuf/validate/validate.proto\x1a\x19google/api/resource.proto\x1a\x1bmalonaz/ai/v1/message.proto\x1a\x1bmalonaz/ai/v1/metrics.proto\x1a\x18malonaz/ai/v1/tool.proto\"\xc2\x03\n" +
	"\x17TextToTextConfiguration\x12&\n" +
	"\n" +
	"max_tokens\x18\x01 \x01(\x05B\a\xbaH\x04\x1a\x02(\x00R\tmaxTokens\x129\n" +
	"\vtemperature\x18\x02 \x01(\x01B\x17\xbaH\x14\x12\x12\x19\x00\x00\x00\x00\x00\x00\x00@)\x00\x00\x00\x00\x00\x00\x00\x00R\vtemperature\x12:\n" +
	"\vtool_choice\x18\x03 \x01(\v2\x19.malonaz.ai.v1.ToolChoiceR\n" +
	"toolChoice\x12I\n" +
	"\x10reasoning_effort\x18\x04 \x01(\x0e2\x1e.malonaz.ai.v1.ReasoningEffortR\x0freasoningEffort\x12.\n" +
	"\x13extract_json_object\x18\x05 \x01(\bR\x11extractJsonObject\x129\n" +
	"\x19stream_partial_tool_calls\x18\x06 \x01(\bR\x16streamPartialToolCalls\x12R\n" +
	"\fimage_config\x18\a \x01(\v2/.malonaz.ai.ai_service.v1.ImageGenerationConfigR\vimageConfig\"\xac\x01\n" +
	"\x15ImageGenerationConfig\x12_\n" +
	"\faspect_ratio\x18\x01 \x01(\tB<\xbaH9r7R\x00R\x031:1R\x032:3R\x033:2R\x033:4R\x034:3R\x034:5R\x035:4R\x049:16R\x0416:9R\x0421:9R\vaspectRatio\x122\n" +
	"\n" +
	"image_size\x18\x02 \x01(\tB\x13\xbaH\x10r\x0eR\x00R\x021KR\x022KR\x024KR\timageSize\"\x8c\x02\n" +
	"\x11TextToTextRequest\x125\n" +
	"\x05model\x18\x01 \x01(\tB\x1f\xfaA\x16\n" +
	"\x14ai.malonaz.com/Model\xbaH\x03\xc8\x01\x01R\x05model\x12<\n" +
	"\bmessages\x18\x02 \x03(\v2\x16.malonaz.ai.v1.MessageB\b\xbaH\x05\x92\x01\x02\b\x01R\bmessages\x12)\n" +
	"\x05tools\x18\x03 \x03(\v2\x13.malonaz.ai.v1.ToolR\x05tools\x12W\n" +
	"\rconfiguration\x18\x04 \x01(\v21.malonaz.ai.ai_service.v1.TextToTextConfigurationR\rconfiguration\"\xa4\x02\n" +
	"\x12TextToTextResponse\x120\n" +
	"\amessage\x18\x01 \x01(\v2\x16.malonaz.ai.v1.MessageR\amessage\x12O\n" +
	"\vstop_reason\x18\x02 \x01(\x0e2..malonaz.ai.ai_service.v1.TextToTextStopReasonR\n" +
	"stopReason\x12:\n" +
	"\vmodel_usage\x18\x03 \x01(\v2\x19.malonaz.ai.v1.ModelUsageR\n" +
	"modelUsage\x12O\n" +
	"\x12generation_metrics\x18\x04 \x01(\v2 .malonaz.ai.v1.GenerationMetricsR\x11generationMetrics\"\xb3\x02\n" +
	"\x17TextToTextStreamRequest\x125\n" +
	"\x05model\x18\x01 \x01(\tB\x1f\xfaA\x16\n" +
	"\x14ai.malonaz.com/Model\xbaH\x03\xc8\x01\x01R\x05model\x12<\n" +
	"\bmessages\x18\x02 \x03(\v2\x16.malonaz.ai.v1.MessageB\b\xbaH\x05\x92\x01\x02\b\x01R\bmessages\x12)\n" +
	"\x05tools\x18\x03 \x03(\v2\x13.malonaz.ai.v1.ToolR\x05tools\x12\x1f\n" +
	"\vtool_choice\x18\x04 \x01(\tR\n" +
	"toolChoice\x12W\n" +
	"\rconfiguration\x18\x05 \x01(\v21.malonaz.ai.ai_service.v1.TextToTextConfigurationR\rconfiguration\"\x8f\x04\n" +
	"\x18TextToTextStreamResponse\x12%\n" +
	"\rcontent_chunk\x18\x01 \x01(\tH\x00R\fcontentChunk\x12)\n" +
	"\x0freasoning_chunk\x18\x02 \x01(\tH\x00R\x0ereasoningChunk\x12Q\n" +
	"\vstop_reason\x18\x03 \x01(\x0e2..malonaz.ai.ai_service.v1.TextToTextStopReasonH\x00R\n" +
	"stopReason\x126\n" +
	"\ttool_call\x18\x04 \x01(\v2\x17.malonaz.ai.v1.ToolCallH\x00R\btoolCall\x12E\n" +
	"\x11partial_tool_call\x18\x05 \x01(\v2\x17.malonaz.ai.v1.ToolCallH\x00R\x0fpartialToolCall\x12<\n" +
	"\vmodel_usage\x18\x06 \x01(\v2\x19.malonaz.ai.v1.ModelUsageH\x00R\n" +
	"modelUsage\x12Q\n" +
	"\x12generation_metrics\x18\a \x01(\v2 .malonaz.ai.v1.GenerationMetricsH\x00R\x11generationMetrics\x12,\n" +
	"\x05image\x18\b \x01(\v2\x14.malonaz.ai.v1.ImageH\x00R\x05imageB\x10\n" +
	"\acontent\x12\x05\xbaH\x02\b\x01*\xb3\x02\n" +
	"\x14TextToTextStopReason\x12(\n" +
	"$TEXT_TO_TEXT_STOP_REASON_UNSPECIFIED\x10\x00\x12%\n" +
	"!TEXT_TO_TEXT_STOP_REASON_END_TURN\x10\x01\x12'\n" +
	"#TEXT_TO_TEXT_STOP_REASON_MAX_TOKENS\x10\x02\x12&\n" +
	"\"TEXT_TO_TEXT_STOP_REASON_TOOL_CALL\x10\x03\x12*\n" +
	"&TEXT_TO_TEXT_STOP_REASON_STOP_SEQUENCE\x10\x04\x12'\n" +
	"#TEXT_TO_TEXT_STOP_REASON_PAUSE_TURN\x10\x05\x12$\n" +
	" TEXT_TO_TEXT_STOP_REASON_REFUSAL\x10\x06B3Z1github.com/malonaz/core/genproto/ai/ai_service/v1b\x06proto3"

var file_malonaz_ai_ai_service_v1_text_to_text_proto_enumTypes = make([]protoimpl.EnumInfo, 1)
var file_malonaz_ai_ai_service_v1_text_to_text_proto_msgTypes = make([]protoimpl.MessageInfo, 6)
var file_malonaz_ai_ai_service_v1_text_to_text_proto_goTypes = []any{
	(TextToTextStopReason)(0),        // 0: malonaz.ai.ai_service.v1.TextToTextStopReason
	(*TextToTextConfiguration)(nil),  // 1: malonaz.ai.ai_service.v1.TextToTextConfiguration
	(*ImageGenerationConfig)(nil),    // 2: malonaz.ai.ai_service.v1.ImageGenerationConfig
	(*TextToTextRequest)(nil),        // 3: malonaz.ai.ai_service.v1.TextToTextRequest
	(*TextToTextResponse)(nil),       // 4: malonaz.ai.ai_service.v1.TextToTextResponse
	(*TextToTextStreamRequest)(nil),  // 5: malonaz.ai.ai_service.v1.TextToTextStreamRequest
	(*TextToTextStreamResponse)(nil), // 6: malonaz.ai.ai_service.v1.TextToTextStreamResponse
	(*v1.ToolChoice)(nil),            // 7: malonaz.ai.v1.ToolChoice
	(v1.ReasoningEffort)(0),          // 8: malonaz.ai.v1.ReasoningEffort
	(*v1.Message)(nil),               // 9: malonaz.ai.v1.Message
	(*v1.Tool)(nil),                  // 10: malonaz.ai.v1.Tool
	(*v1.ModelUsage)(nil),            // 11: malonaz.ai.v1.ModelUsage
	(*v1.GenerationMetrics)(nil),     // 12: malonaz.ai.v1.GenerationMetrics
	(*v1.ToolCall)(nil),              // 13: malonaz.ai.v1.ToolCall
	(*v1.Image)(nil),                 // 14: malonaz.ai.v1.Image
}
var file_malonaz_ai_ai_service_v1_text_to_text_proto_depIdxs = []int32{
	7,  // 0: malonaz.ai.ai_service.v1.TextToTextConfiguration.tool_choice:type_name -> malonaz.ai.v1.ToolChoice
	8,  // 1: malonaz.ai.ai_service.v1.TextToTextConfiguration.reasoning_effort:type_name -> malonaz.ai.v1.ReasoningEffort
	2,  // 2: malonaz.ai.ai_service.v1.TextToTextConfiguration.image_config:type_name -> malonaz.ai.ai_service.v1.ImageGenerationConfig
	9,  // 3: malonaz.ai.ai_service.v1.TextToTextRequest.messages:type_name -> malonaz.ai.v1.Message
	10, // 4: malonaz.ai.ai_service.v1.TextToTextRequest.tools:type_name -> malonaz.ai.v1.Tool
	1,  // 5: malonaz.ai.ai_service.v1.TextToTextRequest.configuration:type_name -> malonaz.ai.ai_service.v1.TextToTextConfiguration
	9,  // 6: malonaz.ai.ai_service.v1.TextToTextResponse.message:type_name -> malonaz.ai.v1.Message
	0,  // 7: malonaz.ai.ai_service.v1.TextToTextResponse.stop_reason:type_name -> malonaz.ai.ai_service.v1.TextToTextStopReason
	11, // 8: malonaz.ai.ai_service.v1.TextToTextResponse.model_usage:type_name -> malonaz.ai.v1.ModelUsage
	12, // 9: malonaz.ai.ai_service.v1.TextToTextResponse.generation_metrics:type_name -> malonaz.ai.v1.GenerationMetrics
	9,  // 10: malonaz.ai.ai_service.v1.TextToTextStreamRequest.messages:type_name -> malonaz.ai.v1.Message
	10, // 11: malonaz.ai.ai_service.v1.TextToTextStreamRequest.tools:type_name -> malonaz.ai.v1.Tool
	1,  // 12: malonaz.ai.ai_service.v1.TextToTextStreamRequest.configuration:type_name -> malonaz.ai.ai_service.v1.TextToTextConfiguration
	0,  // 13: malonaz.ai.ai_service.v1.TextToTextStreamResponse.stop_reason:type_name -> malonaz.ai.ai_service.v1.TextToTextStopReason
	13, // 14: malonaz.ai.ai_service.v1.TextToTextStreamResponse.tool_call:type_name -> malonaz.ai.v1.ToolCall
	13, // 15: malonaz.ai.ai_service.v1.TextToTextStreamResponse.partial_tool_call:type_name -> malonaz.ai.v1.ToolCall
	11, // 16: malonaz.ai.ai_service.v1.TextToTextStreamResponse.model_usage:type_name -> malonaz.ai.v1.ModelUsage
	12, // 17: malonaz.ai.ai_service.v1.TextToTextStreamResponse.generation_metrics:type_name -> malonaz.ai.v1.GenerationMetrics
	14, // 18: malonaz.ai.ai_service.v1.TextToTextStreamResponse.image:type_name -> malonaz.ai.v1.Image
	19, // [19:19] is the sub-list for method output_type
	19, // [19:19] is the sub-list for method input_type
	19, // [19:19] is the sub-list for extension type_name
	19, // [19:19] is the sub-list for extension extendee
	0,  // [0:19] is the sub-list for field type_name
}

func init() { file_malonaz_ai_ai_service_v1_text_to_text_proto_init() }
func file_malonaz_ai_ai_service_v1_text_to_text_proto_init() {
	if File_malonaz_ai_ai_service_v1_text_to_text_proto != nil {
		return
	}
	file_malonaz_ai_ai_service_v1_text_to_text_proto_msgTypes[5].OneofWrappers = []any{
		(*TextToTextStreamResponse_ContentChunk)(nil),
		(*TextToTextStreamResponse_ReasoningChunk)(nil),
		(*TextToTextStreamResponse_StopReason)(nil),
		(*TextToTextStreamResponse_ToolCall)(nil),
		(*TextToTextStreamResponse_PartialToolCall)(nil),
		(*TextToTextStreamResponse_ModelUsage)(nil),
		(*TextToTextStreamResponse_GenerationMetrics)(nil),
		(*TextToTextStreamResponse_Image)(nil),
	}
	type x struct{}
	out := protoimpl.TypeBuilder{
		File: protoimpl.DescBuilder{
			GoPackagePath: reflect.TypeOf(x{}).PkgPath(),
			RawDescriptor: unsafe.Slice(unsafe.StringData(file_malonaz_ai_ai_service_v1_text_to_text_proto_rawDesc), len(file_malonaz_ai_ai_service_v1_text_to_text_proto_rawDesc)),
			NumEnums:      1,
			NumMessages:   6,
			NumExtensions: 0,
			NumServices:   0,
		},
		GoTypes:           file_malonaz_ai_ai_service_v1_text_to_text_proto_goTypes,
		DependencyIndexes: file_malonaz_ai_ai_service_v1_text_to_text_proto_depIdxs,
		EnumInfos:         file_malonaz_ai_ai_service_v1_text_to_text_proto_enumTypes,
		MessageInfos:      file_malonaz_ai_ai_service_v1_text_to_text_proto_msgTypes,
	}.Build()
	File_malonaz_ai_ai_service_v1_text_to_text_proto = out.File
	file_malonaz_ai_ai_service_v1_text_to_text_proto_goTypes = nil
	file_malonaz_ai_ai_service_v1_text_to_text_proto_depIdxs = nil
}
