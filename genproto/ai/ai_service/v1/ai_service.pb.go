// Code generated by protoc-gen-go. DO NOT EDIT.
// versions:
// 	protoc-gen-go v1.36.9
// 	protoc        v6.30.0
// source: malonaz/ai/ai_service/v1/ai_service.proto

package v1

import (
	_ "buf.build/gen/go/bufbuild/protovalidate/protocolbuffers/go/buf/validate"
	v1 "github.com/malonaz/core/genproto/ai/v1"
	v11 "github.com/malonaz/core/genproto/audio/v1"
	_ "github.com/malonaz/core/genproto/codegen/aip/v1"
	_ "google.golang.org/genproto/googleapis/api/annotations"
	protoreflect "google.golang.org/protobuf/reflect/protoreflect"
	protoimpl "google.golang.org/protobuf/runtime/protoimpl"
	reflect "reflect"
	sync "sync"
	unsafe "unsafe"
)

const (
	// Verify that this generated code is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(20 - protoimpl.MinVersion)
	// Verify that runtime/protoimpl is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(protoimpl.MaxVersion - 20)
)

// Reason why generation stopped.
type TextToTextStopReason int32

const (
	// Used to detect an unset field.
	TextToTextStopReason_TEXT_TO_TEXT_STOP_REASON_UNSPECIFIED TextToTextStopReason = 0
	// The model reached a natural stopping point.
	TextToTextStopReason_TEXT_TO_TEXT_STOP_REASON_END_TURN TextToTextStopReason = 1
	// The model reached the maximum token limit specified in the request.
	TextToTextStopReason_TEXT_TO_TEXT_STOP_REASON_MAX_TOKENS TextToTextStopReason = 2
	// The model requested to call one or more tools.
	TextToTextStopReason_TEXT_TO_TEXT_STOP_REASON_TOOL_CALL TextToTextStopReason = 3
	// one of your provided custom `stop_sequences` was generated
	TextToTextStopReason_TEXT_TO_TEXT_STOP_REASON_STOP_SEQUENCE TextToTextStopReason = 4
	// The model paused its turn to allow the user to interject.
	TextToTextStopReason_TEXT_TO_TEXT_STOP_REASON_PAUSE_TURN TextToTextStopReason = 5
	// The model refused to respond due to safety or policy reasons.
	TextToTextStopReason_TEXT_TO_TEXT_STOP_REASON_REFUSAL TextToTextStopReason = 6
)

// Enum value maps for TextToTextStopReason.
var (
	TextToTextStopReason_name = map[int32]string{
		0: "TEXT_TO_TEXT_STOP_REASON_UNSPECIFIED",
		1: "TEXT_TO_TEXT_STOP_REASON_END_TURN",
		2: "TEXT_TO_TEXT_STOP_REASON_MAX_TOKENS",
		3: "TEXT_TO_TEXT_STOP_REASON_TOOL_CALL",
		4: "TEXT_TO_TEXT_STOP_REASON_STOP_SEQUENCE",
		5: "TEXT_TO_TEXT_STOP_REASON_PAUSE_TURN",
		6: "TEXT_TO_TEXT_STOP_REASON_REFUSAL",
	}
	TextToTextStopReason_value = map[string]int32{
		"TEXT_TO_TEXT_STOP_REASON_UNSPECIFIED":   0,
		"TEXT_TO_TEXT_STOP_REASON_END_TURN":      1,
		"TEXT_TO_TEXT_STOP_REASON_MAX_TOKENS":    2,
		"TEXT_TO_TEXT_STOP_REASON_TOOL_CALL":     3,
		"TEXT_TO_TEXT_STOP_REASON_STOP_SEQUENCE": 4,
		"TEXT_TO_TEXT_STOP_REASON_PAUSE_TURN":    5,
		"TEXT_TO_TEXT_STOP_REASON_REFUSAL":       6,
	}
)

func (x TextToTextStopReason) Enum() *TextToTextStopReason {
	p := new(TextToTextStopReason)
	*p = x
	return p
}

func (x TextToTextStopReason) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (TextToTextStopReason) Descriptor() protoreflect.EnumDescriptor {
	return file_malonaz_ai_ai_service_v1_ai_service_proto_enumTypes[0].Descriptor()
}

func (TextToTextStopReason) Type() protoreflect.EnumType {
	return &file_malonaz_ai_ai_service_v1_ai_service_proto_enumTypes[0]
}

func (x TextToTextStopReason) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use TextToTextStopReason.Descriptor instead.
func (TextToTextStopReason) EnumDescriptor() ([]byte, []int) {
	return file_malonaz_ai_ai_service_v1_ai_service_proto_rawDescGZIP(), []int{0}
}

// Request message for Ai.CreateModel.
type CreateModelRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The resource name of the parent provider for which this model will be created.
	// Format: providers/{provider}
	Parent string `protobuf:"bytes,1,opt,name=parent,proto3" json:"parent,omitempty"`
	// The model to create.
	Model *v1.Model `protobuf:"bytes,2,opt,name=model,proto3" json:"model,omitempty"`
	// The ID to use for the resource, which will become the final component of
	// the resource name.
	//
	// This value should be 4-63 characters, and valid characters
	// are /[a-z][0-9]-/.
	ModelId       string `protobuf:"bytes,3,opt,name=model_id,json=modelId,proto3" json:"model_id,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *CreateModelRequest) Reset() {
	*x = CreateModelRequest{}
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[0]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *CreateModelRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*CreateModelRequest) ProtoMessage() {}

func (x *CreateModelRequest) ProtoReflect() protoreflect.Message {
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[0]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use CreateModelRequest.ProtoReflect.Descriptor instead.
func (*CreateModelRequest) Descriptor() ([]byte, []int) {
	return file_malonaz_ai_ai_service_v1_ai_service_proto_rawDescGZIP(), []int{0}
}

func (x *CreateModelRequest) GetParent() string {
	if x != nil {
		return x.Parent
	}
	return ""
}

func (x *CreateModelRequest) GetModel() *v1.Model {
	if x != nil {
		return x.Model
	}
	return nil
}

func (x *CreateModelRequest) GetModelId() string {
	if x != nil {
		return x.ModelId
	}
	return ""
}

// Request message for Ai.GetModel.
type GetModelRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The resource name of the model to retrieve.
	// Format: providers/{provider}/models/{model}
	Name          string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *GetModelRequest) Reset() {
	*x = GetModelRequest{}
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[1]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GetModelRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GetModelRequest) ProtoMessage() {}

func (x *GetModelRequest) ProtoReflect() protoreflect.Message {
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[1]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use GetModelRequest.ProtoReflect.Descriptor instead.
func (*GetModelRequest) Descriptor() ([]byte, []int) {
	return file_malonaz_ai_ai_service_v1_ai_service_proto_rawDescGZIP(), []int{1}
}

func (x *GetModelRequest) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

// Request message for Ai.ListModels.
type ListModelsRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The resource name of the parent, which owns this collection of models.
	// Format: providers/{provider}
	Parent string `protobuf:"bytes,1,opt,name=parent,proto3" json:"parent,omitempty"`
	// Requested page size. Server may return fewer models than requested.
	// If unspecified, server will pick an appropriate default.
	PageSize int32 `protobuf:"varint,2,opt,name=page_size,json=pageSize,proto3" json:"page_size,omitempty"`
	// A token identifying a page of results the server should return.
	// This is the value of
	// [ListModelsResponse.next_page_token][ai.ai_service.v1.ListModelsResponse.next_page_token]
	// returned from the previous call to `ListModels` method.
	PageToken     string `protobuf:"bytes,3,opt,name=page_token,json=pageToken,proto3" json:"page_token,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ListModelsRequest) Reset() {
	*x = ListModelsRequest{}
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[2]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ListModelsRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ListModelsRequest) ProtoMessage() {}

func (x *ListModelsRequest) ProtoReflect() protoreflect.Message {
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[2]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ListModelsRequest.ProtoReflect.Descriptor instead.
func (*ListModelsRequest) Descriptor() ([]byte, []int) {
	return file_malonaz_ai_ai_service_v1_ai_service_proto_rawDescGZIP(), []int{2}
}

func (x *ListModelsRequest) GetParent() string {
	if x != nil {
		return x.Parent
	}
	return ""
}

func (x *ListModelsRequest) GetPageSize() int32 {
	if x != nil {
		return x.PageSize
	}
	return 0
}

func (x *ListModelsRequest) GetPageToken() string {
	if x != nil {
		return x.PageToken
	}
	return ""
}

// Response message for Ai.ListModels.
type ListModelsResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The list of models.
	Models []*v1.Model `protobuf:"bytes,1,rep,name=models,proto3" json:"models,omitempty"`
	// A token to retrieve next page of results. Pass this value in the
	// [ListModelsRequest.page_token][ai.ai_service.v1.ListModelsRequest.page_token]
	// field in the subsequent call to `ListModels` method to retrieve the next
	// page of results.
	NextPageToken string `protobuf:"bytes,2,opt,name=next_page_token,json=nextPageToken,proto3" json:"next_page_token,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ListModelsResponse) Reset() {
	*x = ListModelsResponse{}
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[3]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ListModelsResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ListModelsResponse) ProtoMessage() {}

func (x *ListModelsResponse) ProtoReflect() protoreflect.Message {
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[3]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ListModelsResponse.ProtoReflect.Descriptor instead.
func (*ListModelsResponse) Descriptor() ([]byte, []int) {
	return file_malonaz_ai_ai_service_v1_ai_service_proto_rawDescGZIP(), []int{3}
}

func (x *ListModelsResponse) GetModels() []*v1.Model {
	if x != nil {
		return x.Models
	}
	return nil
}

func (x *ListModelsResponse) GetNextPageToken() string {
	if x != nil {
		return x.NextPageToken
	}
	return ""
}

// Request message for Ai.SpeechToText.
type SpeechToTextRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The resource name of the model used.
	// Format: providers/{provider}/models/{model}
	Model string `protobuf:"bytes,1,opt,name=model,proto3" json:"model,omitempty"`
	// Audio data in PCM format.
	Audio []byte `protobuf:"bytes,2,opt,name=audio,proto3" json:"audio,omitempty"`
	// Optional language code to improve transcription accuracy (e.g., "en", "es").
	Language      string `protobuf:"bytes,3,opt,name=language,proto3" json:"language,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *SpeechToTextRequest) Reset() {
	*x = SpeechToTextRequest{}
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[4]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *SpeechToTextRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*SpeechToTextRequest) ProtoMessage() {}

func (x *SpeechToTextRequest) ProtoReflect() protoreflect.Message {
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[4]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use SpeechToTextRequest.ProtoReflect.Descriptor instead.
func (*SpeechToTextRequest) Descriptor() ([]byte, []int) {
	return file_malonaz_ai_ai_service_v1_ai_service_proto_rawDescGZIP(), []int{4}
}

func (x *SpeechToTextRequest) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *SpeechToTextRequest) GetAudio() []byte {
	if x != nil {
		return x.Audio
	}
	return nil
}

func (x *SpeechToTextRequest) GetLanguage() string {
	if x != nil {
		return x.Language
	}
	return ""
}

// Response message for Ai.SpeechToText.
type SpeechToTextResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The transcribed text.
	Transcript string `protobuf:"bytes,1,opt,name=transcript,proto3" json:"transcript,omitempty"`
	// Model usage metrics.
	ModelUsage *v1.ModelUsage `protobuf:"bytes,2,opt,name=model_usage,json=modelUsage,proto3" json:"model_usage,omitempty"`
	// Generation metrics.
	GenerationMetrics *v1.GenerationMetrics `protobuf:"bytes,3,opt,name=generation_metrics,json=generationMetrics,proto3" json:"generation_metrics,omitempty"`
	unknownFields     protoimpl.UnknownFields
	sizeCache         protoimpl.SizeCache
}

func (x *SpeechToTextResponse) Reset() {
	*x = SpeechToTextResponse{}
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[5]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *SpeechToTextResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*SpeechToTextResponse) ProtoMessage() {}

func (x *SpeechToTextResponse) ProtoReflect() protoreflect.Message {
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[5]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use SpeechToTextResponse.ProtoReflect.Descriptor instead.
func (*SpeechToTextResponse) Descriptor() ([]byte, []int) {
	return file_malonaz_ai_ai_service_v1_ai_service_proto_rawDescGZIP(), []int{5}
}

func (x *SpeechToTextResponse) GetTranscript() string {
	if x != nil {
		return x.Transcript
	}
	return ""
}

func (x *SpeechToTextResponse) GetModelUsage() *v1.ModelUsage {
	if x != nil {
		return x.ModelUsage
	}
	return nil
}

func (x *SpeechToTextResponse) GetGenerationMetrics() *v1.GenerationMetrics {
	if x != nil {
		return x.GenerationMetrics
	}
	return nil
}

// Configuration for text to text generation.
type TextToTextConfiguration struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Maximum number of tokens to generate. Includes reasoning tokens.
	MaxTokens int32 `protobuf:"varint,1,opt,name=max_tokens,json=maxTokens,proto3" json:"max_tokens,omitempty"`
	// Sampling temperature (0.0 to 2.0).
	Temperature float64 `protobuf:"fixed64,2,opt,name=temperature,proto3" json:"temperature,omitempty"`
	// Represents the level of reasoning effort for AI model responses.
	// The reasoning effort parameter guides the model on how many reasoning tokens
	// to generate before creating a response to the prompt. Higher effort levels
	// result in more thorough reasoning at the cost of speed and token usage.
	ReasoningEffort v1.ReasoningEffort `protobuf:"varint,3,opt,name=reasoning_effort,json=reasoningEffort,proto3,enum=malonaz.ai.v1.ReasoningEffort" json:"reasoning_effort,omitempty"`
	// If true, we attempt to clean the output to extract a json object.
	// Fails the request if a json object cannot be found.
	ExtractJsonObject bool `protobuf:"varint,4,opt,name=extract_json_object,json=extractJsonObject,proto3" json:"extract_json_object,omitempty"`
	unknownFields     protoimpl.UnknownFields
	sizeCache         protoimpl.SizeCache
}

func (x *TextToTextConfiguration) Reset() {
	*x = TextToTextConfiguration{}
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[6]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TextToTextConfiguration) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TextToTextConfiguration) ProtoMessage() {}

func (x *TextToTextConfiguration) ProtoReflect() protoreflect.Message {
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[6]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use TextToTextConfiguration.ProtoReflect.Descriptor instead.
func (*TextToTextConfiguration) Descriptor() ([]byte, []int) {
	return file_malonaz_ai_ai_service_v1_ai_service_proto_rawDescGZIP(), []int{6}
}

func (x *TextToTextConfiguration) GetMaxTokens() int32 {
	if x != nil {
		return x.MaxTokens
	}
	return 0
}

func (x *TextToTextConfiguration) GetTemperature() float64 {
	if x != nil {
		return x.Temperature
	}
	return 0
}

func (x *TextToTextConfiguration) GetReasoningEffort() v1.ReasoningEffort {
	if x != nil {
		return x.ReasoningEffort
	}
	return v1.ReasoningEffort(0)
}

func (x *TextToTextConfiguration) GetExtractJsonObject() bool {
	if x != nil {
		return x.ExtractJsonObject
	}
	return false
}

// Request message for Ai.TextToText.
type TextToTextRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The resource name of the model used.
	// Format: providers/{provider}/models/{model}
	Model string `protobuf:"bytes,1,opt,name=model,proto3" json:"model,omitempty"`
	// The conversation messages.
	Messages []*v1.Message `protobuf:"bytes,2,rep,name=messages,proto3" json:"messages,omitempty"`
	// Tools available for the model to call.
	Tools []*v1.Tool `protobuf:"bytes,3,rep,name=tools,proto3" json:"tools,omitempty"`
	// For the model to use a tool.
	ToolChoice string `protobuf:"bytes,4,opt,name=tool_choice,json=toolChoice,proto3" json:"tool_choice,omitempty"`
	// Additional configuration.
	Configuration *TextToTextConfiguration `protobuf:"bytes,5,opt,name=configuration,proto3" json:"configuration,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *TextToTextRequest) Reset() {
	*x = TextToTextRequest{}
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[7]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TextToTextRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TextToTextRequest) ProtoMessage() {}

func (x *TextToTextRequest) ProtoReflect() protoreflect.Message {
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[7]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use TextToTextRequest.ProtoReflect.Descriptor instead.
func (*TextToTextRequest) Descriptor() ([]byte, []int) {
	return file_malonaz_ai_ai_service_v1_ai_service_proto_rawDescGZIP(), []int{7}
}

func (x *TextToTextRequest) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *TextToTextRequest) GetMessages() []*v1.Message {
	if x != nil {
		return x.Messages
	}
	return nil
}

func (x *TextToTextRequest) GetTools() []*v1.Tool {
	if x != nil {
		return x.Tools
	}
	return nil
}

func (x *TextToTextRequest) GetToolChoice() string {
	if x != nil {
		return x.ToolChoice
	}
	return ""
}

func (x *TextToTextRequest) GetConfiguration() *TextToTextConfiguration {
	if x != nil {
		return x.Configuration
	}
	return nil
}

// Response message for Ai.TextToText.
type TextToTextResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The generated message.
	Message *v1.Message `protobuf:"bytes,1,opt,name=message,proto3" json:"message,omitempty"`
	// Reason why generation stopped.
	StopReason TextToTextStopReason `protobuf:"varint,2,opt,name=stop_reason,json=stopReason,proto3,enum=malonaz.ai.ai_service.v1.TextToTextStopReason" json:"stop_reason,omitempty"`
	// Model usage metrics.
	ModelUsage *v1.ModelUsage `protobuf:"bytes,3,opt,name=model_usage,json=modelUsage,proto3" json:"model_usage,omitempty"`
	// Generation metrics.
	GenerationMetrics *v1.GenerationMetrics `protobuf:"bytes,4,opt,name=generation_metrics,json=generationMetrics,proto3" json:"generation_metrics,omitempty"`
	unknownFields     protoimpl.UnknownFields
	sizeCache         protoimpl.SizeCache
}

func (x *TextToTextResponse) Reset() {
	*x = TextToTextResponse{}
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[8]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TextToTextResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TextToTextResponse) ProtoMessage() {}

func (x *TextToTextResponse) ProtoReflect() protoreflect.Message {
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[8]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use TextToTextResponse.ProtoReflect.Descriptor instead.
func (*TextToTextResponse) Descriptor() ([]byte, []int) {
	return file_malonaz_ai_ai_service_v1_ai_service_proto_rawDescGZIP(), []int{8}
}

func (x *TextToTextResponse) GetMessage() *v1.Message {
	if x != nil {
		return x.Message
	}
	return nil
}

func (x *TextToTextResponse) GetStopReason() TextToTextStopReason {
	if x != nil {
		return x.StopReason
	}
	return TextToTextStopReason_TEXT_TO_TEXT_STOP_REASON_UNSPECIFIED
}

func (x *TextToTextResponse) GetModelUsage() *v1.ModelUsage {
	if x != nil {
		return x.ModelUsage
	}
	return nil
}

func (x *TextToTextResponse) GetGenerationMetrics() *v1.GenerationMetrics {
	if x != nil {
		return x.GenerationMetrics
	}
	return nil
}

// Request message for Ai.TextToTextStream.
type TextToTextStreamRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The resource name of the model used.
	// Format: providers/{provider}/models/{model}
	Model string `protobuf:"bytes,1,opt,name=model,proto3" json:"model,omitempty"`
	// The conversation messages.
	Messages []*v1.Message `protobuf:"bytes,2,rep,name=messages,proto3" json:"messages,omitempty"`
	// Tools available for the model to call.
	Tools []*v1.Tool `protobuf:"bytes,3,rep,name=tools,proto3" json:"tools,omitempty"`
	// For the model to use a tool.
	ToolChoice string `protobuf:"bytes,4,opt,name=tool_choice,json=toolChoice,proto3" json:"tool_choice,omitempty"`
	// Additional configuration.
	Configuration *TextToTextConfiguration `protobuf:"bytes,5,opt,name=configuration,proto3" json:"configuration,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *TextToTextStreamRequest) Reset() {
	*x = TextToTextStreamRequest{}
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[9]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TextToTextStreamRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TextToTextStreamRequest) ProtoMessage() {}

func (x *TextToTextStreamRequest) ProtoReflect() protoreflect.Message {
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[9]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use TextToTextStreamRequest.ProtoReflect.Descriptor instead.
func (*TextToTextStreamRequest) Descriptor() ([]byte, []int) {
	return file_malonaz_ai_ai_service_v1_ai_service_proto_rawDescGZIP(), []int{9}
}

func (x *TextToTextStreamRequest) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *TextToTextStreamRequest) GetMessages() []*v1.Message {
	if x != nil {
		return x.Messages
	}
	return nil
}

func (x *TextToTextStreamRequest) GetTools() []*v1.Tool {
	if x != nil {
		return x.Tools
	}
	return nil
}

func (x *TextToTextStreamRequest) GetToolChoice() string {
	if x != nil {
		return x.ToolChoice
	}
	return ""
}

func (x *TextToTextStreamRequest) GetConfiguration() *TextToTextConfiguration {
	if x != nil {
		return x.Configuration
	}
	return nil
}

// Response message for Ai.TextToTextStream.
type TextToTextStreamResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Content of this response.
	//
	// Types that are valid to be assigned to Content:
	//
	//	*TextToTextStreamResponse_ContentChunk
	//	*TextToTextStreamResponse_ReasoningChunk
	//	*TextToTextStreamResponse_StopReason
	//	*TextToTextStreamResponse_ToolCall
	//	*TextToTextStreamResponse_ModelUsage
	//	*TextToTextStreamResponse_GenerationMetrics
	Content       isTextToTextStreamResponse_Content `protobuf_oneof:"content"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *TextToTextStreamResponse) Reset() {
	*x = TextToTextStreamResponse{}
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[10]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TextToTextStreamResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TextToTextStreamResponse) ProtoMessage() {}

func (x *TextToTextStreamResponse) ProtoReflect() protoreflect.Message {
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[10]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use TextToTextStreamResponse.ProtoReflect.Descriptor instead.
func (*TextToTextStreamResponse) Descriptor() ([]byte, []int) {
	return file_malonaz_ai_ai_service_v1_ai_service_proto_rawDescGZIP(), []int{10}
}

func (x *TextToTextStreamResponse) GetContent() isTextToTextStreamResponse_Content {
	if x != nil {
		return x.Content
	}
	return nil
}

func (x *TextToTextStreamResponse) GetContentChunk() string {
	if x != nil {
		if x, ok := x.Content.(*TextToTextStreamResponse_ContentChunk); ok {
			return x.ContentChunk
		}
	}
	return ""
}

func (x *TextToTextStreamResponse) GetReasoningChunk() string {
	if x != nil {
		if x, ok := x.Content.(*TextToTextStreamResponse_ReasoningChunk); ok {
			return x.ReasoningChunk
		}
	}
	return ""
}

func (x *TextToTextStreamResponse) GetStopReason() TextToTextStopReason {
	if x != nil {
		if x, ok := x.Content.(*TextToTextStreamResponse_StopReason); ok {
			return x.StopReason
		}
	}
	return TextToTextStopReason_TEXT_TO_TEXT_STOP_REASON_UNSPECIFIED
}

func (x *TextToTextStreamResponse) GetToolCall() *v1.ToolCall {
	if x != nil {
		if x, ok := x.Content.(*TextToTextStreamResponse_ToolCall); ok {
			return x.ToolCall
		}
	}
	return nil
}

func (x *TextToTextStreamResponse) GetModelUsage() *v1.ModelUsage {
	if x != nil {
		if x, ok := x.Content.(*TextToTextStreamResponse_ModelUsage); ok {
			return x.ModelUsage
		}
	}
	return nil
}

func (x *TextToTextStreamResponse) GetGenerationMetrics() *v1.GenerationMetrics {
	if x != nil {
		if x, ok := x.Content.(*TextToTextStreamResponse_GenerationMetrics); ok {
			return x.GenerationMetrics
		}
	}
	return nil
}

type isTextToTextStreamResponse_Content interface {
	isTextToTextStreamResponse_Content()
}

type TextToTextStreamResponse_ContentChunk struct {
	// A chunk of the generated message content.
	ContentChunk string `protobuf:"bytes,1,opt,name=content_chunk,json=contentChunk,proto3,oneof"`
}

type TextToTextStreamResponse_ReasoningChunk struct {
	// Reasoning content chunk (if model supports reasoning).
	ReasoningChunk string `protobuf:"bytes,2,opt,name=reasoning_chunk,json=reasoningChunk,proto3,oneof"`
}

type TextToTextStreamResponse_StopReason struct {
	// Reason why generation stopped.
	StopReason TextToTextStopReason `protobuf:"varint,3,opt,name=stop_reason,json=stopReason,proto3,enum=malonaz.ai.ai_service.v1.TextToTextStopReason,oneof"`
}

type TextToTextStreamResponse_ToolCall struct {
	// Tool calls requested by the assistant (sent when complete).
	ToolCall *v1.ToolCall `protobuf:"bytes,4,opt,name=tool_call,json=toolCall,proto3,oneof"`
}

type TextToTextStreamResponse_ModelUsage struct {
	// Model usage event (sent last).
	ModelUsage *v1.ModelUsage `protobuf:"bytes,5,opt,name=model_usage,json=modelUsage,proto3,oneof"`
}

type TextToTextStreamResponse_GenerationMetrics struct {
	// Generation metrics (sent last).
	GenerationMetrics *v1.GenerationMetrics `protobuf:"bytes,6,opt,name=generation_metrics,json=generationMetrics,proto3,oneof"`
}

func (*TextToTextStreamResponse_ContentChunk) isTextToTextStreamResponse_Content() {}

func (*TextToTextStreamResponse_ReasoningChunk) isTextToTextStreamResponse_Content() {}

func (*TextToTextStreamResponse_StopReason) isTextToTextStreamResponse_Content() {}

func (*TextToTextStreamResponse_ToolCall) isTextToTextStreamResponse_Content() {}

func (*TextToTextStreamResponse_ModelUsage) isTextToTextStreamResponse_Content() {}

func (*TextToTextStreamResponse_GenerationMetrics) isTextToTextStreamResponse_Content() {}

// Request message for Ai.TextToSpeech.
type TextToSpeechRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The resource name of the model used.
	// Format: providers/{provider}/models/{model}
	Model string `protobuf:"bytes,1,opt,name=model,proto3" json:"model,omitempty"`
	// The text to convert to speech.
	Text string `protobuf:"bytes,2,opt,name=text,proto3" json:"text,omitempty"`
	// Voice identifier to use for speech generation.
	Voice         string `protobuf:"bytes,3,opt,name=voice,proto3" json:"voice,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *TextToSpeechRequest) Reset() {
	*x = TextToSpeechRequest{}
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[11]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TextToSpeechRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TextToSpeechRequest) ProtoMessage() {}

func (x *TextToSpeechRequest) ProtoReflect() protoreflect.Message {
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[11]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use TextToSpeechRequest.ProtoReflect.Descriptor instead.
func (*TextToSpeechRequest) Descriptor() ([]byte, []int) {
	return file_malonaz_ai_ai_service_v1_ai_service_proto_rawDescGZIP(), []int{11}
}

func (x *TextToSpeechRequest) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *TextToSpeechRequest) GetText() string {
	if x != nil {
		return x.Text
	}
	return ""
}

func (x *TextToSpeechRequest) GetVoice() string {
	if x != nil {
		return x.Voice
	}
	return ""
}

// Response message for Ai.TextToSpeech.
type TextToSpeechResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Audio format of the audio.
	AudioFormat *v11.Format `protobuf:"bytes,1,opt,name=audio_format,json=audioFormat,proto3" json:"audio_format,omitempty"`
	// Audio data chunk in PCM16 format.
	AudioChunk *v11.Chunk `protobuf:"bytes,2,opt,name=audio_chunk,json=audioChunk,proto3" json:"audio_chunk,omitempty"`
	// Model usage metrics.
	ModelUsage *v1.ModelUsage `protobuf:"bytes,3,opt,name=model_usage,json=modelUsage,proto3" json:"model_usage,omitempty"`
	// Generation metrics.
	GenerationMetrics *v1.GenerationMetrics `protobuf:"bytes,4,opt,name=generation_metrics,json=generationMetrics,proto3" json:"generation_metrics,omitempty"`
	unknownFields     protoimpl.UnknownFields
	sizeCache         protoimpl.SizeCache
}

func (x *TextToSpeechResponse) Reset() {
	*x = TextToSpeechResponse{}
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[12]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TextToSpeechResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TextToSpeechResponse) ProtoMessage() {}

func (x *TextToSpeechResponse) ProtoReflect() protoreflect.Message {
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[12]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use TextToSpeechResponse.ProtoReflect.Descriptor instead.
func (*TextToSpeechResponse) Descriptor() ([]byte, []int) {
	return file_malonaz_ai_ai_service_v1_ai_service_proto_rawDescGZIP(), []int{12}
}

func (x *TextToSpeechResponse) GetAudioFormat() *v11.Format {
	if x != nil {
		return x.AudioFormat
	}
	return nil
}

func (x *TextToSpeechResponse) GetAudioChunk() *v11.Chunk {
	if x != nil {
		return x.AudioChunk
	}
	return nil
}

func (x *TextToSpeechResponse) GetModelUsage() *v1.ModelUsage {
	if x != nil {
		return x.ModelUsage
	}
	return nil
}

func (x *TextToSpeechResponse) GetGenerationMetrics() *v1.GenerationMetrics {
	if x != nil {
		return x.GenerationMetrics
	}
	return nil
}

// Request message for Ai.TextToSpeechStream.
type TextToSpeechStreamRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The resource name of the model used.
	// Format: providers/{provider}/models/{model}
	Model string `protobuf:"bytes,1,opt,name=model,proto3" json:"model,omitempty"`
	// The text to convert to speech.
	Text string `protobuf:"bytes,2,opt,name=text,proto3" json:"text,omitempty"`
	// Voice identifier to use for speech generation.
	Voice         string `protobuf:"bytes,3,opt,name=voice,proto3" json:"voice,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *TextToSpeechStreamRequest) Reset() {
	*x = TextToSpeechStreamRequest{}
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[13]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TextToSpeechStreamRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TextToSpeechStreamRequest) ProtoMessage() {}

func (x *TextToSpeechStreamRequest) ProtoReflect() protoreflect.Message {
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[13]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use TextToSpeechStreamRequest.ProtoReflect.Descriptor instead.
func (*TextToSpeechStreamRequest) Descriptor() ([]byte, []int) {
	return file_malonaz_ai_ai_service_v1_ai_service_proto_rawDescGZIP(), []int{13}
}

func (x *TextToSpeechStreamRequest) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *TextToSpeechStreamRequest) GetText() string {
	if x != nil {
		return x.Text
	}
	return ""
}

func (x *TextToSpeechStreamRequest) GetVoice() string {
	if x != nil {
		return x.Voice
	}
	return ""
}

// Response message for Ai.TextToSpeechStream.
type TextToSpeechStreamResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Content of this response.
	//
	// Types that are valid to be assigned to Content:
	//
	//	*TextToSpeechStreamResponse_AudioFormat
	//	*TextToSpeechStreamResponse_AudioChunk
	//	*TextToSpeechStreamResponse_ModelUsage
	//	*TextToSpeechStreamResponse_GenerationMetrics
	Content       isTextToSpeechStreamResponse_Content `protobuf_oneof:"content"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *TextToSpeechStreamResponse) Reset() {
	*x = TextToSpeechStreamResponse{}
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[14]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TextToSpeechStreamResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TextToSpeechStreamResponse) ProtoMessage() {}

func (x *TextToSpeechStreamResponse) ProtoReflect() protoreflect.Message {
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[14]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use TextToSpeechStreamResponse.ProtoReflect.Descriptor instead.
func (*TextToSpeechStreamResponse) Descriptor() ([]byte, []int) {
	return file_malonaz_ai_ai_service_v1_ai_service_proto_rawDescGZIP(), []int{14}
}

func (x *TextToSpeechStreamResponse) GetContent() isTextToSpeechStreamResponse_Content {
	if x != nil {
		return x.Content
	}
	return nil
}

func (x *TextToSpeechStreamResponse) GetAudioFormat() *v11.Format {
	if x != nil {
		if x, ok := x.Content.(*TextToSpeechStreamResponse_AudioFormat); ok {
			return x.AudioFormat
		}
	}
	return nil
}

func (x *TextToSpeechStreamResponse) GetAudioChunk() *v11.Chunk {
	if x != nil {
		if x, ok := x.Content.(*TextToSpeechStreamResponse_AudioChunk); ok {
			return x.AudioChunk
		}
	}
	return nil
}

func (x *TextToSpeechStreamResponse) GetModelUsage() *v1.ModelUsage {
	if x != nil {
		if x, ok := x.Content.(*TextToSpeechStreamResponse_ModelUsage); ok {
			return x.ModelUsage
		}
	}
	return nil
}

func (x *TextToSpeechStreamResponse) GetGenerationMetrics() *v1.GenerationMetrics {
	if x != nil {
		if x, ok := x.Content.(*TextToSpeechStreamResponse_GenerationMetrics); ok {
			return x.GenerationMetrics
		}
	}
	return nil
}

type isTextToSpeechStreamResponse_Content interface {
	isTextToSpeechStreamResponse_Content()
}

type TextToSpeechStreamResponse_AudioFormat struct {
	// Audio format of the audio stream.
	AudioFormat *v11.Format `protobuf:"bytes,1,opt,name=audio_format,json=audioFormat,proto3,oneof"`
}

type TextToSpeechStreamResponse_AudioChunk struct {
	// Audio data chunk in PCM16 format.
	AudioChunk *v11.Chunk `protobuf:"bytes,2,opt,name=audio_chunk,json=audioChunk,proto3,oneof"`
}

type TextToSpeechStreamResponse_ModelUsage struct {
	// Model usage event (sent last).
	ModelUsage *v1.ModelUsage `protobuf:"bytes,3,opt,name=model_usage,json=modelUsage,proto3,oneof"`
}

type TextToSpeechStreamResponse_GenerationMetrics struct {
	// Generation metrics.
	GenerationMetrics *v1.GenerationMetrics `protobuf:"bytes,4,opt,name=generation_metrics,json=generationMetrics,proto3,oneof"`
}

func (*TextToSpeechStreamResponse_AudioFormat) isTextToSpeechStreamResponse_Content() {}

func (*TextToSpeechStreamResponse_AudioChunk) isTextToSpeechStreamResponse_Content() {}

func (*TextToSpeechStreamResponse_ModelUsage) isTextToSpeechStreamResponse_Content() {}

func (*TextToSpeechStreamResponse_GenerationMetrics) isTextToSpeechStreamResponse_Content() {}

var File_malonaz_ai_ai_service_v1_ai_service_proto protoreflect.FileDescriptor

const file_malonaz_ai_ai_service_v1_ai_service_proto_rawDesc = "" +
	"\n" +
	")malonaz/ai/ai_service/v1/ai_service.proto\x12\x18malonaz.ai.ai_service.v1\x1a\x1bbuf/validate/validate.proto\x1a\x1cgoogle/api/annotations.proto\x1a\x17google/api/client.proto\x1a\x19google/api/resource.proto\x1a\x1bmalonaz/ai/v1/message.proto\x1a\x1bmalonaz/ai/v1/metrics.proto\x1a\x19malonaz/ai/v1/model.proto\x1a\x18malonaz/ai/v1/tool.proto\x1a\x1cmalonaz/audio/v1/audio.proto\x1a malonaz/codegen/aip/v1/aip.proto\"\xa7\x01\n" +
	"\x12CreateModelRequest\x12:\n" +
	"\x06parent\x18\x01 \x01(\tB\"\xfaA\x19\n" +
	"\x17ai.malonaz.com/Provider\xbaH\x03\xc8\x01\x01R\x06parent\x122\n" +
	"\x05model\x18\x02 \x01(\v2\x14.malonaz.ai.v1.ModelB\x06\xbaH\x03\xc8\x01\x01R\x05model\x12!\n" +
	"\bmodel_id\x18\x03 \x01(\tB\x06\xbaH\x03\xc8\x01\x01R\amodelId\"F\n" +
	"\x0fGetModelRequest\x123\n" +
	"\x04name\x18\x01 \x01(\tB\x1f\xfaA\x16\n" +
	"\x14ai.malonaz.com/Model\xbaH\x03\xc8\x01\x01R\x04name\"\xa0\x01\n" +
	"\x11ListModelsRequest\x12:\n" +
	"\x06parent\x18\x01 \x01(\tB\"\xfaA\x19\n" +
	"\x17ai.malonaz.com/Provider\xbaH\x03\xc8\x01\x01R\x06parent\x12'\n" +
	"\tpage_size\x18\x02 \x01(\x05B\n" +
	"\xbaH\a\x1a\x05\x18\xe8\a(\x00R\bpageSize\x12\x1d\n" +
	"\n" +
	"page_token\x18\x03 \x01(\tR\tpageToken:\a\x82\xf3-\x03\b\xf4\x03\"j\n" +
	"\x12ListModelsResponse\x12,\n" +
	"\x06models\x18\x01 \x03(\v2\x14.malonaz.ai.v1.ModelR\x06models\x12&\n" +
	"\x0fnext_page_token\x18\x02 \x01(\tR\rnextPageToken\"\x86\x01\n" +
	"\x13SpeechToTextRequest\x125\n" +
	"\x05model\x18\x01 \x01(\tB\x1f\xfaA\x16\n" +
	"\x14ai.malonaz.com/Model\xbaH\x03\xc8\x01\x01R\x05model\x12\x1c\n" +
	"\x05audio\x18\x02 \x01(\fB\x06\xbaH\x03\xc8\x01\x01R\x05audio\x12\x1a\n" +
	"\blanguage\x18\x03 \x01(\tR\blanguage\"\xc3\x01\n" +
	"\x14SpeechToTextResponse\x12\x1e\n" +
	"\n" +
	"transcript\x18\x01 \x01(\tR\n" +
	"transcript\x12:\n" +
	"\vmodel_usage\x18\x02 \x01(\v2\x19.malonaz.ai.v1.ModelUsageR\n" +
	"modelUsage\x12O\n" +
	"\x12generation_metrics\x18\x03 \x01(\v2 .malonaz.ai.v1.GenerationMetricsR\x11generationMetrics\"\xf7\x01\n" +
	"\x17TextToTextConfiguration\x12&\n" +
	"\n" +
	"max_tokens\x18\x01 \x01(\x05B\a\xbaH\x04\x1a\x02(\x00R\tmaxTokens\x129\n" +
	"\vtemperature\x18\x02 \x01(\x01B\x17\xbaH\x14\x12\x12\x19\x00\x00\x00\x00\x00\x00\x00@)\x00\x00\x00\x00\x00\x00\x00\x00R\vtemperature\x12I\n" +
	"\x10reasoning_effort\x18\x03 \x01(\x0e2\x1e.malonaz.ai.v1.ReasoningEffortR\x0freasoningEffort\x12.\n" +
	"\x13extract_json_object\x18\x04 \x01(\bR\x11extractJsonObject\"\xad\x02\n" +
	"\x11TextToTextRequest\x125\n" +
	"\x05model\x18\x01 \x01(\tB\x1f\xfaA\x16\n" +
	"\x14ai.malonaz.com/Model\xbaH\x03\xc8\x01\x01R\x05model\x12<\n" +
	"\bmessages\x18\x02 \x03(\v2\x16.malonaz.ai.v1.MessageB\b\xbaH\x05\x92\x01\x02\b\x01R\bmessages\x12)\n" +
	"\x05tools\x18\x03 \x03(\v2\x13.malonaz.ai.v1.ToolR\x05tools\x12\x1f\n" +
	"\vtool_choice\x18\x04 \x01(\tR\n" +
	"toolChoice\x12W\n" +
	"\rconfiguration\x18\x05 \x01(\v21.malonaz.ai.ai_service.v1.TextToTextConfigurationR\rconfiguration\"\xa4\x02\n" +
	"\x12TextToTextResponse\x120\n" +
	"\amessage\x18\x01 \x01(\v2\x16.malonaz.ai.v1.MessageR\amessage\x12O\n" +
	"\vstop_reason\x18\x02 \x01(\x0e2..malonaz.ai.ai_service.v1.TextToTextStopReasonR\n" +
	"stopReason\x12:\n" +
	"\vmodel_usage\x18\x03 \x01(\v2\x19.malonaz.ai.v1.ModelUsageR\n" +
	"modelUsage\x12O\n" +
	"\x12generation_metrics\x18\x04 \x01(\v2 .malonaz.ai.v1.GenerationMetricsR\x11generationMetrics\"\xb3\x02\n" +
	"\x17TextToTextStreamRequest\x125\n" +
	"\x05model\x18\x01 \x01(\tB\x1f\xfaA\x16\n" +
	"\x14ai.malonaz.com/Model\xbaH\x03\xc8\x01\x01R\x05model\x12<\n" +
	"\bmessages\x18\x02 \x03(\v2\x16.malonaz.ai.v1.MessageB\b\xbaH\x05\x92\x01\x02\b\x01R\bmessages\x12)\n" +
	"\x05tools\x18\x03 \x03(\v2\x13.malonaz.ai.v1.ToolR\x05tools\x12\x1f\n" +
	"\vtool_choice\x18\x04 \x01(\tR\n" +
	"toolChoice\x12W\n" +
	"\rconfiguration\x18\x05 \x01(\v21.malonaz.ai.ai_service.v1.TextToTextConfigurationR\rconfiguration\"\x9a\x03\n" +
	"\x18TextToTextStreamResponse\x12%\n" +
	"\rcontent_chunk\x18\x01 \x01(\tH\x00R\fcontentChunk\x12)\n" +
	"\x0freasoning_chunk\x18\x02 \x01(\tH\x00R\x0ereasoningChunk\x12Q\n" +
	"\vstop_reason\x18\x03 \x01(\x0e2..malonaz.ai.ai_service.v1.TextToTextStopReasonH\x00R\n" +
	"stopReason\x126\n" +
	"\ttool_call\x18\x04 \x01(\v2\x17.malonaz.ai.v1.ToolCallH\x00R\btoolCall\x12<\n" +
	"\vmodel_usage\x18\x05 \x01(\v2\x19.malonaz.ai.v1.ModelUsageH\x00R\n" +
	"modelUsage\x12Q\n" +
	"\x12generation_metrics\x18\x06 \x01(\v2 .malonaz.ai.v1.GenerationMetricsH\x00R\x11generationMetricsB\x10\n" +
	"\acontent\x12\x05\xbaH\x02\b\x01\"\x88\x01\n" +
	"\x13TextToSpeechRequest\x125\n" +
	"\x05model\x18\x01 \x01(\tB\x1f\xfaA\x16\n" +
	"\x14ai.malonaz.com/Model\xbaH\x03\xc8\x01\x01R\x05model\x12\x1b\n" +
	"\x04text\x18\x02 \x01(\tB\a\xbaH\x04r\x02\x10\x01R\x04text\x12\x1d\n" +
	"\x05voice\x18\x03 \x01(\tB\a\xbaH\x04r\x02\x10\x01R\x05voice\"\x9a\x02\n" +
	"\x14TextToSpeechResponse\x12;\n" +
	"\faudio_format\x18\x01 \x01(\v2\x18.malonaz.audio.v1.FormatR\vaudioFormat\x128\n" +
	"\vaudio_chunk\x18\x02 \x01(\v2\x17.malonaz.audio.v1.ChunkR\n" +
	"audioChunk\x12:\n" +
	"\vmodel_usage\x18\x03 \x01(\v2\x19.malonaz.ai.v1.ModelUsageR\n" +
	"modelUsage\x12O\n" +
	"\x12generation_metrics\x18\x04 \x01(\v2 .malonaz.ai.v1.GenerationMetricsR\x11generationMetrics\"\x8e\x01\n" +
	"\x19TextToSpeechStreamRequest\x125\n" +
	"\x05model\x18\x01 \x01(\tB\x1f\xfaA\x16\n" +
	"\x14ai.malonaz.com/Model\xbaH\x03\xc8\x01\x01R\x05model\x12\x1b\n" +
	"\x04text\x18\x02 \x01(\tB\a\xbaH\x04r\x02\x10\x01R\x04text\x12\x1d\n" +
	"\x05voice\x18\x03 \x01(\tB\a\xbaH\x04r\x02\x10\x01R\x05voice\"\xba\x02\n" +
	"\x1aTextToSpeechStreamResponse\x12=\n" +
	"\faudio_format\x18\x01 \x01(\v2\x18.malonaz.audio.v1.FormatH\x00R\vaudioFormat\x12:\n" +
	"\vaudio_chunk\x18\x02 \x01(\v2\x17.malonaz.audio.v1.ChunkH\x00R\n" +
	"audioChunk\x12<\n" +
	"\vmodel_usage\x18\x03 \x01(\v2\x19.malonaz.ai.v1.ModelUsageH\x00R\n" +
	"modelUsage\x12Q\n" +
	"\x12generation_metrics\x18\x04 \x01(\v2 .malonaz.ai.v1.GenerationMetricsH\x00R\x11generationMetricsB\x10\n" +
	"\acontent\x12\x05\xbaH\x02\b\x01*\xb3\x02\n" +
	"\x14TextToTextStopReason\x12(\n" +
	"$TEXT_TO_TEXT_STOP_REASON_UNSPECIFIED\x10\x00\x12%\n" +
	"!TEXT_TO_TEXT_STOP_REASON_END_TURN\x10\x01\x12'\n" +
	"#TEXT_TO_TEXT_STOP_REASON_MAX_TOKENS\x10\x02\x12&\n" +
	"\"TEXT_TO_TEXT_STOP_REASON_TOOL_CALL\x10\x03\x12*\n" +
	"&TEXT_TO_TEXT_STOP_REASON_STOP_SEQUENCE\x10\x04\x12'\n" +
	"#TEXT_TO_TEXT_STOP_REASON_PAUSE_TURN\x10\x05\x12$\n" +
	" TEXT_TO_TEXT_STOP_REASON_REFUSAL\x10\x062\xdb\n" +
	"\n" +
	"\x02Ai\x12\x81\x01\n" +
	"\vCreateModel\x12,.malonaz.ai.ai_service.v1.CreateModelRequest\x1a\x14.malonaz.ai.v1.Model\".\xdaA\x04name\x82\xd3\xe4\x93\x02!\"\x1f/v1/{name=providers/*/models/*}\x12{\n" +
	"\bGetModel\x12).malonaz.ai.ai_service.v1.GetModelRequest\x1a\x14.malonaz.ai.v1.Model\".\xdaA\x04name\x82\xd3\xe4\x93\x02!\x12\x1f/v1/{name=providers/*/models/*}\x12\x9b\x01\n" +
	"\n" +
	"ListModels\x12+.malonaz.ai.ai_service.v1.ListModelsRequest\x1a,.malonaz.ai.ai_service.v1.ListModelsResponse\"2\xdaA\x06parent\x82\xd3\xe4\x93\x02#\x12!/v1/{parent=providers/*/*}/models\x12\xb0\x01\n" +
	"\fSpeechToText\x12-.malonaz.ai.ai_service.v1.SpeechToTextRequest\x1a..malonaz.ai.ai_service.v1.SpeechToTextResponse\"A\x82\xd3\xe4\x93\x02;:\x01*\"6/v1/providers/{provider}/models/{model}/speech-to-text\x12\xa8\x01\n" +
	"\n" +
	"TextToText\x12+.malonaz.ai.ai_service.v1.TextToTextRequest\x1a,.malonaz.ai.ai_service.v1.TextToTextResponse\"?\x82\xd3\xe4\x93\x029:\x01*\"4/v1/providers/{provider}/models/{model}/text-to-text\x12\xc3\x01\n" +
	"\x10TextToTextStream\x121.malonaz.ai.ai_service.v1.TextToTextStreamRequest\x1a2.malonaz.ai.ai_service.v1.TextToTextStreamResponse\"F\x82\xd3\xe4\x93\x02@:\x01*\";/v1/providers/{provider}/models/{model}/text-to-text:stream0\x01\x12\xb0\x01\n" +
	"\fTextToSpeech\x12-.malonaz.ai.ai_service.v1.TextToSpeechRequest\x1a..malonaz.ai.ai_service.v1.TextToSpeechResponse\"A\x82\xd3\xe4\x93\x02;:\x01*\"6/v1/providers/{provider}/models/{model}/text-to-speech\x12\xcb\x01\n" +
	"\x12TextToSpeechStream\x123.malonaz.ai.ai_service.v1.TextToSpeechStreamRequest\x1a4.malonaz.ai.ai_service.v1.TextToSpeechStreamResponse\"H\x82\xd3\xe4\x93\x02B:\x01*\"=/v1/providers/{provider}/models/{model}/text-to-speech:stream0\x01\x1a\x11\xcaA\x0eai.malonaz.comB3Z1github.com/malonaz/core/genproto/ai/ai_service/v1b\x06proto3"

var (
	file_malonaz_ai_ai_service_v1_ai_service_proto_rawDescOnce sync.Once
	file_malonaz_ai_ai_service_v1_ai_service_proto_rawDescData []byte
)

func file_malonaz_ai_ai_service_v1_ai_service_proto_rawDescGZIP() []byte {
	file_malonaz_ai_ai_service_v1_ai_service_proto_rawDescOnce.Do(func() {
		file_malonaz_ai_ai_service_v1_ai_service_proto_rawDescData = protoimpl.X.CompressGZIP(unsafe.Slice(unsafe.StringData(file_malonaz_ai_ai_service_v1_ai_service_proto_rawDesc), len(file_malonaz_ai_ai_service_v1_ai_service_proto_rawDesc)))
	})
	return file_malonaz_ai_ai_service_v1_ai_service_proto_rawDescData
}

var file_malonaz_ai_ai_service_v1_ai_service_proto_enumTypes = make([]protoimpl.EnumInfo, 1)
var file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes = make([]protoimpl.MessageInfo, 15)
var file_malonaz_ai_ai_service_v1_ai_service_proto_goTypes = []any{
	(TextToTextStopReason)(0),          // 0: malonaz.ai.ai_service.v1.TextToTextStopReason
	(*CreateModelRequest)(nil),         // 1: malonaz.ai.ai_service.v1.CreateModelRequest
	(*GetModelRequest)(nil),            // 2: malonaz.ai.ai_service.v1.GetModelRequest
	(*ListModelsRequest)(nil),          // 3: malonaz.ai.ai_service.v1.ListModelsRequest
	(*ListModelsResponse)(nil),         // 4: malonaz.ai.ai_service.v1.ListModelsResponse
	(*SpeechToTextRequest)(nil),        // 5: malonaz.ai.ai_service.v1.SpeechToTextRequest
	(*SpeechToTextResponse)(nil),       // 6: malonaz.ai.ai_service.v1.SpeechToTextResponse
	(*TextToTextConfiguration)(nil),    // 7: malonaz.ai.ai_service.v1.TextToTextConfiguration
	(*TextToTextRequest)(nil),          // 8: malonaz.ai.ai_service.v1.TextToTextRequest
	(*TextToTextResponse)(nil),         // 9: malonaz.ai.ai_service.v1.TextToTextResponse
	(*TextToTextStreamRequest)(nil),    // 10: malonaz.ai.ai_service.v1.TextToTextStreamRequest
	(*TextToTextStreamResponse)(nil),   // 11: malonaz.ai.ai_service.v1.TextToTextStreamResponse
	(*TextToSpeechRequest)(nil),        // 12: malonaz.ai.ai_service.v1.TextToSpeechRequest
	(*TextToSpeechResponse)(nil),       // 13: malonaz.ai.ai_service.v1.TextToSpeechResponse
	(*TextToSpeechStreamRequest)(nil),  // 14: malonaz.ai.ai_service.v1.TextToSpeechStreamRequest
	(*TextToSpeechStreamResponse)(nil), // 15: malonaz.ai.ai_service.v1.TextToSpeechStreamResponse
	(*v1.Model)(nil),                   // 16: malonaz.ai.v1.Model
	(*v1.ModelUsage)(nil),              // 17: malonaz.ai.v1.ModelUsage
	(*v1.GenerationMetrics)(nil),       // 18: malonaz.ai.v1.GenerationMetrics
	(v1.ReasoningEffort)(0),            // 19: malonaz.ai.v1.ReasoningEffort
	(*v1.Message)(nil),                 // 20: malonaz.ai.v1.Message
	(*v1.Tool)(nil),                    // 21: malonaz.ai.v1.Tool
	(*v1.ToolCall)(nil),                // 22: malonaz.ai.v1.ToolCall
	(*v11.Format)(nil),                 // 23: malonaz.audio.v1.Format
	(*v11.Chunk)(nil),                  // 24: malonaz.audio.v1.Chunk
}
var file_malonaz_ai_ai_service_v1_ai_service_proto_depIdxs = []int32{
	16, // 0: malonaz.ai.ai_service.v1.CreateModelRequest.model:type_name -> malonaz.ai.v1.Model
	16, // 1: malonaz.ai.ai_service.v1.ListModelsResponse.models:type_name -> malonaz.ai.v1.Model
	17, // 2: malonaz.ai.ai_service.v1.SpeechToTextResponse.model_usage:type_name -> malonaz.ai.v1.ModelUsage
	18, // 3: malonaz.ai.ai_service.v1.SpeechToTextResponse.generation_metrics:type_name -> malonaz.ai.v1.GenerationMetrics
	19, // 4: malonaz.ai.ai_service.v1.TextToTextConfiguration.reasoning_effort:type_name -> malonaz.ai.v1.ReasoningEffort
	20, // 5: malonaz.ai.ai_service.v1.TextToTextRequest.messages:type_name -> malonaz.ai.v1.Message
	21, // 6: malonaz.ai.ai_service.v1.TextToTextRequest.tools:type_name -> malonaz.ai.v1.Tool
	7,  // 7: malonaz.ai.ai_service.v1.TextToTextRequest.configuration:type_name -> malonaz.ai.ai_service.v1.TextToTextConfiguration
	20, // 8: malonaz.ai.ai_service.v1.TextToTextResponse.message:type_name -> malonaz.ai.v1.Message
	0,  // 9: malonaz.ai.ai_service.v1.TextToTextResponse.stop_reason:type_name -> malonaz.ai.ai_service.v1.TextToTextStopReason
	17, // 10: malonaz.ai.ai_service.v1.TextToTextResponse.model_usage:type_name -> malonaz.ai.v1.ModelUsage
	18, // 11: malonaz.ai.ai_service.v1.TextToTextResponse.generation_metrics:type_name -> malonaz.ai.v1.GenerationMetrics
	20, // 12: malonaz.ai.ai_service.v1.TextToTextStreamRequest.messages:type_name -> malonaz.ai.v1.Message
	21, // 13: malonaz.ai.ai_service.v1.TextToTextStreamRequest.tools:type_name -> malonaz.ai.v1.Tool
	7,  // 14: malonaz.ai.ai_service.v1.TextToTextStreamRequest.configuration:type_name -> malonaz.ai.ai_service.v1.TextToTextConfiguration
	0,  // 15: malonaz.ai.ai_service.v1.TextToTextStreamResponse.stop_reason:type_name -> malonaz.ai.ai_service.v1.TextToTextStopReason
	22, // 16: malonaz.ai.ai_service.v1.TextToTextStreamResponse.tool_call:type_name -> malonaz.ai.v1.ToolCall
	17, // 17: malonaz.ai.ai_service.v1.TextToTextStreamResponse.model_usage:type_name -> malonaz.ai.v1.ModelUsage
	18, // 18: malonaz.ai.ai_service.v1.TextToTextStreamResponse.generation_metrics:type_name -> malonaz.ai.v1.GenerationMetrics
	23, // 19: malonaz.ai.ai_service.v1.TextToSpeechResponse.audio_format:type_name -> malonaz.audio.v1.Format
	24, // 20: malonaz.ai.ai_service.v1.TextToSpeechResponse.audio_chunk:type_name -> malonaz.audio.v1.Chunk
	17, // 21: malonaz.ai.ai_service.v1.TextToSpeechResponse.model_usage:type_name -> malonaz.ai.v1.ModelUsage
	18, // 22: malonaz.ai.ai_service.v1.TextToSpeechResponse.generation_metrics:type_name -> malonaz.ai.v1.GenerationMetrics
	23, // 23: malonaz.ai.ai_service.v1.TextToSpeechStreamResponse.audio_format:type_name -> malonaz.audio.v1.Format
	24, // 24: malonaz.ai.ai_service.v1.TextToSpeechStreamResponse.audio_chunk:type_name -> malonaz.audio.v1.Chunk
	17, // 25: malonaz.ai.ai_service.v1.TextToSpeechStreamResponse.model_usage:type_name -> malonaz.ai.v1.ModelUsage
	18, // 26: malonaz.ai.ai_service.v1.TextToSpeechStreamResponse.generation_metrics:type_name -> malonaz.ai.v1.GenerationMetrics
	1,  // 27: malonaz.ai.ai_service.v1.Ai.CreateModel:input_type -> malonaz.ai.ai_service.v1.CreateModelRequest
	2,  // 28: malonaz.ai.ai_service.v1.Ai.GetModel:input_type -> malonaz.ai.ai_service.v1.GetModelRequest
	3,  // 29: malonaz.ai.ai_service.v1.Ai.ListModels:input_type -> malonaz.ai.ai_service.v1.ListModelsRequest
	5,  // 30: malonaz.ai.ai_service.v1.Ai.SpeechToText:input_type -> malonaz.ai.ai_service.v1.SpeechToTextRequest
	8,  // 31: malonaz.ai.ai_service.v1.Ai.TextToText:input_type -> malonaz.ai.ai_service.v1.TextToTextRequest
	10, // 32: malonaz.ai.ai_service.v1.Ai.TextToTextStream:input_type -> malonaz.ai.ai_service.v1.TextToTextStreamRequest
	12, // 33: malonaz.ai.ai_service.v1.Ai.TextToSpeech:input_type -> malonaz.ai.ai_service.v1.TextToSpeechRequest
	14, // 34: malonaz.ai.ai_service.v1.Ai.TextToSpeechStream:input_type -> malonaz.ai.ai_service.v1.TextToSpeechStreamRequest
	16, // 35: malonaz.ai.ai_service.v1.Ai.CreateModel:output_type -> malonaz.ai.v1.Model
	16, // 36: malonaz.ai.ai_service.v1.Ai.GetModel:output_type -> malonaz.ai.v1.Model
	4,  // 37: malonaz.ai.ai_service.v1.Ai.ListModels:output_type -> malonaz.ai.ai_service.v1.ListModelsResponse
	6,  // 38: malonaz.ai.ai_service.v1.Ai.SpeechToText:output_type -> malonaz.ai.ai_service.v1.SpeechToTextResponse
	9,  // 39: malonaz.ai.ai_service.v1.Ai.TextToText:output_type -> malonaz.ai.ai_service.v1.TextToTextResponse
	11, // 40: malonaz.ai.ai_service.v1.Ai.TextToTextStream:output_type -> malonaz.ai.ai_service.v1.TextToTextStreamResponse
	13, // 41: malonaz.ai.ai_service.v1.Ai.TextToSpeech:output_type -> malonaz.ai.ai_service.v1.TextToSpeechResponse
	15, // 42: malonaz.ai.ai_service.v1.Ai.TextToSpeechStream:output_type -> malonaz.ai.ai_service.v1.TextToSpeechStreamResponse
	35, // [35:43] is the sub-list for method output_type
	27, // [27:35] is the sub-list for method input_type
	27, // [27:27] is the sub-list for extension type_name
	27, // [27:27] is the sub-list for extension extendee
	0,  // [0:27] is the sub-list for field type_name
}

func init() { file_malonaz_ai_ai_service_v1_ai_service_proto_init() }
func file_malonaz_ai_ai_service_v1_ai_service_proto_init() {
	if File_malonaz_ai_ai_service_v1_ai_service_proto != nil {
		return
	}
	file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[10].OneofWrappers = []any{
		(*TextToTextStreamResponse_ContentChunk)(nil),
		(*TextToTextStreamResponse_ReasoningChunk)(nil),
		(*TextToTextStreamResponse_StopReason)(nil),
		(*TextToTextStreamResponse_ToolCall)(nil),
		(*TextToTextStreamResponse_ModelUsage)(nil),
		(*TextToTextStreamResponse_GenerationMetrics)(nil),
	}
	file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[14].OneofWrappers = []any{
		(*TextToSpeechStreamResponse_AudioFormat)(nil),
		(*TextToSpeechStreamResponse_AudioChunk)(nil),
		(*TextToSpeechStreamResponse_ModelUsage)(nil),
		(*TextToSpeechStreamResponse_GenerationMetrics)(nil),
	}
	type x struct{}
	out := protoimpl.TypeBuilder{
		File: protoimpl.DescBuilder{
			GoPackagePath: reflect.TypeOf(x{}).PkgPath(),
			RawDescriptor: unsafe.Slice(unsafe.StringData(file_malonaz_ai_ai_service_v1_ai_service_proto_rawDesc), len(file_malonaz_ai_ai_service_v1_ai_service_proto_rawDesc)),
			NumEnums:      1,
			NumMessages:   15,
			NumExtensions: 0,
			NumServices:   1,
		},
		GoTypes:           file_malonaz_ai_ai_service_v1_ai_service_proto_goTypes,
		DependencyIndexes: file_malonaz_ai_ai_service_v1_ai_service_proto_depIdxs,
		EnumInfos:         file_malonaz_ai_ai_service_v1_ai_service_proto_enumTypes,
		MessageInfos:      file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes,
	}.Build()
	File_malonaz_ai_ai_service_v1_ai_service_proto = out.File
	file_malonaz_ai_ai_service_v1_ai_service_proto_goTypes = nil
	file_malonaz_ai_ai_service_v1_ai_service_proto_depIdxs = nil
}
