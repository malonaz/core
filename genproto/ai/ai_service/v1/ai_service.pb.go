// Code generated by protoc-gen-go. DO NOT EDIT.
// versions:
// 	protoc-gen-go v1.36.9
// 	protoc        v6.32.1
// source: malonaz/ai/ai_service/v1/ai_service.proto

//go:build !protoopaque

package v1

import (
	_ "buf.build/gen/go/bufbuild/protovalidate/protocolbuffers/go/buf/validate"
	v1 "github.com/malonaz/core/genproto/ai/v1"
	v11 "github.com/malonaz/core/genproto/audio/v1"
	_ "github.com/malonaz/core/genproto/codegen/aip/v1"
	_ "google.golang.org/genproto/googleapis/api/annotations"
	protoreflect "google.golang.org/protobuf/reflect/protoreflect"
	protoimpl "google.golang.org/protobuf/runtime/protoimpl"
	structpb "google.golang.org/protobuf/types/known/structpb"
	reflect "reflect"
	unsafe "unsafe"
)

const (
	// Verify that this generated code is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(20 - protoimpl.MinVersion)
	// Verify that runtime/protoimpl is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(protoimpl.MaxVersion - 20)
)

// Reason why generation stopped.
type TextToTextStopReason int32

const (
	// Used to detect an unset field.
	TextToTextStopReason_TEXT_TO_TEXT_STOP_REASON_UNSPECIFIED TextToTextStopReason = 0
	// The model reached a natural stopping point.
	TextToTextStopReason_TEXT_TO_TEXT_STOP_REASON_END_TURN TextToTextStopReason = 1
	// The model reached the maximum token limit specified in the request.
	TextToTextStopReason_TEXT_TO_TEXT_STOP_REASON_MAX_TOKENS TextToTextStopReason = 2
	// The model requested to call one or more tools.
	TextToTextStopReason_TEXT_TO_TEXT_STOP_REASON_TOOL_CALL TextToTextStopReason = 3
	// one of your provided custom `stop_sequences` was generated
	TextToTextStopReason_TEXT_TO_TEXT_STOP_REASON_STOP_SEQUENCE TextToTextStopReason = 4
	// The model paused its turn to allow the user to interject.
	TextToTextStopReason_TEXT_TO_TEXT_STOP_REASON_PAUSE_TURN TextToTextStopReason = 5
	// The model refused to respond due to safety or policy reasons.
	TextToTextStopReason_TEXT_TO_TEXT_STOP_REASON_REFUSAL TextToTextStopReason = 6
)

// Enum value maps for TextToTextStopReason.
var (
	TextToTextStopReason_name = map[int32]string{
		0: "TEXT_TO_TEXT_STOP_REASON_UNSPECIFIED",
		1: "TEXT_TO_TEXT_STOP_REASON_END_TURN",
		2: "TEXT_TO_TEXT_STOP_REASON_MAX_TOKENS",
		3: "TEXT_TO_TEXT_STOP_REASON_TOOL_CALL",
		4: "TEXT_TO_TEXT_STOP_REASON_STOP_SEQUENCE",
		5: "TEXT_TO_TEXT_STOP_REASON_PAUSE_TURN",
		6: "TEXT_TO_TEXT_STOP_REASON_REFUSAL",
	}
	TextToTextStopReason_value = map[string]int32{
		"TEXT_TO_TEXT_STOP_REASON_UNSPECIFIED":   0,
		"TEXT_TO_TEXT_STOP_REASON_END_TURN":      1,
		"TEXT_TO_TEXT_STOP_REASON_MAX_TOKENS":    2,
		"TEXT_TO_TEXT_STOP_REASON_TOOL_CALL":     3,
		"TEXT_TO_TEXT_STOP_REASON_STOP_SEQUENCE": 4,
		"TEXT_TO_TEXT_STOP_REASON_PAUSE_TURN":    5,
		"TEXT_TO_TEXT_STOP_REASON_REFUSAL":       6,
	}
)

func (x TextToTextStopReason) Enum() *TextToTextStopReason {
	p := new(TextToTextStopReason)
	*p = x
	return p
}

func (x TextToTextStopReason) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (TextToTextStopReason) Descriptor() protoreflect.EnumDescriptor {
	return file_malonaz_ai_ai_service_v1_ai_service_proto_enumTypes[0].Descriptor()
}

func (TextToTextStopReason) Type() protoreflect.EnumType {
	return &file_malonaz_ai_ai_service_v1_ai_service_proto_enumTypes[0]
}

func (x TextToTextStopReason) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Request message for Ai.CreateModel.
type CreateModelRequest struct {
	state protoimpl.MessageState `protogen:"hybrid.v1"`
	// The resource name of the parent provider for which this model will be created.
	// Format: providers/{provider}
	Parent string `protobuf:"bytes,1,opt,name=parent,proto3" json:"parent,omitempty"`
	// The model to create.
	Model *v1.Model `protobuf:"bytes,2,opt,name=model,proto3" json:"model,omitempty"`
	// The ID to use for the resource, which will become the final component of
	// the resource name.
	//
	// This value should be 4-63 characters, and valid characters
	// are /[a-z][0-9]-/.
	ModelId       string `protobuf:"bytes,3,opt,name=model_id,json=modelId,proto3" json:"model_id,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *CreateModelRequest) Reset() {
	*x = CreateModelRequest{}
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[0]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *CreateModelRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*CreateModelRequest) ProtoMessage() {}

func (x *CreateModelRequest) ProtoReflect() protoreflect.Message {
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[0]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

func (x *CreateModelRequest) GetParent() string {
	if x != nil {
		return x.Parent
	}
	return ""
}

func (x *CreateModelRequest) GetModel() *v1.Model {
	if x != nil {
		return x.Model
	}
	return nil
}

func (x *CreateModelRequest) GetModelId() string {
	if x != nil {
		return x.ModelId
	}
	return ""
}

func (x *CreateModelRequest) SetParent(v string) {
	x.Parent = v
}

func (x *CreateModelRequest) SetModel(v *v1.Model) {
	x.Model = v
}

func (x *CreateModelRequest) SetModelId(v string) {
	x.ModelId = v
}

func (x *CreateModelRequest) HasModel() bool {
	if x == nil {
		return false
	}
	return x.Model != nil
}

func (x *CreateModelRequest) ClearModel() {
	x.Model = nil
}

type CreateModelRequest_builder struct {
	_ [0]func() // Prevents comparability and use of unkeyed literals for the builder.

	// The resource name of the parent provider for which this model will be created.
	// Format: providers/{provider}
	Parent string
	// The model to create.
	Model *v1.Model
	// The ID to use for the resource, which will become the final component of
	// the resource name.
	//
	// This value should be 4-63 characters, and valid characters
	// are /[a-z][0-9]-/.
	ModelId string
}

func (b0 CreateModelRequest_builder) Build() *CreateModelRequest {
	m0 := &CreateModelRequest{}
	b, x := &b0, m0
	_, _ = b, x
	x.Parent = b.Parent
	x.Model = b.Model
	x.ModelId = b.ModelId
	return m0
}

// Request message for Ai.GetModel.
type GetModelRequest struct {
	state protoimpl.MessageState `protogen:"hybrid.v1"`
	// The resource name of the model to retrieve.
	// Format: providers/{provider}/models/{model}
	Name          string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *GetModelRequest) Reset() {
	*x = GetModelRequest{}
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[1]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GetModelRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GetModelRequest) ProtoMessage() {}

func (x *GetModelRequest) ProtoReflect() protoreflect.Message {
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[1]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

func (x *GetModelRequest) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *GetModelRequest) SetName(v string) {
	x.Name = v
}

type GetModelRequest_builder struct {
	_ [0]func() // Prevents comparability and use of unkeyed literals for the builder.

	// The resource name of the model to retrieve.
	// Format: providers/{provider}/models/{model}
	Name string
}

func (b0 GetModelRequest_builder) Build() *GetModelRequest {
	m0 := &GetModelRequest{}
	b, x := &b0, m0
	_, _ = b, x
	x.Name = b.Name
	return m0
}

// Request message for Ai.ListModels.
type ListModelsRequest struct {
	state protoimpl.MessageState `protogen:"hybrid.v1"`
	// The resource name of the parent, which owns this collection of models.
	// Format: providers/{provider}
	Parent string `protobuf:"bytes,1,opt,name=parent,proto3" json:"parent,omitempty"`
	// Requested page size. Server may return fewer models than requested.
	// If unspecified, server will pick an appropriate default.
	PageSize int32 `protobuf:"varint,2,opt,name=page_size,json=pageSize,proto3" json:"page_size,omitempty"`
	// A token identifying a page of results the server should return.
	// This is the value of
	// [ListModelsResponse.next_page_token][ai.ai_service.v1.ListModelsResponse.next_page_token]
	// returned from the previous call to `ListModels` method.
	PageToken     string `protobuf:"bytes,3,opt,name=page_token,json=pageToken,proto3" json:"page_token,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ListModelsRequest) Reset() {
	*x = ListModelsRequest{}
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[2]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ListModelsRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ListModelsRequest) ProtoMessage() {}

func (x *ListModelsRequest) ProtoReflect() protoreflect.Message {
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[2]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

func (x *ListModelsRequest) GetParent() string {
	if x != nil {
		return x.Parent
	}
	return ""
}

func (x *ListModelsRequest) GetPageSize() int32 {
	if x != nil {
		return x.PageSize
	}
	return 0
}

func (x *ListModelsRequest) GetPageToken() string {
	if x != nil {
		return x.PageToken
	}
	return ""
}

func (x *ListModelsRequest) SetParent(v string) {
	x.Parent = v
}

func (x *ListModelsRequest) SetPageSize(v int32) {
	x.PageSize = v
}

func (x *ListModelsRequest) SetPageToken(v string) {
	x.PageToken = v
}

type ListModelsRequest_builder struct {
	_ [0]func() // Prevents comparability and use of unkeyed literals for the builder.

	// The resource name of the parent, which owns this collection of models.
	// Format: providers/{provider}
	Parent string
	// Requested page size. Server may return fewer models than requested.
	// If unspecified, server will pick an appropriate default.
	PageSize int32
	// A token identifying a page of results the server should return.
	// This is the value of
	// [ListModelsResponse.next_page_token][ai.ai_service.v1.ListModelsResponse.next_page_token]
	// returned from the previous call to `ListModels` method.
	PageToken string
}

func (b0 ListModelsRequest_builder) Build() *ListModelsRequest {
	m0 := &ListModelsRequest{}
	b, x := &b0, m0
	_, _ = b, x
	x.Parent = b.Parent
	x.PageSize = b.PageSize
	x.PageToken = b.PageToken
	return m0
}

// Response message for Ai.ListModels.
type ListModelsResponse struct {
	state protoimpl.MessageState `protogen:"hybrid.v1"`
	// The list of models.
	Models []*v1.Model `protobuf:"bytes,1,rep,name=models,proto3" json:"models,omitempty"`
	// A token to retrieve next page of results. Pass this value in the
	// [ListModelsRequest.page_token][ai.ai_service.v1.ListModelsRequest.page_token]
	// field in the subsequent call to `ListModels` method to retrieve the next
	// page of results.
	NextPageToken string `protobuf:"bytes,2,opt,name=next_page_token,json=nextPageToken,proto3" json:"next_page_token,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ListModelsResponse) Reset() {
	*x = ListModelsResponse{}
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[3]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ListModelsResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ListModelsResponse) ProtoMessage() {}

func (x *ListModelsResponse) ProtoReflect() protoreflect.Message {
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[3]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

func (x *ListModelsResponse) GetModels() []*v1.Model {
	if x != nil {
		return x.Models
	}
	return nil
}

func (x *ListModelsResponse) GetNextPageToken() string {
	if x != nil {
		return x.NextPageToken
	}
	return ""
}

func (x *ListModelsResponse) SetModels(v []*v1.Model) {
	x.Models = v
}

func (x *ListModelsResponse) SetNextPageToken(v string) {
	x.NextPageToken = v
}

type ListModelsResponse_builder struct {
	_ [0]func() // Prevents comparability and use of unkeyed literals for the builder.

	// The list of models.
	Models []*v1.Model
	// A token to retrieve next page of results. Pass this value in the
	// [ListModelsRequest.page_token][ai.ai_service.v1.ListModelsRequest.page_token]
	// field in the subsequent call to `ListModels` method to retrieve the next
	// page of results.
	NextPageToken string
}

func (b0 ListModelsResponse_builder) Build() *ListModelsResponse {
	m0 := &ListModelsResponse{}
	b, x := &b0, m0
	_, _ = b, x
	x.Models = b.Models
	x.NextPageToken = b.NextPageToken
	return m0
}

// Request message for Ai.CreateVoice.
type CreateVoiceRequest struct {
	state protoimpl.MessageState `protogen:"hybrid.v1"`
	// The voice to create.
	Voice *v1.Voice `protobuf:"bytes,1,opt,name=voice,proto3" json:"voice,omitempty"`
	// The ID to use for the resource, which will become the final component of
	// the resource name.
	//
	// This value should be 4-63 characters, and valid characters
	// are /[a-z][0-9]-/.
	VoiceId       string `protobuf:"bytes,2,opt,name=voice_id,json=voiceId,proto3" json:"voice_id,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *CreateVoiceRequest) Reset() {
	*x = CreateVoiceRequest{}
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[4]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *CreateVoiceRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*CreateVoiceRequest) ProtoMessage() {}

func (x *CreateVoiceRequest) ProtoReflect() protoreflect.Message {
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[4]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

func (x *CreateVoiceRequest) GetVoice() *v1.Voice {
	if x != nil {
		return x.Voice
	}
	return nil
}

func (x *CreateVoiceRequest) GetVoiceId() string {
	if x != nil {
		return x.VoiceId
	}
	return ""
}

func (x *CreateVoiceRequest) SetVoice(v *v1.Voice) {
	x.Voice = v
}

func (x *CreateVoiceRequest) SetVoiceId(v string) {
	x.VoiceId = v
}

func (x *CreateVoiceRequest) HasVoice() bool {
	if x == nil {
		return false
	}
	return x.Voice != nil
}

func (x *CreateVoiceRequest) ClearVoice() {
	x.Voice = nil
}

type CreateVoiceRequest_builder struct {
	_ [0]func() // Prevents comparability and use of unkeyed literals for the builder.

	// The voice to create.
	Voice *v1.Voice
	// The ID to use for the resource, which will become the final component of
	// the resource name.
	//
	// This value should be 4-63 characters, and valid characters
	// are /[a-z][0-9]-/.
	VoiceId string
}

func (b0 CreateVoiceRequest_builder) Build() *CreateVoiceRequest {
	m0 := &CreateVoiceRequest{}
	b, x := &b0, m0
	_, _ = b, x
	x.Voice = b.Voice
	x.VoiceId = b.VoiceId
	return m0
}

// Request message for Ai.GetVoice.
type GetVoiceRequest struct {
	state protoimpl.MessageState `protogen:"hybrid.v1"`
	// The resource name of the voice to retrieve.
	// Format: providers/{provider}/voices/{voice}
	Name          string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *GetVoiceRequest) Reset() {
	*x = GetVoiceRequest{}
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[5]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *GetVoiceRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*GetVoiceRequest) ProtoMessage() {}

func (x *GetVoiceRequest) ProtoReflect() protoreflect.Message {
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[5]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

func (x *GetVoiceRequest) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *GetVoiceRequest) SetName(v string) {
	x.Name = v
}

type GetVoiceRequest_builder struct {
	_ [0]func() // Prevents comparability and use of unkeyed literals for the builder.

	// The resource name of the voice to retrieve.
	// Format: providers/{provider}/voices/{voice}
	Name string
}

func (b0 GetVoiceRequest_builder) Build() *GetVoiceRequest {
	m0 := &GetVoiceRequest{}
	b, x := &b0, m0
	_, _ = b, x
	x.Name = b.Name
	return m0
}

// Request message for Ai.ListVoices.
type ListVoicesRequest struct {
	state protoimpl.MessageState `protogen:"hybrid.v1"`
	// Requested page size. Server may return fewer voices than requested.
	// If unspecified, server will pick an appropriate default.
	PageSize int32 `protobuf:"varint,1,opt,name=page_size,json=pageSize,proto3" json:"page_size,omitempty"`
	// A token identifying a page of results the server should return.
	// This is the value of
	// [ListVoicesResponse.next_page_token][ai.ai_service.v1.ListVoicesResponse.next_page_token]
	// returned from the previous call to `ListVoices` method.
	PageToken     string `protobuf:"bytes,2,opt,name=page_token,json=pageToken,proto3" json:"page_token,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ListVoicesRequest) Reset() {
	*x = ListVoicesRequest{}
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[6]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ListVoicesRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ListVoicesRequest) ProtoMessage() {}

func (x *ListVoicesRequest) ProtoReflect() protoreflect.Message {
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[6]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

func (x *ListVoicesRequest) GetPageSize() int32 {
	if x != nil {
		return x.PageSize
	}
	return 0
}

func (x *ListVoicesRequest) GetPageToken() string {
	if x != nil {
		return x.PageToken
	}
	return ""
}

func (x *ListVoicesRequest) SetPageSize(v int32) {
	x.PageSize = v
}

func (x *ListVoicesRequest) SetPageToken(v string) {
	x.PageToken = v
}

type ListVoicesRequest_builder struct {
	_ [0]func() // Prevents comparability and use of unkeyed literals for the builder.

	// Requested page size. Server may return fewer voices than requested.
	// If unspecified, server will pick an appropriate default.
	PageSize int32
	// A token identifying a page of results the server should return.
	// This is the value of
	// [ListVoicesResponse.next_page_token][ai.ai_service.v1.ListVoicesResponse.next_page_token]
	// returned from the previous call to `ListVoices` method.
	PageToken string
}

func (b0 ListVoicesRequest_builder) Build() *ListVoicesRequest {
	m0 := &ListVoicesRequest{}
	b, x := &b0, m0
	_, _ = b, x
	x.PageSize = b.PageSize
	x.PageToken = b.PageToken
	return m0
}

// Response message for Ai.ListVoices.
type ListVoicesResponse struct {
	state protoimpl.MessageState `protogen:"hybrid.v1"`
	// The list of voices.
	Voices []*v1.Voice `protobuf:"bytes,1,rep,name=voices,proto3" json:"voices,omitempty"`
	// A token to retrieve next page of results. Pass this value in the
	// [ListVoicesRequest.page_token][ai.ai_service.v1.ListVoicesRequest.page_token]
	// field in the subsequent call to `ListVoices` method to retrieve the next
	// page of results.
	NextPageToken string `protobuf:"bytes,2,opt,name=next_page_token,json=nextPageToken,proto3" json:"next_page_token,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ListVoicesResponse) Reset() {
	*x = ListVoicesResponse{}
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[7]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ListVoicesResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ListVoicesResponse) ProtoMessage() {}

func (x *ListVoicesResponse) ProtoReflect() protoreflect.Message {
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[7]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

func (x *ListVoicesResponse) GetVoices() []*v1.Voice {
	if x != nil {
		return x.Voices
	}
	return nil
}

func (x *ListVoicesResponse) GetNextPageToken() string {
	if x != nil {
		return x.NextPageToken
	}
	return ""
}

func (x *ListVoicesResponse) SetVoices(v []*v1.Voice) {
	x.Voices = v
}

func (x *ListVoicesResponse) SetNextPageToken(v string) {
	x.NextPageToken = v
}

type ListVoicesResponse_builder struct {
	_ [0]func() // Prevents comparability and use of unkeyed literals for the builder.

	// The list of voices.
	Voices []*v1.Voice
	// A token to retrieve next page of results. Pass this value in the
	// [ListVoicesRequest.page_token][ai.ai_service.v1.ListVoicesRequest.page_token]
	// field in the subsequent call to `ListVoices` method to retrieve the next
	// page of results.
	NextPageToken string
}

func (b0 ListVoicesResponse_builder) Build() *ListVoicesResponse {
	m0 := &ListVoicesResponse{}
	b, x := &b0, m0
	_, _ = b, x
	x.Voices = b.Voices
	x.NextPageToken = b.NextPageToken
	return m0
}

// Request message for Ai.SpeechToText.
type SpeechToTextRequest struct {
	state protoimpl.MessageState `protogen:"hybrid.v1"`
	// The resource name of the model used.
	// Format: providers/{provider}/models/{model}
	Model string `protobuf:"bytes,1,opt,name=model,proto3" json:"model,omitempty"`
	// Audio format of the audio.
	AudioFormat *v11.Format `protobuf:"bytes,2,opt,name=audio_format,json=audioFormat,proto3" json:"audio_format,omitempty"`
	// Audio to transcribe.
	AudioChunk *v11.Chunk `protobuf:"bytes,3,opt,name=audio_chunk,json=audioChunk,proto3" json:"audio_chunk,omitempty"`
	// Optional language code to improve transcription accuracy (e.g., "en", "es").
	LanguageCode  string `protobuf:"bytes,4,opt,name=language_code,json=languageCode,proto3" json:"language_code,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *SpeechToTextRequest) Reset() {
	*x = SpeechToTextRequest{}
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[8]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *SpeechToTextRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*SpeechToTextRequest) ProtoMessage() {}

func (x *SpeechToTextRequest) ProtoReflect() protoreflect.Message {
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[8]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

func (x *SpeechToTextRequest) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *SpeechToTextRequest) GetAudioFormat() *v11.Format {
	if x != nil {
		return x.AudioFormat
	}
	return nil
}

func (x *SpeechToTextRequest) GetAudioChunk() *v11.Chunk {
	if x != nil {
		return x.AudioChunk
	}
	return nil
}

func (x *SpeechToTextRequest) GetLanguageCode() string {
	if x != nil {
		return x.LanguageCode
	}
	return ""
}

func (x *SpeechToTextRequest) SetModel(v string) {
	x.Model = v
}

func (x *SpeechToTextRequest) SetAudioFormat(v *v11.Format) {
	x.AudioFormat = v
}

func (x *SpeechToTextRequest) SetAudioChunk(v *v11.Chunk) {
	x.AudioChunk = v
}

func (x *SpeechToTextRequest) SetLanguageCode(v string) {
	x.LanguageCode = v
}

func (x *SpeechToTextRequest) HasAudioFormat() bool {
	if x == nil {
		return false
	}
	return x.AudioFormat != nil
}

func (x *SpeechToTextRequest) HasAudioChunk() bool {
	if x == nil {
		return false
	}
	return x.AudioChunk != nil
}

func (x *SpeechToTextRequest) ClearAudioFormat() {
	x.AudioFormat = nil
}

func (x *SpeechToTextRequest) ClearAudioChunk() {
	x.AudioChunk = nil
}

type SpeechToTextRequest_builder struct {
	_ [0]func() // Prevents comparability and use of unkeyed literals for the builder.

	// The resource name of the model used.
	// Format: providers/{provider}/models/{model}
	Model string
	// Audio format of the audio.
	AudioFormat *v11.Format
	// Audio to transcribe.
	AudioChunk *v11.Chunk
	// Optional language code to improve transcription accuracy (e.g., "en", "es").
	LanguageCode string
}

func (b0 SpeechToTextRequest_builder) Build() *SpeechToTextRequest {
	m0 := &SpeechToTextRequest{}
	b, x := &b0, m0
	_, _ = b, x
	x.Model = b.Model
	x.AudioFormat = b.AudioFormat
	x.AudioChunk = b.AudioChunk
	x.LanguageCode = b.LanguageCode
	return m0
}

// Response message for Ai.SpeechToText.
type SpeechToTextResponse struct {
	state protoimpl.MessageState `protogen:"hybrid.v1"`
	// The transcribed text.
	Transcript string `protobuf:"bytes,1,opt,name=transcript,proto3" json:"transcript,omitempty"`
	// Model usage metrics.
	ModelUsage *v1.ModelUsage `protobuf:"bytes,2,opt,name=model_usage,json=modelUsage,proto3" json:"model_usage,omitempty"`
	// Generation metrics.
	GenerationMetrics *v1.GenerationMetrics `protobuf:"bytes,3,opt,name=generation_metrics,json=generationMetrics,proto3" json:"generation_metrics,omitempty"`
	unknownFields     protoimpl.UnknownFields
	sizeCache         protoimpl.SizeCache
}

func (x *SpeechToTextResponse) Reset() {
	*x = SpeechToTextResponse{}
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[9]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *SpeechToTextResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*SpeechToTextResponse) ProtoMessage() {}

func (x *SpeechToTextResponse) ProtoReflect() protoreflect.Message {
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[9]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

func (x *SpeechToTextResponse) GetTranscript() string {
	if x != nil {
		return x.Transcript
	}
	return ""
}

func (x *SpeechToTextResponse) GetModelUsage() *v1.ModelUsage {
	if x != nil {
		return x.ModelUsage
	}
	return nil
}

func (x *SpeechToTextResponse) GetGenerationMetrics() *v1.GenerationMetrics {
	if x != nil {
		return x.GenerationMetrics
	}
	return nil
}

func (x *SpeechToTextResponse) SetTranscript(v string) {
	x.Transcript = v
}

func (x *SpeechToTextResponse) SetModelUsage(v *v1.ModelUsage) {
	x.ModelUsage = v
}

func (x *SpeechToTextResponse) SetGenerationMetrics(v *v1.GenerationMetrics) {
	x.GenerationMetrics = v
}

func (x *SpeechToTextResponse) HasModelUsage() bool {
	if x == nil {
		return false
	}
	return x.ModelUsage != nil
}

func (x *SpeechToTextResponse) HasGenerationMetrics() bool {
	if x == nil {
		return false
	}
	return x.GenerationMetrics != nil
}

func (x *SpeechToTextResponse) ClearModelUsage() {
	x.ModelUsage = nil
}

func (x *SpeechToTextResponse) ClearGenerationMetrics() {
	x.GenerationMetrics = nil
}

type SpeechToTextResponse_builder struct {
	_ [0]func() // Prevents comparability and use of unkeyed literals for the builder.

	// The transcribed text.
	Transcript string
	// Model usage metrics.
	ModelUsage *v1.ModelUsage
	// Generation metrics.
	GenerationMetrics *v1.GenerationMetrics
}

func (b0 SpeechToTextResponse_builder) Build() *SpeechToTextResponse {
	m0 := &SpeechToTextResponse{}
	b, x := &b0, m0
	_, _ = b, x
	x.Transcript = b.Transcript
	x.ModelUsage = b.ModelUsage
	x.GenerationMetrics = b.GenerationMetrics
	return m0
}

// Configuration for text to text generation.
type TextToTextConfiguration struct {
	state protoimpl.MessageState `protogen:"hybrid.v1"`
	// Maximum number of tokens to generate. Includes reasoning tokens.
	MaxTokens int32 `protobuf:"varint,1,opt,name=max_tokens,json=maxTokens,proto3" json:"max_tokens,omitempty"`
	// Sampling temperature (0.0 to 2.0).
	Temperature float64 `protobuf:"fixed64,2,opt,name=temperature,proto3" json:"temperature,omitempty"`
	// Controls which tool(s) the model should use.
	ToolChoice *v1.ToolChoice `protobuf:"bytes,3,opt,name=tool_choice,json=toolChoice,proto3" json:"tool_choice,omitempty"`
	// Represents the level of reasoning effort for AI model responses.
	// The reasoning effort parameter guides the model on how many reasoning tokens
	// to generate before creating a response to the prompt. Higher effort levels
	// result in more thorough reasoning at the cost of speed and token usage.
	ReasoningEffort v1.ReasoningEffort `protobuf:"varint,4,opt,name=reasoning_effort,json=reasoningEffort,proto3,enum=malonaz.ai.v1.ReasoningEffort" json:"reasoning_effort,omitempty"`
	// If true, we attempt to clean the output to extract a json object.
	// Fails the request if a json object cannot be found.
	ExtractJsonObject bool `protobuf:"varint,5,opt,name=extract_json_object,json=extractJsonObject,proto3" json:"extract_json_object,omitempty"`
	// If true, we stream partial tool calls.
	StreamPartialToolCalls bool `protobuf:"varint,6,opt,name=stream_partial_tool_calls,json=streamPartialToolCalls,proto3" json:"stream_partial_tool_calls,omitempty"`
	unknownFields          protoimpl.UnknownFields
	sizeCache              protoimpl.SizeCache
}

func (x *TextToTextConfiguration) Reset() {
	*x = TextToTextConfiguration{}
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[10]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TextToTextConfiguration) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TextToTextConfiguration) ProtoMessage() {}

func (x *TextToTextConfiguration) ProtoReflect() protoreflect.Message {
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[10]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

func (x *TextToTextConfiguration) GetMaxTokens() int32 {
	if x != nil {
		return x.MaxTokens
	}
	return 0
}

func (x *TextToTextConfiguration) GetTemperature() float64 {
	if x != nil {
		return x.Temperature
	}
	return 0
}

func (x *TextToTextConfiguration) GetToolChoice() *v1.ToolChoice {
	if x != nil {
		return x.ToolChoice
	}
	return nil
}

func (x *TextToTextConfiguration) GetReasoningEffort() v1.ReasoningEffort {
	if x != nil {
		return x.ReasoningEffort
	}
	return v1.ReasoningEffort(0)
}

func (x *TextToTextConfiguration) GetExtractJsonObject() bool {
	if x != nil {
		return x.ExtractJsonObject
	}
	return false
}

func (x *TextToTextConfiguration) GetStreamPartialToolCalls() bool {
	if x != nil {
		return x.StreamPartialToolCalls
	}
	return false
}

func (x *TextToTextConfiguration) SetMaxTokens(v int32) {
	x.MaxTokens = v
}

func (x *TextToTextConfiguration) SetTemperature(v float64) {
	x.Temperature = v
}

func (x *TextToTextConfiguration) SetToolChoice(v *v1.ToolChoice) {
	x.ToolChoice = v
}

func (x *TextToTextConfiguration) SetReasoningEffort(v v1.ReasoningEffort) {
	x.ReasoningEffort = v
}

func (x *TextToTextConfiguration) SetExtractJsonObject(v bool) {
	x.ExtractJsonObject = v
}

func (x *TextToTextConfiguration) SetStreamPartialToolCalls(v bool) {
	x.StreamPartialToolCalls = v
}

func (x *TextToTextConfiguration) HasToolChoice() bool {
	if x == nil {
		return false
	}
	return x.ToolChoice != nil
}

func (x *TextToTextConfiguration) ClearToolChoice() {
	x.ToolChoice = nil
}

type TextToTextConfiguration_builder struct {
	_ [0]func() // Prevents comparability and use of unkeyed literals for the builder.

	// Maximum number of tokens to generate. Includes reasoning tokens.
	MaxTokens int32
	// Sampling temperature (0.0 to 2.0).
	Temperature float64
	// Controls which tool(s) the model should use.
	ToolChoice *v1.ToolChoice
	// Represents the level of reasoning effort for AI model responses.
	// The reasoning effort parameter guides the model on how many reasoning tokens
	// to generate before creating a response to the prompt. Higher effort levels
	// result in more thorough reasoning at the cost of speed and token usage.
	ReasoningEffort v1.ReasoningEffort
	// If true, we attempt to clean the output to extract a json object.
	// Fails the request if a json object cannot be found.
	ExtractJsonObject bool
	// If true, we stream partial tool calls.
	StreamPartialToolCalls bool
}

func (b0 TextToTextConfiguration_builder) Build() *TextToTextConfiguration {
	m0 := &TextToTextConfiguration{}
	b, x := &b0, m0
	_, _ = b, x
	x.MaxTokens = b.MaxTokens
	x.Temperature = b.Temperature
	x.ToolChoice = b.ToolChoice
	x.ReasoningEffort = b.ReasoningEffort
	x.ExtractJsonObject = b.ExtractJsonObject
	x.StreamPartialToolCalls = b.StreamPartialToolCalls
	return m0
}

// Request message for Ai.TextToText.
type TextToTextRequest struct {
	state protoimpl.MessageState `protogen:"hybrid.v1"`
	// The resource name of the model used.
	// Format: providers/{provider}/models/{model}
	Model string `protobuf:"bytes,1,opt,name=model,proto3" json:"model,omitempty"`
	// The conversation messages.
	Messages []*v1.Message `protobuf:"bytes,2,rep,name=messages,proto3" json:"messages,omitempty"`
	// Tools available for the model to call.
	Tools []*v1.Tool `protobuf:"bytes,3,rep,name=tools,proto3" json:"tools,omitempty"`
	// Additional configuration.
	Configuration *TextToTextConfiguration `protobuf:"bytes,4,opt,name=configuration,proto3" json:"configuration,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *TextToTextRequest) Reset() {
	*x = TextToTextRequest{}
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[11]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TextToTextRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TextToTextRequest) ProtoMessage() {}

func (x *TextToTextRequest) ProtoReflect() protoreflect.Message {
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[11]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

func (x *TextToTextRequest) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *TextToTextRequest) GetMessages() []*v1.Message {
	if x != nil {
		return x.Messages
	}
	return nil
}

func (x *TextToTextRequest) GetTools() []*v1.Tool {
	if x != nil {
		return x.Tools
	}
	return nil
}

func (x *TextToTextRequest) GetConfiguration() *TextToTextConfiguration {
	if x != nil {
		return x.Configuration
	}
	return nil
}

func (x *TextToTextRequest) SetModel(v string) {
	x.Model = v
}

func (x *TextToTextRequest) SetMessages(v []*v1.Message) {
	x.Messages = v
}

func (x *TextToTextRequest) SetTools(v []*v1.Tool) {
	x.Tools = v
}

func (x *TextToTextRequest) SetConfiguration(v *TextToTextConfiguration) {
	x.Configuration = v
}

func (x *TextToTextRequest) HasConfiguration() bool {
	if x == nil {
		return false
	}
	return x.Configuration != nil
}

func (x *TextToTextRequest) ClearConfiguration() {
	x.Configuration = nil
}

type TextToTextRequest_builder struct {
	_ [0]func() // Prevents comparability and use of unkeyed literals for the builder.

	// The resource name of the model used.
	// Format: providers/{provider}/models/{model}
	Model string
	// The conversation messages.
	Messages []*v1.Message
	// Tools available for the model to call.
	Tools []*v1.Tool
	// Additional configuration.
	Configuration *TextToTextConfiguration
}

func (b0 TextToTextRequest_builder) Build() *TextToTextRequest {
	m0 := &TextToTextRequest{}
	b, x := &b0, m0
	_, _ = b, x
	x.Model = b.Model
	x.Messages = b.Messages
	x.Tools = b.Tools
	x.Configuration = b.Configuration
	return m0
}

// Response message for Ai.TextToText.
type TextToTextResponse struct {
	state protoimpl.MessageState `protogen:"hybrid.v1"`
	// The generated message.
	Message *v1.Message `protobuf:"bytes,1,opt,name=message,proto3" json:"message,omitempty"`
	// Reason why generation stopped.
	StopReason TextToTextStopReason `protobuf:"varint,2,opt,name=stop_reason,json=stopReason,proto3,enum=malonaz.ai.ai_service.v1.TextToTextStopReason" json:"stop_reason,omitempty"`
	// Model usage metrics.
	ModelUsage *v1.ModelUsage `protobuf:"bytes,3,opt,name=model_usage,json=modelUsage,proto3" json:"model_usage,omitempty"`
	// Generation metrics.
	GenerationMetrics *v1.GenerationMetrics `protobuf:"bytes,4,opt,name=generation_metrics,json=generationMetrics,proto3" json:"generation_metrics,omitempty"`
	unknownFields     protoimpl.UnknownFields
	sizeCache         protoimpl.SizeCache
}

func (x *TextToTextResponse) Reset() {
	*x = TextToTextResponse{}
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[12]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TextToTextResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TextToTextResponse) ProtoMessage() {}

func (x *TextToTextResponse) ProtoReflect() protoreflect.Message {
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[12]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

func (x *TextToTextResponse) GetMessage() *v1.Message {
	if x != nil {
		return x.Message
	}
	return nil
}

func (x *TextToTextResponse) GetStopReason() TextToTextStopReason {
	if x != nil {
		return x.StopReason
	}
	return TextToTextStopReason_TEXT_TO_TEXT_STOP_REASON_UNSPECIFIED
}

func (x *TextToTextResponse) GetModelUsage() *v1.ModelUsage {
	if x != nil {
		return x.ModelUsage
	}
	return nil
}

func (x *TextToTextResponse) GetGenerationMetrics() *v1.GenerationMetrics {
	if x != nil {
		return x.GenerationMetrics
	}
	return nil
}

func (x *TextToTextResponse) SetMessage(v *v1.Message) {
	x.Message = v
}

func (x *TextToTextResponse) SetStopReason(v TextToTextStopReason) {
	x.StopReason = v
}

func (x *TextToTextResponse) SetModelUsage(v *v1.ModelUsage) {
	x.ModelUsage = v
}

func (x *TextToTextResponse) SetGenerationMetrics(v *v1.GenerationMetrics) {
	x.GenerationMetrics = v
}

func (x *TextToTextResponse) HasMessage() bool {
	if x == nil {
		return false
	}
	return x.Message != nil
}

func (x *TextToTextResponse) HasModelUsage() bool {
	if x == nil {
		return false
	}
	return x.ModelUsage != nil
}

func (x *TextToTextResponse) HasGenerationMetrics() bool {
	if x == nil {
		return false
	}
	return x.GenerationMetrics != nil
}

func (x *TextToTextResponse) ClearMessage() {
	x.Message = nil
}

func (x *TextToTextResponse) ClearModelUsage() {
	x.ModelUsage = nil
}

func (x *TextToTextResponse) ClearGenerationMetrics() {
	x.GenerationMetrics = nil
}

type TextToTextResponse_builder struct {
	_ [0]func() // Prevents comparability and use of unkeyed literals for the builder.

	// The generated message.
	Message *v1.Message
	// Reason why generation stopped.
	StopReason TextToTextStopReason
	// Model usage metrics.
	ModelUsage *v1.ModelUsage
	// Generation metrics.
	GenerationMetrics *v1.GenerationMetrics
}

func (b0 TextToTextResponse_builder) Build() *TextToTextResponse {
	m0 := &TextToTextResponse{}
	b, x := &b0, m0
	_, _ = b, x
	x.Message = b.Message
	x.StopReason = b.StopReason
	x.ModelUsage = b.ModelUsage
	x.GenerationMetrics = b.GenerationMetrics
	return m0
}

// Request message for Ai.TextToTextStream.
type TextToTextStreamRequest struct {
	state protoimpl.MessageState `protogen:"hybrid.v1"`
	// The resource name of the model used.
	// Format: providers/{provider}/models/{model}
	Model string `protobuf:"bytes,1,opt,name=model,proto3" json:"model,omitempty"`
	// The conversation messages.
	Messages []*v1.Message `protobuf:"bytes,2,rep,name=messages,proto3" json:"messages,omitempty"`
	// Tools available for the model to call.
	Tools []*v1.Tool `protobuf:"bytes,3,rep,name=tools,proto3" json:"tools,omitempty"`
	// For the model to use a tool.
	ToolChoice string `protobuf:"bytes,4,opt,name=tool_choice,json=toolChoice,proto3" json:"tool_choice,omitempty"`
	// Additional configuration.
	Configuration *TextToTextConfiguration `protobuf:"bytes,5,opt,name=configuration,proto3" json:"configuration,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *TextToTextStreamRequest) Reset() {
	*x = TextToTextStreamRequest{}
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[13]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TextToTextStreamRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TextToTextStreamRequest) ProtoMessage() {}

func (x *TextToTextStreamRequest) ProtoReflect() protoreflect.Message {
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[13]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

func (x *TextToTextStreamRequest) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *TextToTextStreamRequest) GetMessages() []*v1.Message {
	if x != nil {
		return x.Messages
	}
	return nil
}

func (x *TextToTextStreamRequest) GetTools() []*v1.Tool {
	if x != nil {
		return x.Tools
	}
	return nil
}

func (x *TextToTextStreamRequest) GetToolChoice() string {
	if x != nil {
		return x.ToolChoice
	}
	return ""
}

func (x *TextToTextStreamRequest) GetConfiguration() *TextToTextConfiguration {
	if x != nil {
		return x.Configuration
	}
	return nil
}

func (x *TextToTextStreamRequest) SetModel(v string) {
	x.Model = v
}

func (x *TextToTextStreamRequest) SetMessages(v []*v1.Message) {
	x.Messages = v
}

func (x *TextToTextStreamRequest) SetTools(v []*v1.Tool) {
	x.Tools = v
}

func (x *TextToTextStreamRequest) SetToolChoice(v string) {
	x.ToolChoice = v
}

func (x *TextToTextStreamRequest) SetConfiguration(v *TextToTextConfiguration) {
	x.Configuration = v
}

func (x *TextToTextStreamRequest) HasConfiguration() bool {
	if x == nil {
		return false
	}
	return x.Configuration != nil
}

func (x *TextToTextStreamRequest) ClearConfiguration() {
	x.Configuration = nil
}

type TextToTextStreamRequest_builder struct {
	_ [0]func() // Prevents comparability and use of unkeyed literals for the builder.

	// The resource name of the model used.
	// Format: providers/{provider}/models/{model}
	Model string
	// The conversation messages.
	Messages []*v1.Message
	// Tools available for the model to call.
	Tools []*v1.Tool
	// For the model to use a tool.
	ToolChoice string
	// Additional configuration.
	Configuration *TextToTextConfiguration
}

func (b0 TextToTextStreamRequest_builder) Build() *TextToTextStreamRequest {
	m0 := &TextToTextStreamRequest{}
	b, x := &b0, m0
	_, _ = b, x
	x.Model = b.Model
	x.Messages = b.Messages
	x.Tools = b.Tools
	x.ToolChoice = b.ToolChoice
	x.Configuration = b.Configuration
	return m0
}

// Response message for Ai.TextToTextStream.
type TextToTextStreamResponse struct {
	state protoimpl.MessageState `protogen:"hybrid.v1"`
	// Content of this response.
	//
	// Types that are valid to be assigned to Content:
	//
	//	*TextToTextStreamResponse_ContentChunk
	//	*TextToTextStreamResponse_ReasoningChunk
	//	*TextToTextStreamResponse_StopReason
	//	*TextToTextStreamResponse_ToolCall
	//	*TextToTextStreamResponse_PartialToolCall
	//	*TextToTextStreamResponse_ModelUsage
	//	*TextToTextStreamResponse_GenerationMetrics
	Content       isTextToTextStreamResponse_Content `protobuf_oneof:"content"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *TextToTextStreamResponse) Reset() {
	*x = TextToTextStreamResponse{}
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[14]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TextToTextStreamResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TextToTextStreamResponse) ProtoMessage() {}

func (x *TextToTextStreamResponse) ProtoReflect() protoreflect.Message {
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[14]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

func (x *TextToTextStreamResponse) GetContent() isTextToTextStreamResponse_Content {
	if x != nil {
		return x.Content
	}
	return nil
}

func (x *TextToTextStreamResponse) GetContentChunk() string {
	if x != nil {
		if x, ok := x.Content.(*TextToTextStreamResponse_ContentChunk); ok {
			return x.ContentChunk
		}
	}
	return ""
}

func (x *TextToTextStreamResponse) GetReasoningChunk() string {
	if x != nil {
		if x, ok := x.Content.(*TextToTextStreamResponse_ReasoningChunk); ok {
			return x.ReasoningChunk
		}
	}
	return ""
}

func (x *TextToTextStreamResponse) GetStopReason() TextToTextStopReason {
	if x != nil {
		if x, ok := x.Content.(*TextToTextStreamResponse_StopReason); ok {
			return x.StopReason
		}
	}
	return TextToTextStopReason_TEXT_TO_TEXT_STOP_REASON_UNSPECIFIED
}

func (x *TextToTextStreamResponse) GetToolCall() *v1.ToolCall {
	if x != nil {
		if x, ok := x.Content.(*TextToTextStreamResponse_ToolCall); ok {
			return x.ToolCall
		}
	}
	return nil
}

func (x *TextToTextStreamResponse) GetPartialToolCall() *v1.ToolCall {
	if x != nil {
		if x, ok := x.Content.(*TextToTextStreamResponse_PartialToolCall); ok {
			return x.PartialToolCall
		}
	}
	return nil
}

func (x *TextToTextStreamResponse) GetModelUsage() *v1.ModelUsage {
	if x != nil {
		if x, ok := x.Content.(*TextToTextStreamResponse_ModelUsage); ok {
			return x.ModelUsage
		}
	}
	return nil
}

func (x *TextToTextStreamResponse) GetGenerationMetrics() *v1.GenerationMetrics {
	if x != nil {
		if x, ok := x.Content.(*TextToTextStreamResponse_GenerationMetrics); ok {
			return x.GenerationMetrics
		}
	}
	return nil
}

func (x *TextToTextStreamResponse) SetContentChunk(v string) {
	x.Content = &TextToTextStreamResponse_ContentChunk{v}
}

func (x *TextToTextStreamResponse) SetReasoningChunk(v string) {
	x.Content = &TextToTextStreamResponse_ReasoningChunk{v}
}

func (x *TextToTextStreamResponse) SetStopReason(v TextToTextStopReason) {
	x.Content = &TextToTextStreamResponse_StopReason{v}
}

func (x *TextToTextStreamResponse) SetToolCall(v *v1.ToolCall) {
	if v == nil {
		x.Content = nil
		return
	}
	x.Content = &TextToTextStreamResponse_ToolCall{v}
}

func (x *TextToTextStreamResponse) SetPartialToolCall(v *v1.ToolCall) {
	if v == nil {
		x.Content = nil
		return
	}
	x.Content = &TextToTextStreamResponse_PartialToolCall{v}
}

func (x *TextToTextStreamResponse) SetModelUsage(v *v1.ModelUsage) {
	if v == nil {
		x.Content = nil
		return
	}
	x.Content = &TextToTextStreamResponse_ModelUsage{v}
}

func (x *TextToTextStreamResponse) SetGenerationMetrics(v *v1.GenerationMetrics) {
	if v == nil {
		x.Content = nil
		return
	}
	x.Content = &TextToTextStreamResponse_GenerationMetrics{v}
}

func (x *TextToTextStreamResponse) HasContent() bool {
	if x == nil {
		return false
	}
	return x.Content != nil
}

func (x *TextToTextStreamResponse) HasContentChunk() bool {
	if x == nil {
		return false
	}
	_, ok := x.Content.(*TextToTextStreamResponse_ContentChunk)
	return ok
}

func (x *TextToTextStreamResponse) HasReasoningChunk() bool {
	if x == nil {
		return false
	}
	_, ok := x.Content.(*TextToTextStreamResponse_ReasoningChunk)
	return ok
}

func (x *TextToTextStreamResponse) HasStopReason() bool {
	if x == nil {
		return false
	}
	_, ok := x.Content.(*TextToTextStreamResponse_StopReason)
	return ok
}

func (x *TextToTextStreamResponse) HasToolCall() bool {
	if x == nil {
		return false
	}
	_, ok := x.Content.(*TextToTextStreamResponse_ToolCall)
	return ok
}

func (x *TextToTextStreamResponse) HasPartialToolCall() bool {
	if x == nil {
		return false
	}
	_, ok := x.Content.(*TextToTextStreamResponse_PartialToolCall)
	return ok
}

func (x *TextToTextStreamResponse) HasModelUsage() bool {
	if x == nil {
		return false
	}
	_, ok := x.Content.(*TextToTextStreamResponse_ModelUsage)
	return ok
}

func (x *TextToTextStreamResponse) HasGenerationMetrics() bool {
	if x == nil {
		return false
	}
	_, ok := x.Content.(*TextToTextStreamResponse_GenerationMetrics)
	return ok
}

func (x *TextToTextStreamResponse) ClearContent() {
	x.Content = nil
}

func (x *TextToTextStreamResponse) ClearContentChunk() {
	if _, ok := x.Content.(*TextToTextStreamResponse_ContentChunk); ok {
		x.Content = nil
	}
}

func (x *TextToTextStreamResponse) ClearReasoningChunk() {
	if _, ok := x.Content.(*TextToTextStreamResponse_ReasoningChunk); ok {
		x.Content = nil
	}
}

func (x *TextToTextStreamResponse) ClearStopReason() {
	if _, ok := x.Content.(*TextToTextStreamResponse_StopReason); ok {
		x.Content = nil
	}
}

func (x *TextToTextStreamResponse) ClearToolCall() {
	if _, ok := x.Content.(*TextToTextStreamResponse_ToolCall); ok {
		x.Content = nil
	}
}

func (x *TextToTextStreamResponse) ClearPartialToolCall() {
	if _, ok := x.Content.(*TextToTextStreamResponse_PartialToolCall); ok {
		x.Content = nil
	}
}

func (x *TextToTextStreamResponse) ClearModelUsage() {
	if _, ok := x.Content.(*TextToTextStreamResponse_ModelUsage); ok {
		x.Content = nil
	}
}

func (x *TextToTextStreamResponse) ClearGenerationMetrics() {
	if _, ok := x.Content.(*TextToTextStreamResponse_GenerationMetrics); ok {
		x.Content = nil
	}
}

const TextToTextStreamResponse_Content_not_set_case case_TextToTextStreamResponse_Content = 0
const TextToTextStreamResponse_ContentChunk_case case_TextToTextStreamResponse_Content = 1
const TextToTextStreamResponse_ReasoningChunk_case case_TextToTextStreamResponse_Content = 2
const TextToTextStreamResponse_StopReason_case case_TextToTextStreamResponse_Content = 3
const TextToTextStreamResponse_ToolCall_case case_TextToTextStreamResponse_Content = 4
const TextToTextStreamResponse_PartialToolCall_case case_TextToTextStreamResponse_Content = 5
const TextToTextStreamResponse_ModelUsage_case case_TextToTextStreamResponse_Content = 6
const TextToTextStreamResponse_GenerationMetrics_case case_TextToTextStreamResponse_Content = 7

func (x *TextToTextStreamResponse) WhichContent() case_TextToTextStreamResponse_Content {
	if x == nil {
		return TextToTextStreamResponse_Content_not_set_case
	}
	switch x.Content.(type) {
	case *TextToTextStreamResponse_ContentChunk:
		return TextToTextStreamResponse_ContentChunk_case
	case *TextToTextStreamResponse_ReasoningChunk:
		return TextToTextStreamResponse_ReasoningChunk_case
	case *TextToTextStreamResponse_StopReason:
		return TextToTextStreamResponse_StopReason_case
	case *TextToTextStreamResponse_ToolCall:
		return TextToTextStreamResponse_ToolCall_case
	case *TextToTextStreamResponse_PartialToolCall:
		return TextToTextStreamResponse_PartialToolCall_case
	case *TextToTextStreamResponse_ModelUsage:
		return TextToTextStreamResponse_ModelUsage_case
	case *TextToTextStreamResponse_GenerationMetrics:
		return TextToTextStreamResponse_GenerationMetrics_case
	default:
		return TextToTextStreamResponse_Content_not_set_case
	}
}

type TextToTextStreamResponse_builder struct {
	_ [0]func() // Prevents comparability and use of unkeyed literals for the builder.

	// Content of this response.

	// Fields of oneof Content:
	// A chunk of the generated message content.
	ContentChunk *string
	// Reasoning content chunk (if model supports reasoning).
	ReasoningChunk *string
	// Reason why generation stopped.
	StopReason *TextToTextStopReason
	// Tool calls requested by the assistant (sent when complete).
	ToolCall *v1.ToolCall
	// Tool calls requested by the assistant (sent when complete).
	PartialToolCall *v1.ToolCall
	// Model usage event (sent last).
	ModelUsage *v1.ModelUsage
	// Generation metrics (sent last).
	GenerationMetrics *v1.GenerationMetrics
	// -- end of Content
}

func (b0 TextToTextStreamResponse_builder) Build() *TextToTextStreamResponse {
	m0 := &TextToTextStreamResponse{}
	b, x := &b0, m0
	_, _ = b, x
	if b.ContentChunk != nil {
		x.Content = &TextToTextStreamResponse_ContentChunk{*b.ContentChunk}
	}
	if b.ReasoningChunk != nil {
		x.Content = &TextToTextStreamResponse_ReasoningChunk{*b.ReasoningChunk}
	}
	if b.StopReason != nil {
		x.Content = &TextToTextStreamResponse_StopReason{*b.StopReason}
	}
	if b.ToolCall != nil {
		x.Content = &TextToTextStreamResponse_ToolCall{b.ToolCall}
	}
	if b.PartialToolCall != nil {
		x.Content = &TextToTextStreamResponse_PartialToolCall{b.PartialToolCall}
	}
	if b.ModelUsage != nil {
		x.Content = &TextToTextStreamResponse_ModelUsage{b.ModelUsage}
	}
	if b.GenerationMetrics != nil {
		x.Content = &TextToTextStreamResponse_GenerationMetrics{b.GenerationMetrics}
	}
	return m0
}

type case_TextToTextStreamResponse_Content protoreflect.FieldNumber

func (x case_TextToTextStreamResponse_Content) String() string {
	md := file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[14].Descriptor()
	if x == 0 {
		return "not set"
	}
	return protoimpl.X.MessageFieldStringOf(md, protoreflect.FieldNumber(x))
}

type isTextToTextStreamResponse_Content interface {
	isTextToTextStreamResponse_Content()
}

type TextToTextStreamResponse_ContentChunk struct {
	// A chunk of the generated message content.
	ContentChunk string `protobuf:"bytes,1,opt,name=content_chunk,json=contentChunk,proto3,oneof"`
}

type TextToTextStreamResponse_ReasoningChunk struct {
	// Reasoning content chunk (if model supports reasoning).
	ReasoningChunk string `protobuf:"bytes,2,opt,name=reasoning_chunk,json=reasoningChunk,proto3,oneof"`
}

type TextToTextStreamResponse_StopReason struct {
	// Reason why generation stopped.
	StopReason TextToTextStopReason `protobuf:"varint,3,opt,name=stop_reason,json=stopReason,proto3,enum=malonaz.ai.ai_service.v1.TextToTextStopReason,oneof"`
}

type TextToTextStreamResponse_ToolCall struct {
	// Tool calls requested by the assistant (sent when complete).
	ToolCall *v1.ToolCall `protobuf:"bytes,4,opt,name=tool_call,json=toolCall,proto3,oneof"`
}

type TextToTextStreamResponse_PartialToolCall struct {
	// Tool calls requested by the assistant (sent when complete).
	PartialToolCall *v1.ToolCall `protobuf:"bytes,5,opt,name=partial_tool_call,json=partialToolCall,proto3,oneof"`
}

type TextToTextStreamResponse_ModelUsage struct {
	// Model usage event (sent last).
	ModelUsage *v1.ModelUsage `protobuf:"bytes,6,opt,name=model_usage,json=modelUsage,proto3,oneof"`
}

type TextToTextStreamResponse_GenerationMetrics struct {
	// Generation metrics (sent last).
	GenerationMetrics *v1.GenerationMetrics `protobuf:"bytes,7,opt,name=generation_metrics,json=generationMetrics,proto3,oneof"`
}

func (*TextToTextStreamResponse_ContentChunk) isTextToTextStreamResponse_Content() {}

func (*TextToTextStreamResponse_ReasoningChunk) isTextToTextStreamResponse_Content() {}

func (*TextToTextStreamResponse_StopReason) isTextToTextStreamResponse_Content() {}

func (*TextToTextStreamResponse_ToolCall) isTextToTextStreamResponse_Content() {}

func (*TextToTextStreamResponse_PartialToolCall) isTextToTextStreamResponse_Content() {}

func (*TextToTextStreamResponse_ModelUsage) isTextToTextStreamResponse_Content() {}

func (*TextToTextStreamResponse_GenerationMetrics) isTextToTextStreamResponse_Content() {}

// Configuration for text to speech generation.
type TextToSpeechConfiguration struct {
	state protoimpl.MessageState `protogen:"hybrid.v1"`
	// Optional language code to improve accuracy (e.g., "en", "es").
	LanguageCode string `protobuf:"bytes,1,opt,name=language_code,json=languageCode,proto3" json:"language_code,omitempty"`
	// Preferred sample rate.e.g., 16000, 44100, 48000
	// Best effort basis, caller should inspect the `audio_format` output.
	PreferredSampleRate int32 `protobuf:"varint,2,opt,name=preferred_sample_rate,json=preferredSampleRate,proto3" json:"preferred_sample_rate,omitempty"`
	// Override provider-specific request fields.
	// See provider documentation for supported settings.
	// Any field here gets injected into the request to the provider at the root level.
	ProviderSettings *structpb.Struct `protobuf:"bytes,3,opt,name=provider_settings,json=providerSettings,proto3" json:"provider_settings,omitempty"`
	unknownFields    protoimpl.UnknownFields
	sizeCache        protoimpl.SizeCache
}

func (x *TextToSpeechConfiguration) Reset() {
	*x = TextToSpeechConfiguration{}
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[15]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TextToSpeechConfiguration) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TextToSpeechConfiguration) ProtoMessage() {}

func (x *TextToSpeechConfiguration) ProtoReflect() protoreflect.Message {
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[15]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

func (x *TextToSpeechConfiguration) GetLanguageCode() string {
	if x != nil {
		return x.LanguageCode
	}
	return ""
}

func (x *TextToSpeechConfiguration) GetPreferredSampleRate() int32 {
	if x != nil {
		return x.PreferredSampleRate
	}
	return 0
}

func (x *TextToSpeechConfiguration) GetProviderSettings() *structpb.Struct {
	if x != nil {
		return x.ProviderSettings
	}
	return nil
}

func (x *TextToSpeechConfiguration) SetLanguageCode(v string) {
	x.LanguageCode = v
}

func (x *TextToSpeechConfiguration) SetPreferredSampleRate(v int32) {
	x.PreferredSampleRate = v
}

func (x *TextToSpeechConfiguration) SetProviderSettings(v *structpb.Struct) {
	x.ProviderSettings = v
}

func (x *TextToSpeechConfiguration) HasProviderSettings() bool {
	if x == nil {
		return false
	}
	return x.ProviderSettings != nil
}

func (x *TextToSpeechConfiguration) ClearProviderSettings() {
	x.ProviderSettings = nil
}

type TextToSpeechConfiguration_builder struct {
	_ [0]func() // Prevents comparability and use of unkeyed literals for the builder.

	// Optional language code to improve accuracy (e.g., "en", "es").
	LanguageCode string
	// Preferred sample rate.e.g., 16000, 44100, 48000
	// Best effort basis, caller should inspect the `audio_format` output.
	PreferredSampleRate int32
	// Override provider-specific request fields.
	// See provider documentation for supported settings.
	// Any field here gets injected into the request to the provider at the root level.
	ProviderSettings *structpb.Struct
}

func (b0 TextToSpeechConfiguration_builder) Build() *TextToSpeechConfiguration {
	m0 := &TextToSpeechConfiguration{}
	b, x := &b0, m0
	_, _ = b, x
	x.LanguageCode = b.LanguageCode
	x.PreferredSampleRate = b.PreferredSampleRate
	x.ProviderSettings = b.ProviderSettings
	return m0
}

// Request message for Ai.TextToSpeech.
type TextToSpeechRequest struct {
	state protoimpl.MessageState `protogen:"hybrid.v1"`
	// The resource name of the model used.
	// Format: providers/{provider}/models/{model}
	Model string `protobuf:"bytes,1,opt,name=model,proto3" json:"model,omitempty"`
	// The resource name of the voice to use.
	// Format: voices/{voice}
	Voice string `protobuf:"bytes,2,opt,name=voice,proto3" json:"voice,omitempty"`
	// The provider voice id.
	ProviderVoiceId string `protobuf:"bytes,3,opt,name=provider_voice_id,json=providerVoiceId,proto3" json:"provider_voice_id,omitempty"`
	// The text to convert to speech.
	Text string `protobuf:"bytes,4,opt,name=text,proto3" json:"text,omitempty"`
	// Additional configuration.
	Configuration *TextToSpeechConfiguration `protobuf:"bytes,5,opt,name=configuration,proto3" json:"configuration,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *TextToSpeechRequest) Reset() {
	*x = TextToSpeechRequest{}
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[16]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TextToSpeechRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TextToSpeechRequest) ProtoMessage() {}

func (x *TextToSpeechRequest) ProtoReflect() protoreflect.Message {
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[16]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

func (x *TextToSpeechRequest) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *TextToSpeechRequest) GetVoice() string {
	if x != nil {
		return x.Voice
	}
	return ""
}

func (x *TextToSpeechRequest) GetProviderVoiceId() string {
	if x != nil {
		return x.ProviderVoiceId
	}
	return ""
}

func (x *TextToSpeechRequest) GetText() string {
	if x != nil {
		return x.Text
	}
	return ""
}

func (x *TextToSpeechRequest) GetConfiguration() *TextToSpeechConfiguration {
	if x != nil {
		return x.Configuration
	}
	return nil
}

func (x *TextToSpeechRequest) SetModel(v string) {
	x.Model = v
}

func (x *TextToSpeechRequest) SetVoice(v string) {
	x.Voice = v
}

func (x *TextToSpeechRequest) SetProviderVoiceId(v string) {
	x.ProviderVoiceId = v
}

func (x *TextToSpeechRequest) SetText(v string) {
	x.Text = v
}

func (x *TextToSpeechRequest) SetConfiguration(v *TextToSpeechConfiguration) {
	x.Configuration = v
}

func (x *TextToSpeechRequest) HasConfiguration() bool {
	if x == nil {
		return false
	}
	return x.Configuration != nil
}

func (x *TextToSpeechRequest) ClearConfiguration() {
	x.Configuration = nil
}

type TextToSpeechRequest_builder struct {
	_ [0]func() // Prevents comparability and use of unkeyed literals for the builder.

	// The resource name of the model used.
	// Format: providers/{provider}/models/{model}
	Model string
	// The resource name of the voice to use.
	// Format: voices/{voice}
	Voice string
	// The provider voice id.
	ProviderVoiceId string
	// The text to convert to speech.
	Text string
	// Additional configuration.
	Configuration *TextToSpeechConfiguration
}

func (b0 TextToSpeechRequest_builder) Build() *TextToSpeechRequest {
	m0 := &TextToSpeechRequest{}
	b, x := &b0, m0
	_, _ = b, x
	x.Model = b.Model
	x.Voice = b.Voice
	x.ProviderVoiceId = b.ProviderVoiceId
	x.Text = b.Text
	x.Configuration = b.Configuration
	return m0
}

// Response message for Ai.TextToSpeech.
type TextToSpeechResponse struct {
	state protoimpl.MessageState `protogen:"hybrid.v1"`
	// Audio format of the audio.
	AudioFormat *v11.Format `protobuf:"bytes,1,opt,name=audio_format,json=audioFormat,proto3" json:"audio_format,omitempty"`
	// Audio data chunk in PCM16 format.
	AudioChunk *v11.Chunk `protobuf:"bytes,2,opt,name=audio_chunk,json=audioChunk,proto3" json:"audio_chunk,omitempty"`
	// Model usage metrics.
	ModelUsage *v1.ModelUsage `protobuf:"bytes,3,opt,name=model_usage,json=modelUsage,proto3" json:"model_usage,omitempty"`
	// Generation metrics.
	GenerationMetrics *v1.GenerationMetrics `protobuf:"bytes,4,opt,name=generation_metrics,json=generationMetrics,proto3" json:"generation_metrics,omitempty"`
	unknownFields     protoimpl.UnknownFields
	sizeCache         protoimpl.SizeCache
}

func (x *TextToSpeechResponse) Reset() {
	*x = TextToSpeechResponse{}
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[17]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TextToSpeechResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TextToSpeechResponse) ProtoMessage() {}

func (x *TextToSpeechResponse) ProtoReflect() protoreflect.Message {
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[17]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

func (x *TextToSpeechResponse) GetAudioFormat() *v11.Format {
	if x != nil {
		return x.AudioFormat
	}
	return nil
}

func (x *TextToSpeechResponse) GetAudioChunk() *v11.Chunk {
	if x != nil {
		return x.AudioChunk
	}
	return nil
}

func (x *TextToSpeechResponse) GetModelUsage() *v1.ModelUsage {
	if x != nil {
		return x.ModelUsage
	}
	return nil
}

func (x *TextToSpeechResponse) GetGenerationMetrics() *v1.GenerationMetrics {
	if x != nil {
		return x.GenerationMetrics
	}
	return nil
}

func (x *TextToSpeechResponse) SetAudioFormat(v *v11.Format) {
	x.AudioFormat = v
}

func (x *TextToSpeechResponse) SetAudioChunk(v *v11.Chunk) {
	x.AudioChunk = v
}

func (x *TextToSpeechResponse) SetModelUsage(v *v1.ModelUsage) {
	x.ModelUsage = v
}

func (x *TextToSpeechResponse) SetGenerationMetrics(v *v1.GenerationMetrics) {
	x.GenerationMetrics = v
}

func (x *TextToSpeechResponse) HasAudioFormat() bool {
	if x == nil {
		return false
	}
	return x.AudioFormat != nil
}

func (x *TextToSpeechResponse) HasAudioChunk() bool {
	if x == nil {
		return false
	}
	return x.AudioChunk != nil
}

func (x *TextToSpeechResponse) HasModelUsage() bool {
	if x == nil {
		return false
	}
	return x.ModelUsage != nil
}

func (x *TextToSpeechResponse) HasGenerationMetrics() bool {
	if x == nil {
		return false
	}
	return x.GenerationMetrics != nil
}

func (x *TextToSpeechResponse) ClearAudioFormat() {
	x.AudioFormat = nil
}

func (x *TextToSpeechResponse) ClearAudioChunk() {
	x.AudioChunk = nil
}

func (x *TextToSpeechResponse) ClearModelUsage() {
	x.ModelUsage = nil
}

func (x *TextToSpeechResponse) ClearGenerationMetrics() {
	x.GenerationMetrics = nil
}

type TextToSpeechResponse_builder struct {
	_ [0]func() // Prevents comparability and use of unkeyed literals for the builder.

	// Audio format of the audio.
	AudioFormat *v11.Format
	// Audio data chunk in PCM16 format.
	AudioChunk *v11.Chunk
	// Model usage metrics.
	ModelUsage *v1.ModelUsage
	// Generation metrics.
	GenerationMetrics *v1.GenerationMetrics
}

func (b0 TextToSpeechResponse_builder) Build() *TextToSpeechResponse {
	m0 := &TextToSpeechResponse{}
	b, x := &b0, m0
	_, _ = b, x
	x.AudioFormat = b.AudioFormat
	x.AudioChunk = b.AudioChunk
	x.ModelUsage = b.ModelUsage
	x.GenerationMetrics = b.GenerationMetrics
	return m0
}

// Request message for Ai.TextToSpeechStream.
type TextToSpeechStreamRequest struct {
	state protoimpl.MessageState `protogen:"hybrid.v1"`
	// The resource name of the model to use.
	// Format: providers/{provider}/models/{model}
	Model string `protobuf:"bytes,1,opt,name=model,proto3" json:"model,omitempty"`
	// The resource name of the voice to use.
	// Format: voices/{voice}
	Voice string `protobuf:"bytes,2,opt,name=voice,proto3" json:"voice,omitempty"`
	// The provider voice id.
	ProviderVoiceId string `protobuf:"bytes,3,opt,name=provider_voice_id,json=providerVoiceId,proto3" json:"provider_voice_id,omitempty"`
	// The text to convert to speech.
	Text string `protobuf:"bytes,4,opt,name=text,proto3" json:"text,omitempty"`
	// Additional configuration.
	Configuration *TextToSpeechConfiguration `protobuf:"bytes,5,opt,name=configuration,proto3" json:"configuration,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *TextToSpeechStreamRequest) Reset() {
	*x = TextToSpeechStreamRequest{}
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[18]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TextToSpeechStreamRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TextToSpeechStreamRequest) ProtoMessage() {}

func (x *TextToSpeechStreamRequest) ProtoReflect() protoreflect.Message {
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[18]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

func (x *TextToSpeechStreamRequest) GetModel() string {
	if x != nil {
		return x.Model
	}
	return ""
}

func (x *TextToSpeechStreamRequest) GetVoice() string {
	if x != nil {
		return x.Voice
	}
	return ""
}

func (x *TextToSpeechStreamRequest) GetProviderVoiceId() string {
	if x != nil {
		return x.ProviderVoiceId
	}
	return ""
}

func (x *TextToSpeechStreamRequest) GetText() string {
	if x != nil {
		return x.Text
	}
	return ""
}

func (x *TextToSpeechStreamRequest) GetConfiguration() *TextToSpeechConfiguration {
	if x != nil {
		return x.Configuration
	}
	return nil
}

func (x *TextToSpeechStreamRequest) SetModel(v string) {
	x.Model = v
}

func (x *TextToSpeechStreamRequest) SetVoice(v string) {
	x.Voice = v
}

func (x *TextToSpeechStreamRequest) SetProviderVoiceId(v string) {
	x.ProviderVoiceId = v
}

func (x *TextToSpeechStreamRequest) SetText(v string) {
	x.Text = v
}

func (x *TextToSpeechStreamRequest) SetConfiguration(v *TextToSpeechConfiguration) {
	x.Configuration = v
}

func (x *TextToSpeechStreamRequest) HasConfiguration() bool {
	if x == nil {
		return false
	}
	return x.Configuration != nil
}

func (x *TextToSpeechStreamRequest) ClearConfiguration() {
	x.Configuration = nil
}

type TextToSpeechStreamRequest_builder struct {
	_ [0]func() // Prevents comparability and use of unkeyed literals for the builder.

	// The resource name of the model to use.
	// Format: providers/{provider}/models/{model}
	Model string
	// The resource name of the voice to use.
	// Format: voices/{voice}
	Voice string
	// The provider voice id.
	ProviderVoiceId string
	// The text to convert to speech.
	Text string
	// Additional configuration.
	Configuration *TextToSpeechConfiguration
}

func (b0 TextToSpeechStreamRequest_builder) Build() *TextToSpeechStreamRequest {
	m0 := &TextToSpeechStreamRequest{}
	b, x := &b0, m0
	_, _ = b, x
	x.Model = b.Model
	x.Voice = b.Voice
	x.ProviderVoiceId = b.ProviderVoiceId
	x.Text = b.Text
	x.Configuration = b.Configuration
	return m0
}

// Response message for Ai.TextToSpeechStream.
type TextToSpeechStreamResponse struct {
	state protoimpl.MessageState `protogen:"hybrid.v1"`
	// Content of this response.
	//
	// Types that are valid to be assigned to Content:
	//
	//	*TextToSpeechStreamResponse_AudioFormat
	//	*TextToSpeechStreamResponse_AudioChunk
	//	*TextToSpeechStreamResponse_ModelUsage
	//	*TextToSpeechStreamResponse_GenerationMetrics
	Content       isTextToSpeechStreamResponse_Content `protobuf_oneof:"content"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *TextToSpeechStreamResponse) Reset() {
	*x = TextToSpeechStreamResponse{}
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[19]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TextToSpeechStreamResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TextToSpeechStreamResponse) ProtoMessage() {}

func (x *TextToSpeechStreamResponse) ProtoReflect() protoreflect.Message {
	mi := &file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[19]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

func (x *TextToSpeechStreamResponse) GetContent() isTextToSpeechStreamResponse_Content {
	if x != nil {
		return x.Content
	}
	return nil
}

func (x *TextToSpeechStreamResponse) GetAudioFormat() *v11.Format {
	if x != nil {
		if x, ok := x.Content.(*TextToSpeechStreamResponse_AudioFormat); ok {
			return x.AudioFormat
		}
	}
	return nil
}

func (x *TextToSpeechStreamResponse) GetAudioChunk() *v11.Chunk {
	if x != nil {
		if x, ok := x.Content.(*TextToSpeechStreamResponse_AudioChunk); ok {
			return x.AudioChunk
		}
	}
	return nil
}

func (x *TextToSpeechStreamResponse) GetModelUsage() *v1.ModelUsage {
	if x != nil {
		if x, ok := x.Content.(*TextToSpeechStreamResponse_ModelUsage); ok {
			return x.ModelUsage
		}
	}
	return nil
}

func (x *TextToSpeechStreamResponse) GetGenerationMetrics() *v1.GenerationMetrics {
	if x != nil {
		if x, ok := x.Content.(*TextToSpeechStreamResponse_GenerationMetrics); ok {
			return x.GenerationMetrics
		}
	}
	return nil
}

func (x *TextToSpeechStreamResponse) SetAudioFormat(v *v11.Format) {
	if v == nil {
		x.Content = nil
		return
	}
	x.Content = &TextToSpeechStreamResponse_AudioFormat{v}
}

func (x *TextToSpeechStreamResponse) SetAudioChunk(v *v11.Chunk) {
	if v == nil {
		x.Content = nil
		return
	}
	x.Content = &TextToSpeechStreamResponse_AudioChunk{v}
}

func (x *TextToSpeechStreamResponse) SetModelUsage(v *v1.ModelUsage) {
	if v == nil {
		x.Content = nil
		return
	}
	x.Content = &TextToSpeechStreamResponse_ModelUsage{v}
}

func (x *TextToSpeechStreamResponse) SetGenerationMetrics(v *v1.GenerationMetrics) {
	if v == nil {
		x.Content = nil
		return
	}
	x.Content = &TextToSpeechStreamResponse_GenerationMetrics{v}
}

func (x *TextToSpeechStreamResponse) HasContent() bool {
	if x == nil {
		return false
	}
	return x.Content != nil
}

func (x *TextToSpeechStreamResponse) HasAudioFormat() bool {
	if x == nil {
		return false
	}
	_, ok := x.Content.(*TextToSpeechStreamResponse_AudioFormat)
	return ok
}

func (x *TextToSpeechStreamResponse) HasAudioChunk() bool {
	if x == nil {
		return false
	}
	_, ok := x.Content.(*TextToSpeechStreamResponse_AudioChunk)
	return ok
}

func (x *TextToSpeechStreamResponse) HasModelUsage() bool {
	if x == nil {
		return false
	}
	_, ok := x.Content.(*TextToSpeechStreamResponse_ModelUsage)
	return ok
}

func (x *TextToSpeechStreamResponse) HasGenerationMetrics() bool {
	if x == nil {
		return false
	}
	_, ok := x.Content.(*TextToSpeechStreamResponse_GenerationMetrics)
	return ok
}

func (x *TextToSpeechStreamResponse) ClearContent() {
	x.Content = nil
}

func (x *TextToSpeechStreamResponse) ClearAudioFormat() {
	if _, ok := x.Content.(*TextToSpeechStreamResponse_AudioFormat); ok {
		x.Content = nil
	}
}

func (x *TextToSpeechStreamResponse) ClearAudioChunk() {
	if _, ok := x.Content.(*TextToSpeechStreamResponse_AudioChunk); ok {
		x.Content = nil
	}
}

func (x *TextToSpeechStreamResponse) ClearModelUsage() {
	if _, ok := x.Content.(*TextToSpeechStreamResponse_ModelUsage); ok {
		x.Content = nil
	}
}

func (x *TextToSpeechStreamResponse) ClearGenerationMetrics() {
	if _, ok := x.Content.(*TextToSpeechStreamResponse_GenerationMetrics); ok {
		x.Content = nil
	}
}

const TextToSpeechStreamResponse_Content_not_set_case case_TextToSpeechStreamResponse_Content = 0
const TextToSpeechStreamResponse_AudioFormat_case case_TextToSpeechStreamResponse_Content = 1
const TextToSpeechStreamResponse_AudioChunk_case case_TextToSpeechStreamResponse_Content = 2
const TextToSpeechStreamResponse_ModelUsage_case case_TextToSpeechStreamResponse_Content = 3
const TextToSpeechStreamResponse_GenerationMetrics_case case_TextToSpeechStreamResponse_Content = 4

func (x *TextToSpeechStreamResponse) WhichContent() case_TextToSpeechStreamResponse_Content {
	if x == nil {
		return TextToSpeechStreamResponse_Content_not_set_case
	}
	switch x.Content.(type) {
	case *TextToSpeechStreamResponse_AudioFormat:
		return TextToSpeechStreamResponse_AudioFormat_case
	case *TextToSpeechStreamResponse_AudioChunk:
		return TextToSpeechStreamResponse_AudioChunk_case
	case *TextToSpeechStreamResponse_ModelUsage:
		return TextToSpeechStreamResponse_ModelUsage_case
	case *TextToSpeechStreamResponse_GenerationMetrics:
		return TextToSpeechStreamResponse_GenerationMetrics_case
	default:
		return TextToSpeechStreamResponse_Content_not_set_case
	}
}

type TextToSpeechStreamResponse_builder struct {
	_ [0]func() // Prevents comparability and use of unkeyed literals for the builder.

	// Content of this response.

	// Fields of oneof Content:
	// Audio format of the audio stream.
	AudioFormat *v11.Format
	// Audio data chunk in PCM16 format.
	AudioChunk *v11.Chunk
	// Model usage event (sent last).
	ModelUsage *v1.ModelUsage
	// Generation metrics.
	GenerationMetrics *v1.GenerationMetrics
	// -- end of Content
}

func (b0 TextToSpeechStreamResponse_builder) Build() *TextToSpeechStreamResponse {
	m0 := &TextToSpeechStreamResponse{}
	b, x := &b0, m0
	_, _ = b, x
	if b.AudioFormat != nil {
		x.Content = &TextToSpeechStreamResponse_AudioFormat{b.AudioFormat}
	}
	if b.AudioChunk != nil {
		x.Content = &TextToSpeechStreamResponse_AudioChunk{b.AudioChunk}
	}
	if b.ModelUsage != nil {
		x.Content = &TextToSpeechStreamResponse_ModelUsage{b.ModelUsage}
	}
	if b.GenerationMetrics != nil {
		x.Content = &TextToSpeechStreamResponse_GenerationMetrics{b.GenerationMetrics}
	}
	return m0
}

type case_TextToSpeechStreamResponse_Content protoreflect.FieldNumber

func (x case_TextToSpeechStreamResponse_Content) String() string {
	md := file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[19].Descriptor()
	if x == 0 {
		return "not set"
	}
	return protoimpl.X.MessageFieldStringOf(md, protoreflect.FieldNumber(x))
}

type isTextToSpeechStreamResponse_Content interface {
	isTextToSpeechStreamResponse_Content()
}

type TextToSpeechStreamResponse_AudioFormat struct {
	// Audio format of the audio stream.
	AudioFormat *v11.Format `protobuf:"bytes,1,opt,name=audio_format,json=audioFormat,proto3,oneof"`
}

type TextToSpeechStreamResponse_AudioChunk struct {
	// Audio data chunk in PCM16 format.
	AudioChunk *v11.Chunk `protobuf:"bytes,2,opt,name=audio_chunk,json=audioChunk,proto3,oneof"`
}

type TextToSpeechStreamResponse_ModelUsage struct {
	// Model usage event (sent last).
	ModelUsage *v1.ModelUsage `protobuf:"bytes,3,opt,name=model_usage,json=modelUsage,proto3,oneof"`
}

type TextToSpeechStreamResponse_GenerationMetrics struct {
	// Generation metrics.
	GenerationMetrics *v1.GenerationMetrics `protobuf:"bytes,4,opt,name=generation_metrics,json=generationMetrics,proto3,oneof"`
}

func (*TextToSpeechStreamResponse_AudioFormat) isTextToSpeechStreamResponse_Content() {}

func (*TextToSpeechStreamResponse_AudioChunk) isTextToSpeechStreamResponse_Content() {}

func (*TextToSpeechStreamResponse_ModelUsage) isTextToSpeechStreamResponse_Content() {}

func (*TextToSpeechStreamResponse_GenerationMetrics) isTextToSpeechStreamResponse_Content() {}

var File_malonaz_ai_ai_service_v1_ai_service_proto protoreflect.FileDescriptor

const file_malonaz_ai_ai_service_v1_ai_service_proto_rawDesc = "" +
	"\n" +
	")malonaz/ai/ai_service/v1/ai_service.proto\x12\x18malonaz.ai.ai_service.v1\x1a\x1bbuf/validate/validate.proto\x1a\x1cgoogle/api/annotations.proto\x1a\x17google/api/client.proto\x1a\x19google/api/resource.proto\x1a\x1cgoogle/protobuf/struct.proto\x1a\x1bmalonaz/ai/v1/message.proto\x1a\x1bmalonaz/ai/v1/metrics.proto\x1a\x19malonaz/ai/v1/model.proto\x1a\x18malonaz/ai/v1/tool.proto\x1a\x19malonaz/ai/v1/voice.proto\x1a\x1cmalonaz/audio/v1/audio.proto\x1a malonaz/codegen/aip/v1/aip.proto\"\xa7\x01\n" +
	"\x12CreateModelRequest\x12:\n" +
	"\x06parent\x18\x01 \x01(\tB\"\xfaA\x19\n" +
	"\x17ai.malonaz.com/Provider\xbaH\x03\xc8\x01\x01R\x06parent\x122\n" +
	"\x05model\x18\x02 \x01(\v2\x14.malonaz.ai.v1.ModelB\x06\xbaH\x03\xc8\x01\x01R\x05model\x12!\n" +
	"\bmodel_id\x18\x03 \x01(\tB\x06\xbaH\x03\xc8\x01\x01R\amodelId\"F\n" +
	"\x0fGetModelRequest\x123\n" +
	"\x04name\x18\x01 \x01(\tB\x1f\xfaA\x16\n" +
	"\x14ai.malonaz.com/Model\xbaH\x03\xc8\x01\x01R\x04name\"\xa0\x01\n" +
	"\x11ListModelsRequest\x12:\n" +
	"\x06parent\x18\x01 \x01(\tB\"\xfaA\x19\n" +
	"\x17ai.malonaz.com/Provider\xbaH\x03\xc8\x01\x01R\x06parent\x12'\n" +
	"\tpage_size\x18\x02 \x01(\x05B\n" +
	"\xbaH\a\x1a\x05\x18\xe8\a(\x00R\bpageSize\x12\x1d\n" +
	"\n" +
	"page_token\x18\x03 \x01(\tR\tpageToken:\a\x82\xf3-\x03\b\xf4\x03\"j\n" +
	"\x12ListModelsResponse\x12,\n" +
	"\x06models\x18\x01 \x03(\v2\x14.malonaz.ai.v1.ModelR\x06models\x12&\n" +
	"\x0fnext_page_token\x18\x02 \x01(\tR\rnextPageToken\"k\n" +
	"\x12CreateVoiceRequest\x122\n" +
	"\x05voice\x18\x01 \x01(\v2\x14.malonaz.ai.v1.VoiceB\x06\xbaH\x03\xc8\x01\x01R\x05voice\x12!\n" +
	"\bvoice_id\x18\x02 \x01(\tB\x06\xbaH\x03\xc8\x01\x01R\avoiceId\"F\n" +
	"\x0fGetVoiceRequest\x123\n" +
	"\x04name\x18\x01 \x01(\tB\x1f\xfaA\x16\n" +
	"\x14ai.malonaz.com/Voice\xbaH\x03\xc8\x01\x01R\x04name\"d\n" +
	"\x11ListVoicesRequest\x12'\n" +
	"\tpage_size\x18\x01 \x01(\x05B\n" +
	"\xbaH\a\x1a\x05\x18\xe8\a(\x00R\bpageSize\x12\x1d\n" +
	"\n" +
	"page_token\x18\x02 \x01(\tR\tpageToken:\a\x82\xf3-\x03\b\xf4\x03\"j\n" +
	"\x12ListVoicesResponse\x12,\n" +
	"\x06voices\x18\x01 \x03(\v2\x14.malonaz.ai.v1.VoiceR\x06voices\x12&\n" +
	"\x0fnext_page_token\x18\x02 \x01(\tR\rnextPageToken\"\xf8\x01\n" +
	"\x13SpeechToTextRequest\x125\n" +
	"\x05model\x18\x01 \x01(\tB\x1f\xfaA\x16\n" +
	"\x14ai.malonaz.com/Model\xbaH\x03\xc8\x01\x01R\x05model\x12C\n" +
	"\faudio_format\x18\x02 \x01(\v2\x18.malonaz.audio.v1.FormatB\x06\xbaH\x03\xc8\x01\x01R\vaudioFormat\x12@\n" +
	"\vaudio_chunk\x18\x03 \x01(\v2\x17.malonaz.audio.v1.ChunkB\x06\xbaH\x03\xc8\x01\x01R\n" +
	"audioChunk\x12#\n" +
	"\rlanguage_code\x18\x04 \x01(\tR\flanguageCode\"\xc3\x01\n" +
	"\x14SpeechToTextResponse\x12\x1e\n" +
	"\n" +
	"transcript\x18\x01 \x01(\tR\n" +
	"transcript\x12:\n" +
	"\vmodel_usage\x18\x02 \x01(\v2\x19.malonaz.ai.v1.ModelUsageR\n" +
	"modelUsage\x12O\n" +
	"\x12generation_metrics\x18\x03 \x01(\v2 .malonaz.ai.v1.GenerationMetricsR\x11generationMetrics\"\xee\x02\n" +
	"\x17TextToTextConfiguration\x12&\n" +
	"\n" +
	"max_tokens\x18\x01 \x01(\x05B\a\xbaH\x04\x1a\x02(\x00R\tmaxTokens\x129\n" +
	"\vtemperature\x18\x02 \x01(\x01B\x17\xbaH\x14\x12\x12\x19\x00\x00\x00\x00\x00\x00\x00@)\x00\x00\x00\x00\x00\x00\x00\x00R\vtemperature\x12:\n" +
	"\vtool_choice\x18\x03 \x01(\v2\x19.malonaz.ai.v1.ToolChoiceR\n" +
	"toolChoice\x12I\n" +
	"\x10reasoning_effort\x18\x04 \x01(\x0e2\x1e.malonaz.ai.v1.ReasoningEffortR\x0freasoningEffort\x12.\n" +
	"\x13extract_json_object\x18\x05 \x01(\bR\x11extractJsonObject\x129\n" +
	"\x19stream_partial_tool_calls\x18\x06 \x01(\bR\x16streamPartialToolCalls\"\x8c\x02\n" +
	"\x11TextToTextRequest\x125\n" +
	"\x05model\x18\x01 \x01(\tB\x1f\xfaA\x16\n" +
	"\x14ai.malonaz.com/Model\xbaH\x03\xc8\x01\x01R\x05model\x12<\n" +
	"\bmessages\x18\x02 \x03(\v2\x16.malonaz.ai.v1.MessageB\b\xbaH\x05\x92\x01\x02\b\x01R\bmessages\x12)\n" +
	"\x05tools\x18\x03 \x03(\v2\x13.malonaz.ai.v1.ToolR\x05tools\x12W\n" +
	"\rconfiguration\x18\x04 \x01(\v21.malonaz.ai.ai_service.v1.TextToTextConfigurationR\rconfiguration\"\xa4\x02\n" +
	"\x12TextToTextResponse\x120\n" +
	"\amessage\x18\x01 \x01(\v2\x16.malonaz.ai.v1.MessageR\amessage\x12O\n" +
	"\vstop_reason\x18\x02 \x01(\x0e2..malonaz.ai.ai_service.v1.TextToTextStopReasonR\n" +
	"stopReason\x12:\n" +
	"\vmodel_usage\x18\x03 \x01(\v2\x19.malonaz.ai.v1.ModelUsageR\n" +
	"modelUsage\x12O\n" +
	"\x12generation_metrics\x18\x04 \x01(\v2 .malonaz.ai.v1.GenerationMetricsR\x11generationMetrics\"\xb3\x02\n" +
	"\x17TextToTextStreamRequest\x125\n" +
	"\x05model\x18\x01 \x01(\tB\x1f\xfaA\x16\n" +
	"\x14ai.malonaz.com/Model\xbaH\x03\xc8\x01\x01R\x05model\x12<\n" +
	"\bmessages\x18\x02 \x03(\v2\x16.malonaz.ai.v1.MessageB\b\xbaH\x05\x92\x01\x02\b\x01R\bmessages\x12)\n" +
	"\x05tools\x18\x03 \x03(\v2\x13.malonaz.ai.v1.ToolR\x05tools\x12\x1f\n" +
	"\vtool_choice\x18\x04 \x01(\tR\n" +
	"toolChoice\x12W\n" +
	"\rconfiguration\x18\x05 \x01(\v21.malonaz.ai.ai_service.v1.TextToTextConfigurationR\rconfiguration\"\xe1\x03\n" +
	"\x18TextToTextStreamResponse\x12%\n" +
	"\rcontent_chunk\x18\x01 \x01(\tH\x00R\fcontentChunk\x12)\n" +
	"\x0freasoning_chunk\x18\x02 \x01(\tH\x00R\x0ereasoningChunk\x12Q\n" +
	"\vstop_reason\x18\x03 \x01(\x0e2..malonaz.ai.ai_service.v1.TextToTextStopReasonH\x00R\n" +
	"stopReason\x126\n" +
	"\ttool_call\x18\x04 \x01(\v2\x17.malonaz.ai.v1.ToolCallH\x00R\btoolCall\x12E\n" +
	"\x11partial_tool_call\x18\x05 \x01(\v2\x17.malonaz.ai.v1.ToolCallH\x00R\x0fpartialToolCall\x12<\n" +
	"\vmodel_usage\x18\x06 \x01(\v2\x19.malonaz.ai.v1.ModelUsageH\x00R\n" +
	"modelUsage\x12Q\n" +
	"\x12generation_metrics\x18\a \x01(\v2 .malonaz.ai.v1.GenerationMetricsH\x00R\x11generationMetricsB\x10\n" +
	"\acontent\x12\x05\xbaH\x02\b\x01\"\xba\x01\n" +
	"\x19TextToSpeechConfiguration\x12#\n" +
	"\rlanguage_code\x18\x01 \x01(\tR\flanguageCode\x122\n" +
	"\x15preferred_sample_rate\x18\x02 \x01(\x05R\x13preferredSampleRate\x12D\n" +
	"\x11provider_settings\x18\x03 \x01(\v2\x17.google.protobuf.StructR\x10providerSettings\"\xd4\x03\n" +
	"\x13TextToSpeechRequest\x125\n" +
	"\x05model\x18\x01 \x01(\tB\x1f\xfaA\x16\n" +
	"\x14ai.malonaz.com/Model\xbaH\x03\xc8\x01\x01R\x05model\x125\n" +
	"\x05voice\x18\x02 \x01(\tB\x1f\xfaA\x16\n" +
	"\x14ai.malonaz.com/Voice\xbaH\x03\xc8\x01\x01R\x05voice\x12*\n" +
	"\x11provider_voice_id\x18\x03 \x01(\tR\x0fproviderVoiceId\x12\x1b\n" +
	"\x04text\x18\x04 \x01(\tB\a\xbaH\x04r\x02\x10\x01R\x04text\x12Y\n" +
	"\rconfiguration\x18\x05 \x01(\v23.malonaz.ai.ai_service.v1.TextToSpeechConfigurationR\rconfiguration:\xaa\x01\xbaH\xa6\x01\x1a\xa3\x01\n" +
	"4ai.v1.TextToSpeechRequest.voice_or_provider_voice_id\x129exactly one of `voice` or `provider_voice_id` must be set\x1a0this.voice != '' || this.provider_voice_id != ''\"\x9a\x02\n" +
	"\x14TextToSpeechResponse\x12;\n" +
	"\faudio_format\x18\x01 \x01(\v2\x18.malonaz.audio.v1.FormatR\vaudioFormat\x128\n" +
	"\vaudio_chunk\x18\x02 \x01(\v2\x17.malonaz.audio.v1.ChunkR\n" +
	"audioChunk\x12:\n" +
	"\vmodel_usage\x18\x03 \x01(\v2\x19.malonaz.ai.v1.ModelUsageR\n" +
	"modelUsage\x12O\n" +
	"\x12generation_metrics\x18\x04 \x01(\v2 .malonaz.ai.v1.GenerationMetricsR\x11generationMetrics\"\xda\x03\n" +
	"\x19TextToSpeechStreamRequest\x125\n" +
	"\x05model\x18\x01 \x01(\tB\x1f\xfaA\x16\n" +
	"\x14ai.malonaz.com/Model\xbaH\x03\xc8\x01\x01R\x05model\x125\n" +
	"\x05voice\x18\x02 \x01(\tB\x1f\xfaA\x16\n" +
	"\x14ai.malonaz.com/Voice\xbaH\x03\xc8\x01\x01R\x05voice\x12*\n" +
	"\x11provider_voice_id\x18\x03 \x01(\tR\x0fproviderVoiceId\x12\x1b\n" +
	"\x04text\x18\x04 \x01(\tB\a\xbaH\x04r\x02\x10\x01R\x04text\x12Y\n" +
	"\rconfiguration\x18\x05 \x01(\v23.malonaz.ai.ai_service.v1.TextToSpeechConfigurationR\rconfiguration:\xaa\x01\xbaH\xa6\x01\x1a\xa3\x01\n" +
	"4ai.v1.TextToSpeechRequest.voice_or_provider_voice_id\x129exactly one of `voice` or `provider_voice_id` must be set\x1a0this.voice != '' || this.provider_voice_id != ''\"\xba\x02\n" +
	"\x1aTextToSpeechStreamResponse\x12=\n" +
	"\faudio_format\x18\x01 \x01(\v2\x18.malonaz.audio.v1.FormatH\x00R\vaudioFormat\x12:\n" +
	"\vaudio_chunk\x18\x02 \x01(\v2\x17.malonaz.audio.v1.ChunkH\x00R\n" +
	"audioChunk\x12<\n" +
	"\vmodel_usage\x18\x03 \x01(\v2\x19.malonaz.ai.v1.ModelUsageH\x00R\n" +
	"modelUsage\x12Q\n" +
	"\x12generation_metrics\x18\x04 \x01(\v2 .malonaz.ai.v1.GenerationMetricsH\x00R\x11generationMetricsB\x10\n" +
	"\acontent\x12\x05\xbaH\x02\b\x01*\xb3\x02\n" +
	"\x14TextToTextStopReason\x12(\n" +
	"$TEXT_TO_TEXT_STOP_REASON_UNSPECIFIED\x10\x00\x12%\n" +
	"!TEXT_TO_TEXT_STOP_REASON_END_TURN\x10\x01\x12'\n" +
	"#TEXT_TO_TEXT_STOP_REASON_MAX_TOKENS\x10\x02\x12&\n" +
	"\"TEXT_TO_TEXT_STOP_REASON_TOOL_CALL\x10\x03\x12*\n" +
	"&TEXT_TO_TEXT_STOP_REASON_STOP_SEQUENCE\x10\x04\x12'\n" +
	"#TEXT_TO_TEXT_STOP_REASON_PAUSE_TURN\x10\x05\x12$\n" +
	" TEXT_TO_TEXT_STOP_REASON_REFUSAL\x10\x062\xa5\r\n" +
	"\tAiService\x12\x89\x01\n" +
	"\vCreateModel\x12,.malonaz.ai.ai_service.v1.CreateModelRequest\x1a\x14.malonaz.ai.v1.Model\"6\xdaA\fparent,model\x82\xd3\xe4\x93\x02!\"\x1f/v1/{parent=providers/*}/models\x12{\n" +
	"\bGetModel\x12).malonaz.ai.ai_service.v1.GetModelRequest\x1a\x14.malonaz.ai.v1.Model\".\xdaA\x04name\x82\xd3\xe4\x93\x02!\x12\x1f/v1/{name=providers/*/models/*}\x12\x99\x01\n" +
	"\n" +
	"ListModels\x12+.malonaz.ai.ai_service.v1.ListModelsRequest\x1a,.malonaz.ai.ai_service.v1.ListModelsResponse\"0\xdaA\x06parent\x82\xd3\xe4\x93\x02!\x12\x1f/v1/{parent=providers/*}/models\x12m\n" +
	"\vCreateVoice\x12,.malonaz.ai.ai_service.v1.CreateVoiceRequest\x1a\x14.malonaz.ai.v1.Voice\"\x1a\xdaA\x05voice\x82\xd3\xe4\x93\x02\f\"\n" +
	"/v1/voices\x12o\n" +
	"\bGetVoice\x12).malonaz.ai.ai_service.v1.GetVoiceRequest\x1a\x14.malonaz.ai.v1.Voice\"\"\xdaA\x04name\x82\xd3\xe4\x93\x02\x15\x12\x13/v1/{name=voices/*}\x12~\n" +
	"\n" +
	"ListVoices\x12+.malonaz.ai.ai_service.v1.ListVoicesRequest\x1a,.malonaz.ai.ai_service.v1.ListVoicesResponse\"\x15\xdaA\x00\x82\xd3\xe4\x93\x02\f\x12\n" +
	"/v1/voices\x12\xa9\x01\n" +
	"\fSpeechToText\x12-.malonaz.ai.ai_service.v1.SpeechToTextRequest\x1a..malonaz.ai.ai_service.v1.SpeechToTextResponse\":\x82\xd3\xe4\x93\x024:\x01*\"//v1/{model=providers/*/models/*}/speech-to-text\x12\xa1\x01\n" +
	"\n" +
	"TextToText\x12+.malonaz.ai.ai_service.v1.TextToTextRequest\x1a,.malonaz.ai.ai_service.v1.TextToTextResponse\"8\x82\xd3\xe4\x93\x022:\x01*\"-/v1/{model=providers/*/models/*}/text-to-text\x12\xbc\x01\n" +
	"\x10TextToTextStream\x121.malonaz.ai.ai_service.v1.TextToTextStreamRequest\x1a2.malonaz.ai.ai_service.v1.TextToTextStreamResponse\"?\x82\xd3\xe4\x93\x029:\x01*\"4/v1/{model=providers/*/models/*}/text-to-text:stream0\x01\x12\xa9\x01\n" +
	"\fTextToSpeech\x12-.malonaz.ai.ai_service.v1.TextToSpeechRequest\x1a..malonaz.ai.ai_service.v1.TextToSpeechResponse\":\x82\xd3\xe4\x93\x024:\x01*\"//v1/{model=providers/*/models/*}/text-to-speech\x12\xc4\x01\n" +
	"\x12TextToSpeechStream\x123.malonaz.ai.ai_service.v1.TextToSpeechStreamRequest\x1a4.malonaz.ai.ai_service.v1.TextToSpeechStreamResponse\"A\x82\xd3\xe4\x93\x02;:\x01*\"6/v1/{model=providers/*/models/*}/text-to-speech:stream0\x01\x1a\x11\xcaA\x0eai.malonaz.comB3Z1github.com/malonaz/core/genproto/ai/ai_service/v1b\x06proto3"

var file_malonaz_ai_ai_service_v1_ai_service_proto_enumTypes = make([]protoimpl.EnumInfo, 1)
var file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes = make([]protoimpl.MessageInfo, 20)
var file_malonaz_ai_ai_service_v1_ai_service_proto_goTypes = []any{
	(TextToTextStopReason)(0),          // 0: malonaz.ai.ai_service.v1.TextToTextStopReason
	(*CreateModelRequest)(nil),         // 1: malonaz.ai.ai_service.v1.CreateModelRequest
	(*GetModelRequest)(nil),            // 2: malonaz.ai.ai_service.v1.GetModelRequest
	(*ListModelsRequest)(nil),          // 3: malonaz.ai.ai_service.v1.ListModelsRequest
	(*ListModelsResponse)(nil),         // 4: malonaz.ai.ai_service.v1.ListModelsResponse
	(*CreateVoiceRequest)(nil),         // 5: malonaz.ai.ai_service.v1.CreateVoiceRequest
	(*GetVoiceRequest)(nil),            // 6: malonaz.ai.ai_service.v1.GetVoiceRequest
	(*ListVoicesRequest)(nil),          // 7: malonaz.ai.ai_service.v1.ListVoicesRequest
	(*ListVoicesResponse)(nil),         // 8: malonaz.ai.ai_service.v1.ListVoicesResponse
	(*SpeechToTextRequest)(nil),        // 9: malonaz.ai.ai_service.v1.SpeechToTextRequest
	(*SpeechToTextResponse)(nil),       // 10: malonaz.ai.ai_service.v1.SpeechToTextResponse
	(*TextToTextConfiguration)(nil),    // 11: malonaz.ai.ai_service.v1.TextToTextConfiguration
	(*TextToTextRequest)(nil),          // 12: malonaz.ai.ai_service.v1.TextToTextRequest
	(*TextToTextResponse)(nil),         // 13: malonaz.ai.ai_service.v1.TextToTextResponse
	(*TextToTextStreamRequest)(nil),    // 14: malonaz.ai.ai_service.v1.TextToTextStreamRequest
	(*TextToTextStreamResponse)(nil),   // 15: malonaz.ai.ai_service.v1.TextToTextStreamResponse
	(*TextToSpeechConfiguration)(nil),  // 16: malonaz.ai.ai_service.v1.TextToSpeechConfiguration
	(*TextToSpeechRequest)(nil),        // 17: malonaz.ai.ai_service.v1.TextToSpeechRequest
	(*TextToSpeechResponse)(nil),       // 18: malonaz.ai.ai_service.v1.TextToSpeechResponse
	(*TextToSpeechStreamRequest)(nil),  // 19: malonaz.ai.ai_service.v1.TextToSpeechStreamRequest
	(*TextToSpeechStreamResponse)(nil), // 20: malonaz.ai.ai_service.v1.TextToSpeechStreamResponse
	(*v1.Model)(nil),                   // 21: malonaz.ai.v1.Model
	(*v1.Voice)(nil),                   // 22: malonaz.ai.v1.Voice
	(*v11.Format)(nil),                 // 23: malonaz.audio.v1.Format
	(*v11.Chunk)(nil),                  // 24: malonaz.audio.v1.Chunk
	(*v1.ModelUsage)(nil),              // 25: malonaz.ai.v1.ModelUsage
	(*v1.GenerationMetrics)(nil),       // 26: malonaz.ai.v1.GenerationMetrics
	(*v1.ToolChoice)(nil),              // 27: malonaz.ai.v1.ToolChoice
	(v1.ReasoningEffort)(0),            // 28: malonaz.ai.v1.ReasoningEffort
	(*v1.Message)(nil),                 // 29: malonaz.ai.v1.Message
	(*v1.Tool)(nil),                    // 30: malonaz.ai.v1.Tool
	(*v1.ToolCall)(nil),                // 31: malonaz.ai.v1.ToolCall
	(*structpb.Struct)(nil),            // 32: google.protobuf.Struct
}
var file_malonaz_ai_ai_service_v1_ai_service_proto_depIdxs = []int32{
	21, // 0: malonaz.ai.ai_service.v1.CreateModelRequest.model:type_name -> malonaz.ai.v1.Model
	21, // 1: malonaz.ai.ai_service.v1.ListModelsResponse.models:type_name -> malonaz.ai.v1.Model
	22, // 2: malonaz.ai.ai_service.v1.CreateVoiceRequest.voice:type_name -> malonaz.ai.v1.Voice
	22, // 3: malonaz.ai.ai_service.v1.ListVoicesResponse.voices:type_name -> malonaz.ai.v1.Voice
	23, // 4: malonaz.ai.ai_service.v1.SpeechToTextRequest.audio_format:type_name -> malonaz.audio.v1.Format
	24, // 5: malonaz.ai.ai_service.v1.SpeechToTextRequest.audio_chunk:type_name -> malonaz.audio.v1.Chunk
	25, // 6: malonaz.ai.ai_service.v1.SpeechToTextResponse.model_usage:type_name -> malonaz.ai.v1.ModelUsage
	26, // 7: malonaz.ai.ai_service.v1.SpeechToTextResponse.generation_metrics:type_name -> malonaz.ai.v1.GenerationMetrics
	27, // 8: malonaz.ai.ai_service.v1.TextToTextConfiguration.tool_choice:type_name -> malonaz.ai.v1.ToolChoice
	28, // 9: malonaz.ai.ai_service.v1.TextToTextConfiguration.reasoning_effort:type_name -> malonaz.ai.v1.ReasoningEffort
	29, // 10: malonaz.ai.ai_service.v1.TextToTextRequest.messages:type_name -> malonaz.ai.v1.Message
	30, // 11: malonaz.ai.ai_service.v1.TextToTextRequest.tools:type_name -> malonaz.ai.v1.Tool
	11, // 12: malonaz.ai.ai_service.v1.TextToTextRequest.configuration:type_name -> malonaz.ai.ai_service.v1.TextToTextConfiguration
	29, // 13: malonaz.ai.ai_service.v1.TextToTextResponse.message:type_name -> malonaz.ai.v1.Message
	0,  // 14: malonaz.ai.ai_service.v1.TextToTextResponse.stop_reason:type_name -> malonaz.ai.ai_service.v1.TextToTextStopReason
	25, // 15: malonaz.ai.ai_service.v1.TextToTextResponse.model_usage:type_name -> malonaz.ai.v1.ModelUsage
	26, // 16: malonaz.ai.ai_service.v1.TextToTextResponse.generation_metrics:type_name -> malonaz.ai.v1.GenerationMetrics
	29, // 17: malonaz.ai.ai_service.v1.TextToTextStreamRequest.messages:type_name -> malonaz.ai.v1.Message
	30, // 18: malonaz.ai.ai_service.v1.TextToTextStreamRequest.tools:type_name -> malonaz.ai.v1.Tool
	11, // 19: malonaz.ai.ai_service.v1.TextToTextStreamRequest.configuration:type_name -> malonaz.ai.ai_service.v1.TextToTextConfiguration
	0,  // 20: malonaz.ai.ai_service.v1.TextToTextStreamResponse.stop_reason:type_name -> malonaz.ai.ai_service.v1.TextToTextStopReason
	31, // 21: malonaz.ai.ai_service.v1.TextToTextStreamResponse.tool_call:type_name -> malonaz.ai.v1.ToolCall
	31, // 22: malonaz.ai.ai_service.v1.TextToTextStreamResponse.partial_tool_call:type_name -> malonaz.ai.v1.ToolCall
	25, // 23: malonaz.ai.ai_service.v1.TextToTextStreamResponse.model_usage:type_name -> malonaz.ai.v1.ModelUsage
	26, // 24: malonaz.ai.ai_service.v1.TextToTextStreamResponse.generation_metrics:type_name -> malonaz.ai.v1.GenerationMetrics
	32, // 25: malonaz.ai.ai_service.v1.TextToSpeechConfiguration.provider_settings:type_name -> google.protobuf.Struct
	16, // 26: malonaz.ai.ai_service.v1.TextToSpeechRequest.configuration:type_name -> malonaz.ai.ai_service.v1.TextToSpeechConfiguration
	23, // 27: malonaz.ai.ai_service.v1.TextToSpeechResponse.audio_format:type_name -> malonaz.audio.v1.Format
	24, // 28: malonaz.ai.ai_service.v1.TextToSpeechResponse.audio_chunk:type_name -> malonaz.audio.v1.Chunk
	25, // 29: malonaz.ai.ai_service.v1.TextToSpeechResponse.model_usage:type_name -> malonaz.ai.v1.ModelUsage
	26, // 30: malonaz.ai.ai_service.v1.TextToSpeechResponse.generation_metrics:type_name -> malonaz.ai.v1.GenerationMetrics
	16, // 31: malonaz.ai.ai_service.v1.TextToSpeechStreamRequest.configuration:type_name -> malonaz.ai.ai_service.v1.TextToSpeechConfiguration
	23, // 32: malonaz.ai.ai_service.v1.TextToSpeechStreamResponse.audio_format:type_name -> malonaz.audio.v1.Format
	24, // 33: malonaz.ai.ai_service.v1.TextToSpeechStreamResponse.audio_chunk:type_name -> malonaz.audio.v1.Chunk
	25, // 34: malonaz.ai.ai_service.v1.TextToSpeechStreamResponse.model_usage:type_name -> malonaz.ai.v1.ModelUsage
	26, // 35: malonaz.ai.ai_service.v1.TextToSpeechStreamResponse.generation_metrics:type_name -> malonaz.ai.v1.GenerationMetrics
	1,  // 36: malonaz.ai.ai_service.v1.AiService.CreateModel:input_type -> malonaz.ai.ai_service.v1.CreateModelRequest
	2,  // 37: malonaz.ai.ai_service.v1.AiService.GetModel:input_type -> malonaz.ai.ai_service.v1.GetModelRequest
	3,  // 38: malonaz.ai.ai_service.v1.AiService.ListModels:input_type -> malonaz.ai.ai_service.v1.ListModelsRequest
	5,  // 39: malonaz.ai.ai_service.v1.AiService.CreateVoice:input_type -> malonaz.ai.ai_service.v1.CreateVoiceRequest
	6,  // 40: malonaz.ai.ai_service.v1.AiService.GetVoice:input_type -> malonaz.ai.ai_service.v1.GetVoiceRequest
	7,  // 41: malonaz.ai.ai_service.v1.AiService.ListVoices:input_type -> malonaz.ai.ai_service.v1.ListVoicesRequest
	9,  // 42: malonaz.ai.ai_service.v1.AiService.SpeechToText:input_type -> malonaz.ai.ai_service.v1.SpeechToTextRequest
	12, // 43: malonaz.ai.ai_service.v1.AiService.TextToText:input_type -> malonaz.ai.ai_service.v1.TextToTextRequest
	14, // 44: malonaz.ai.ai_service.v1.AiService.TextToTextStream:input_type -> malonaz.ai.ai_service.v1.TextToTextStreamRequest
	17, // 45: malonaz.ai.ai_service.v1.AiService.TextToSpeech:input_type -> malonaz.ai.ai_service.v1.TextToSpeechRequest
	19, // 46: malonaz.ai.ai_service.v1.AiService.TextToSpeechStream:input_type -> malonaz.ai.ai_service.v1.TextToSpeechStreamRequest
	21, // 47: malonaz.ai.ai_service.v1.AiService.CreateModel:output_type -> malonaz.ai.v1.Model
	21, // 48: malonaz.ai.ai_service.v1.AiService.GetModel:output_type -> malonaz.ai.v1.Model
	4,  // 49: malonaz.ai.ai_service.v1.AiService.ListModels:output_type -> malonaz.ai.ai_service.v1.ListModelsResponse
	22, // 50: malonaz.ai.ai_service.v1.AiService.CreateVoice:output_type -> malonaz.ai.v1.Voice
	22, // 51: malonaz.ai.ai_service.v1.AiService.GetVoice:output_type -> malonaz.ai.v1.Voice
	8,  // 52: malonaz.ai.ai_service.v1.AiService.ListVoices:output_type -> malonaz.ai.ai_service.v1.ListVoicesResponse
	10, // 53: malonaz.ai.ai_service.v1.AiService.SpeechToText:output_type -> malonaz.ai.ai_service.v1.SpeechToTextResponse
	13, // 54: malonaz.ai.ai_service.v1.AiService.TextToText:output_type -> malonaz.ai.ai_service.v1.TextToTextResponse
	15, // 55: malonaz.ai.ai_service.v1.AiService.TextToTextStream:output_type -> malonaz.ai.ai_service.v1.TextToTextStreamResponse
	18, // 56: malonaz.ai.ai_service.v1.AiService.TextToSpeech:output_type -> malonaz.ai.ai_service.v1.TextToSpeechResponse
	20, // 57: malonaz.ai.ai_service.v1.AiService.TextToSpeechStream:output_type -> malonaz.ai.ai_service.v1.TextToSpeechStreamResponse
	47, // [47:58] is the sub-list for method output_type
	36, // [36:47] is the sub-list for method input_type
	36, // [36:36] is the sub-list for extension type_name
	36, // [36:36] is the sub-list for extension extendee
	0,  // [0:36] is the sub-list for field type_name
}

func init() { file_malonaz_ai_ai_service_v1_ai_service_proto_init() }
func file_malonaz_ai_ai_service_v1_ai_service_proto_init() {
	if File_malonaz_ai_ai_service_v1_ai_service_proto != nil {
		return
	}
	file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[14].OneofWrappers = []any{
		(*TextToTextStreamResponse_ContentChunk)(nil),
		(*TextToTextStreamResponse_ReasoningChunk)(nil),
		(*TextToTextStreamResponse_StopReason)(nil),
		(*TextToTextStreamResponse_ToolCall)(nil),
		(*TextToTextStreamResponse_PartialToolCall)(nil),
		(*TextToTextStreamResponse_ModelUsage)(nil),
		(*TextToTextStreamResponse_GenerationMetrics)(nil),
	}
	file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes[19].OneofWrappers = []any{
		(*TextToSpeechStreamResponse_AudioFormat)(nil),
		(*TextToSpeechStreamResponse_AudioChunk)(nil),
		(*TextToSpeechStreamResponse_ModelUsage)(nil),
		(*TextToSpeechStreamResponse_GenerationMetrics)(nil),
	}
	type x struct{}
	out := protoimpl.TypeBuilder{
		File: protoimpl.DescBuilder{
			GoPackagePath: reflect.TypeOf(x{}).PkgPath(),
			RawDescriptor: unsafe.Slice(unsafe.StringData(file_malonaz_ai_ai_service_v1_ai_service_proto_rawDesc), len(file_malonaz_ai_ai_service_v1_ai_service_proto_rawDesc)),
			NumEnums:      1,
			NumMessages:   20,
			NumExtensions: 0,
			NumServices:   1,
		},
		GoTypes:           file_malonaz_ai_ai_service_v1_ai_service_proto_goTypes,
		DependencyIndexes: file_malonaz_ai_ai_service_v1_ai_service_proto_depIdxs,
		EnumInfos:         file_malonaz_ai_ai_service_v1_ai_service_proto_enumTypes,
		MessageInfos:      file_malonaz_ai_ai_service_v1_ai_service_proto_msgTypes,
	}.Build()
	File_malonaz_ai_ai_service_v1_ai_service_proto = out.File
	file_malonaz_ai_ai_service_v1_ai_service_proto_goTypes = nil
	file_malonaz_ai_ai_service_v1_ai_service_proto_depIdxs = nil
}
