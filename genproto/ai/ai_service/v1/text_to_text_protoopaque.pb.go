// Code generated by protoc-gen-go. DO NOT EDIT.
// versions:
// 	protoc-gen-go v1.36.9
// 	protoc        v6.32.1
// source: malonaz/ai/ai_service/v1/text_to_text.proto

//go:build protoopaque

package v1

import (
	_ "buf.build/gen/go/bufbuild/protovalidate/protocolbuffers/go/buf/validate"
	v1 "github.com/malonaz/core/genproto/ai/v1"
	_ "google.golang.org/genproto/googleapis/api/annotations"
	protoreflect "google.golang.org/protobuf/reflect/protoreflect"
	protoimpl "google.golang.org/protobuf/runtime/protoimpl"
	reflect "reflect"
	unsafe "unsafe"
)

const (
	// Verify that this generated code is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(20 - protoimpl.MinVersion)
	// Verify that runtime/protoimpl is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(protoimpl.MaxVersion - 20)
)

// Reason why generation stopped.
type TextToTextStopReason int32

const (
	// Used to detect an unset field.
	TextToTextStopReason_TEXT_TO_TEXT_STOP_REASON_UNSPECIFIED TextToTextStopReason = 0
	// The model reached a natural stopping point.
	TextToTextStopReason_TEXT_TO_TEXT_STOP_REASON_END_TURN TextToTextStopReason = 1
	// The model reached the maximum token limit specified in the request.
	TextToTextStopReason_TEXT_TO_TEXT_STOP_REASON_MAX_TOKENS TextToTextStopReason = 2
	// The model requested to call one or more tools.
	TextToTextStopReason_TEXT_TO_TEXT_STOP_REASON_TOOL_CALL TextToTextStopReason = 3
	// one of your provided custom `stop_sequences` was generated
	TextToTextStopReason_TEXT_TO_TEXT_STOP_REASON_STOP_SEQUENCE TextToTextStopReason = 4
	// The model paused its turn to allow the user to interject.
	TextToTextStopReason_TEXT_TO_TEXT_STOP_REASON_PAUSE_TURN TextToTextStopReason = 5
	// The model refused to respond due to safety or policy reasons.
	TextToTextStopReason_TEXT_TO_TEXT_STOP_REASON_REFUSAL TextToTextStopReason = 6
)

// Enum value maps for TextToTextStopReason.
var (
	TextToTextStopReason_name = map[int32]string{
		0: "TEXT_TO_TEXT_STOP_REASON_UNSPECIFIED",
		1: "TEXT_TO_TEXT_STOP_REASON_END_TURN",
		2: "TEXT_TO_TEXT_STOP_REASON_MAX_TOKENS",
		3: "TEXT_TO_TEXT_STOP_REASON_TOOL_CALL",
		4: "TEXT_TO_TEXT_STOP_REASON_STOP_SEQUENCE",
		5: "TEXT_TO_TEXT_STOP_REASON_PAUSE_TURN",
		6: "TEXT_TO_TEXT_STOP_REASON_REFUSAL",
	}
	TextToTextStopReason_value = map[string]int32{
		"TEXT_TO_TEXT_STOP_REASON_UNSPECIFIED":   0,
		"TEXT_TO_TEXT_STOP_REASON_END_TURN":      1,
		"TEXT_TO_TEXT_STOP_REASON_MAX_TOKENS":    2,
		"TEXT_TO_TEXT_STOP_REASON_TOOL_CALL":     3,
		"TEXT_TO_TEXT_STOP_REASON_STOP_SEQUENCE": 4,
		"TEXT_TO_TEXT_STOP_REASON_PAUSE_TURN":    5,
		"TEXT_TO_TEXT_STOP_REASON_REFUSAL":       6,
	}
)

func (x TextToTextStopReason) Enum() *TextToTextStopReason {
	p := new(TextToTextStopReason)
	*p = x
	return p
}

func (x TextToTextStopReason) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (TextToTextStopReason) Descriptor() protoreflect.EnumDescriptor {
	return file_malonaz_ai_ai_service_v1_text_to_text_proto_enumTypes[0].Descriptor()
}

func (TextToTextStopReason) Type() protoreflect.EnumType {
	return &file_malonaz_ai_ai_service_v1_text_to_text_proto_enumTypes[0]
}

func (x TextToTextStopReason) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Configuration for text to text generation.
type TextToTextConfiguration struct {
	state                             protoimpl.MessageState `protogen:"opaque.v1"`
	xxx_hidden_MaxTokens              int32                  `protobuf:"varint,1,opt,name=max_tokens,json=maxTokens,proto3"`
	xxx_hidden_Temperature            float64                `protobuf:"fixed64,2,opt,name=temperature,proto3"`
	xxx_hidden_ToolChoice             *v1.ToolChoice         `protobuf:"bytes,3,opt,name=tool_choice,json=toolChoice,proto3"`
	xxx_hidden_ReasoningEffort        v1.ReasoningEffort     `protobuf:"varint,4,opt,name=reasoning_effort,json=reasoningEffort,proto3,enum=malonaz.ai.v1.ReasoningEffort"`
	xxx_hidden_ExtractJsonObject      bool                   `protobuf:"varint,5,opt,name=extract_json_object,json=extractJsonObject,proto3"`
	xxx_hidden_StreamPartialToolCalls bool                   `protobuf:"varint,6,opt,name=stream_partial_tool_calls,json=streamPartialToolCalls,proto3"`
	xxx_hidden_ImageConfig            *ImageGenerationConfig `protobuf:"bytes,7,opt,name=image_config,json=imageConfig,proto3"`
	unknownFields                     protoimpl.UnknownFields
	sizeCache                         protoimpl.SizeCache
}

func (x *TextToTextConfiguration) Reset() {
	*x = TextToTextConfiguration{}
	mi := &file_malonaz_ai_ai_service_v1_text_to_text_proto_msgTypes[0]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TextToTextConfiguration) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TextToTextConfiguration) ProtoMessage() {}

func (x *TextToTextConfiguration) ProtoReflect() protoreflect.Message {
	mi := &file_malonaz_ai_ai_service_v1_text_to_text_proto_msgTypes[0]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

func (x *TextToTextConfiguration) GetMaxTokens() int32 {
	if x != nil {
		return x.xxx_hidden_MaxTokens
	}
	return 0
}

func (x *TextToTextConfiguration) GetTemperature() float64 {
	if x != nil {
		return x.xxx_hidden_Temperature
	}
	return 0
}

func (x *TextToTextConfiguration) GetToolChoice() *v1.ToolChoice {
	if x != nil {
		return x.xxx_hidden_ToolChoice
	}
	return nil
}

func (x *TextToTextConfiguration) GetReasoningEffort() v1.ReasoningEffort {
	if x != nil {
		return x.xxx_hidden_ReasoningEffort
	}
	return v1.ReasoningEffort(0)
}

func (x *TextToTextConfiguration) GetExtractJsonObject() bool {
	if x != nil {
		return x.xxx_hidden_ExtractJsonObject
	}
	return false
}

func (x *TextToTextConfiguration) GetStreamPartialToolCalls() bool {
	if x != nil {
		return x.xxx_hidden_StreamPartialToolCalls
	}
	return false
}

func (x *TextToTextConfiguration) GetImageConfig() *ImageGenerationConfig {
	if x != nil {
		return x.xxx_hidden_ImageConfig
	}
	return nil
}

func (x *TextToTextConfiguration) SetMaxTokens(v int32) {
	x.xxx_hidden_MaxTokens = v
}

func (x *TextToTextConfiguration) SetTemperature(v float64) {
	x.xxx_hidden_Temperature = v
}

func (x *TextToTextConfiguration) SetToolChoice(v *v1.ToolChoice) {
	x.xxx_hidden_ToolChoice = v
}

func (x *TextToTextConfiguration) SetReasoningEffort(v v1.ReasoningEffort) {
	x.xxx_hidden_ReasoningEffort = v
}

func (x *TextToTextConfiguration) SetExtractJsonObject(v bool) {
	x.xxx_hidden_ExtractJsonObject = v
}

func (x *TextToTextConfiguration) SetStreamPartialToolCalls(v bool) {
	x.xxx_hidden_StreamPartialToolCalls = v
}

func (x *TextToTextConfiguration) SetImageConfig(v *ImageGenerationConfig) {
	x.xxx_hidden_ImageConfig = v
}

func (x *TextToTextConfiguration) HasToolChoice() bool {
	if x == nil {
		return false
	}
	return x.xxx_hidden_ToolChoice != nil
}

func (x *TextToTextConfiguration) HasImageConfig() bool {
	if x == nil {
		return false
	}
	return x.xxx_hidden_ImageConfig != nil
}

func (x *TextToTextConfiguration) ClearToolChoice() {
	x.xxx_hidden_ToolChoice = nil
}

func (x *TextToTextConfiguration) ClearImageConfig() {
	x.xxx_hidden_ImageConfig = nil
}

type TextToTextConfiguration_builder struct {
	_ [0]func() // Prevents comparability and use of unkeyed literals for the builder.

	// Maximum number of tokens to generate. Includes reasoning tokens.
	MaxTokens int32
	// Sampling temperature (0.0 to 2.0).
	Temperature float64
	// Controls which tool(s) the model should use.
	ToolChoice *v1.ToolChoice
	// Represents the level of reasoning effort for AI model responses.
	// The reasoning effort parameter guides the model on how many reasoning tokens
	// to generate before creating a response to the prompt. Higher effort levels
	// result in more thorough reasoning at the cost of speed and token usage.
	ReasoningEffort v1.ReasoningEffort
	// If true, we attempt to clean the output to extract a json object.
	// Fails the request if a json object cannot be found.
	ExtractJsonObject bool
	// If true, we stream partial tool calls.
	StreamPartialToolCalls bool
	// Image generation configuration.
	ImageConfig *ImageGenerationConfig
}

func (b0 TextToTextConfiguration_builder) Build() *TextToTextConfiguration {
	m0 := &TextToTextConfiguration{}
	b, x := &b0, m0
	_, _ = b, x
	x.xxx_hidden_MaxTokens = b.MaxTokens
	x.xxx_hidden_Temperature = b.Temperature
	x.xxx_hidden_ToolChoice = b.ToolChoice
	x.xxx_hidden_ReasoningEffort = b.ReasoningEffort
	x.xxx_hidden_ExtractJsonObject = b.ExtractJsonObject
	x.xxx_hidden_StreamPartialToolCalls = b.StreamPartialToolCalls
	x.xxx_hidden_ImageConfig = b.ImageConfig
	return m0
}

// Configuration for image generation in text-to-text requests.
// Used with models that support native image generation (e.g., Gemini 2.5 Flash Image, Gemini 3 Pro Image).
type ImageGenerationConfig struct {
	state                  protoimpl.MessageState `protogen:"opaque.v1"`
	xxx_hidden_AspectRatio string                 `protobuf:"bytes,1,opt,name=aspect_ratio,json=aspectRatio,proto3"`
	xxx_hidden_ImageSize   string                 `protobuf:"bytes,2,opt,name=image_size,json=imageSize,proto3"`
	unknownFields          protoimpl.UnknownFields
	sizeCache              protoimpl.SizeCache
}

func (x *ImageGenerationConfig) Reset() {
	*x = ImageGenerationConfig{}
	mi := &file_malonaz_ai_ai_service_v1_text_to_text_proto_msgTypes[1]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ImageGenerationConfig) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ImageGenerationConfig) ProtoMessage() {}

func (x *ImageGenerationConfig) ProtoReflect() protoreflect.Message {
	mi := &file_malonaz_ai_ai_service_v1_text_to_text_proto_msgTypes[1]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

func (x *ImageGenerationConfig) GetAspectRatio() string {
	if x != nil {
		return x.xxx_hidden_AspectRatio
	}
	return ""
}

func (x *ImageGenerationConfig) GetImageSize() string {
	if x != nil {
		return x.xxx_hidden_ImageSize
	}
	return ""
}

func (x *ImageGenerationConfig) SetAspectRatio(v string) {
	x.xxx_hidden_AspectRatio = v
}

func (x *ImageGenerationConfig) SetImageSize(v string) {
	x.xxx_hidden_ImageSize = v
}

type ImageGenerationConfig_builder struct {
	_ [0]func() // Prevents comparability and use of unkeyed literals for the builder.

	// Aspect ratio of generated images.
	// Defaults to matching input image size, or "1:1" if no input image.
	AspectRatio string
	// Resolution of generated images. Only supported by Gemini 3 Pro Image.
	// Defaults to "1K".
	ImageSize string
}

func (b0 ImageGenerationConfig_builder) Build() *ImageGenerationConfig {
	m0 := &ImageGenerationConfig{}
	b, x := &b0, m0
	_, _ = b, x
	x.xxx_hidden_AspectRatio = b.AspectRatio
	x.xxx_hidden_ImageSize = b.ImageSize
	return m0
}

// Request message for AiService.TextToText.
type TextToTextRequest struct {
	state                    protoimpl.MessageState   `protogen:"opaque.v1"`
	xxx_hidden_Model         string                   `protobuf:"bytes,1,opt,name=model,proto3"`
	xxx_hidden_Messages      *[]*v1.Message           `protobuf:"bytes,2,rep,name=messages,proto3"`
	xxx_hidden_Tools         *[]*v1.Tool              `protobuf:"bytes,3,rep,name=tools,proto3"`
	xxx_hidden_Configuration *TextToTextConfiguration `protobuf:"bytes,4,opt,name=configuration,proto3"`
	unknownFields            protoimpl.UnknownFields
	sizeCache                protoimpl.SizeCache
}

func (x *TextToTextRequest) Reset() {
	*x = TextToTextRequest{}
	mi := &file_malonaz_ai_ai_service_v1_text_to_text_proto_msgTypes[2]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TextToTextRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TextToTextRequest) ProtoMessage() {}

func (x *TextToTextRequest) ProtoReflect() protoreflect.Message {
	mi := &file_malonaz_ai_ai_service_v1_text_to_text_proto_msgTypes[2]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

func (x *TextToTextRequest) GetModel() string {
	if x != nil {
		return x.xxx_hidden_Model
	}
	return ""
}

func (x *TextToTextRequest) GetMessages() []*v1.Message {
	if x != nil {
		if x.xxx_hidden_Messages != nil {
			return *x.xxx_hidden_Messages
		}
	}
	return nil
}

func (x *TextToTextRequest) GetTools() []*v1.Tool {
	if x != nil {
		if x.xxx_hidden_Tools != nil {
			return *x.xxx_hidden_Tools
		}
	}
	return nil
}

func (x *TextToTextRequest) GetConfiguration() *TextToTextConfiguration {
	if x != nil {
		return x.xxx_hidden_Configuration
	}
	return nil
}

func (x *TextToTextRequest) SetModel(v string) {
	x.xxx_hidden_Model = v
}

func (x *TextToTextRequest) SetMessages(v []*v1.Message) {
	x.xxx_hidden_Messages = &v
}

func (x *TextToTextRequest) SetTools(v []*v1.Tool) {
	x.xxx_hidden_Tools = &v
}

func (x *TextToTextRequest) SetConfiguration(v *TextToTextConfiguration) {
	x.xxx_hidden_Configuration = v
}

func (x *TextToTextRequest) HasConfiguration() bool {
	if x == nil {
		return false
	}
	return x.xxx_hidden_Configuration != nil
}

func (x *TextToTextRequest) ClearConfiguration() {
	x.xxx_hidden_Configuration = nil
}

type TextToTextRequest_builder struct {
	_ [0]func() // Prevents comparability and use of unkeyed literals for the builder.

	// The resource name of the model used.
	// Format: providers/{provider}/models/{model}
	Model string
	// The conversation messages.
	Messages []*v1.Message
	// Tools available for the model to call.
	Tools []*v1.Tool
	// Additional configuration.
	Configuration *TextToTextConfiguration
}

func (b0 TextToTextRequest_builder) Build() *TextToTextRequest {
	m0 := &TextToTextRequest{}
	b, x := &b0, m0
	_, _ = b, x
	x.xxx_hidden_Model = b.Model
	x.xxx_hidden_Messages = &b.Messages
	x.xxx_hidden_Tools = &b.Tools
	x.xxx_hidden_Configuration = b.Configuration
	return m0
}

// Response message for AiService.TextToText.
type TextToTextResponse struct {
	state                        protoimpl.MessageState `protogen:"opaque.v1"`
	xxx_hidden_Message           *v1.Message            `protobuf:"bytes,1,opt,name=message,proto3"`
	xxx_hidden_StopReason        TextToTextStopReason   `protobuf:"varint,2,opt,name=stop_reason,json=stopReason,proto3,enum=malonaz.ai.ai_service.v1.TextToTextStopReason"`
	xxx_hidden_ModelUsage        *v1.ModelUsage         `protobuf:"bytes,3,opt,name=model_usage,json=modelUsage,proto3"`
	xxx_hidden_GenerationMetrics *v1.GenerationMetrics  `protobuf:"bytes,4,opt,name=generation_metrics,json=generationMetrics,proto3"`
	unknownFields                protoimpl.UnknownFields
	sizeCache                    protoimpl.SizeCache
}

func (x *TextToTextResponse) Reset() {
	*x = TextToTextResponse{}
	mi := &file_malonaz_ai_ai_service_v1_text_to_text_proto_msgTypes[3]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TextToTextResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TextToTextResponse) ProtoMessage() {}

func (x *TextToTextResponse) ProtoReflect() protoreflect.Message {
	mi := &file_malonaz_ai_ai_service_v1_text_to_text_proto_msgTypes[3]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

func (x *TextToTextResponse) GetMessage() *v1.Message {
	if x != nil {
		return x.xxx_hidden_Message
	}
	return nil
}

func (x *TextToTextResponse) GetStopReason() TextToTextStopReason {
	if x != nil {
		return x.xxx_hidden_StopReason
	}
	return TextToTextStopReason_TEXT_TO_TEXT_STOP_REASON_UNSPECIFIED
}

func (x *TextToTextResponse) GetModelUsage() *v1.ModelUsage {
	if x != nil {
		return x.xxx_hidden_ModelUsage
	}
	return nil
}

func (x *TextToTextResponse) GetGenerationMetrics() *v1.GenerationMetrics {
	if x != nil {
		return x.xxx_hidden_GenerationMetrics
	}
	return nil
}

func (x *TextToTextResponse) SetMessage(v *v1.Message) {
	x.xxx_hidden_Message = v
}

func (x *TextToTextResponse) SetStopReason(v TextToTextStopReason) {
	x.xxx_hidden_StopReason = v
}

func (x *TextToTextResponse) SetModelUsage(v *v1.ModelUsage) {
	x.xxx_hidden_ModelUsage = v
}

func (x *TextToTextResponse) SetGenerationMetrics(v *v1.GenerationMetrics) {
	x.xxx_hidden_GenerationMetrics = v
}

func (x *TextToTextResponse) HasMessage() bool {
	if x == nil {
		return false
	}
	return x.xxx_hidden_Message != nil
}

func (x *TextToTextResponse) HasModelUsage() bool {
	if x == nil {
		return false
	}
	return x.xxx_hidden_ModelUsage != nil
}

func (x *TextToTextResponse) HasGenerationMetrics() bool {
	if x == nil {
		return false
	}
	return x.xxx_hidden_GenerationMetrics != nil
}

func (x *TextToTextResponse) ClearMessage() {
	x.xxx_hidden_Message = nil
}

func (x *TextToTextResponse) ClearModelUsage() {
	x.xxx_hidden_ModelUsage = nil
}

func (x *TextToTextResponse) ClearGenerationMetrics() {
	x.xxx_hidden_GenerationMetrics = nil
}

type TextToTextResponse_builder struct {
	_ [0]func() // Prevents comparability and use of unkeyed literals for the builder.

	// The generated message.
	Message *v1.Message
	// Reason why generation stopped.
	StopReason TextToTextStopReason
	// Model usage metrics.
	ModelUsage *v1.ModelUsage
	// Generation metrics.
	GenerationMetrics *v1.GenerationMetrics
}

func (b0 TextToTextResponse_builder) Build() *TextToTextResponse {
	m0 := &TextToTextResponse{}
	b, x := &b0, m0
	_, _ = b, x
	x.xxx_hidden_Message = b.Message
	x.xxx_hidden_StopReason = b.StopReason
	x.xxx_hidden_ModelUsage = b.ModelUsage
	x.xxx_hidden_GenerationMetrics = b.GenerationMetrics
	return m0
}

// Request message for AiService.TextToTextStream.
type TextToTextStreamRequest struct {
	state                    protoimpl.MessageState   `protogen:"opaque.v1"`
	xxx_hidden_Model         string                   `protobuf:"bytes,1,opt,name=model,proto3"`
	xxx_hidden_Messages      *[]*v1.Message           `protobuf:"bytes,2,rep,name=messages,proto3"`
	xxx_hidden_Tools         *[]*v1.Tool              `protobuf:"bytes,3,rep,name=tools,proto3"`
	xxx_hidden_ToolChoice    string                   `protobuf:"bytes,4,opt,name=tool_choice,json=toolChoice,proto3"`
	xxx_hidden_Configuration *TextToTextConfiguration `protobuf:"bytes,5,opt,name=configuration,proto3"`
	unknownFields            protoimpl.UnknownFields
	sizeCache                protoimpl.SizeCache
}

func (x *TextToTextStreamRequest) Reset() {
	*x = TextToTextStreamRequest{}
	mi := &file_malonaz_ai_ai_service_v1_text_to_text_proto_msgTypes[4]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TextToTextStreamRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TextToTextStreamRequest) ProtoMessage() {}

func (x *TextToTextStreamRequest) ProtoReflect() protoreflect.Message {
	mi := &file_malonaz_ai_ai_service_v1_text_to_text_proto_msgTypes[4]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

func (x *TextToTextStreamRequest) GetModel() string {
	if x != nil {
		return x.xxx_hidden_Model
	}
	return ""
}

func (x *TextToTextStreamRequest) GetMessages() []*v1.Message {
	if x != nil {
		if x.xxx_hidden_Messages != nil {
			return *x.xxx_hidden_Messages
		}
	}
	return nil
}

func (x *TextToTextStreamRequest) GetTools() []*v1.Tool {
	if x != nil {
		if x.xxx_hidden_Tools != nil {
			return *x.xxx_hidden_Tools
		}
	}
	return nil
}

func (x *TextToTextStreamRequest) GetToolChoice() string {
	if x != nil {
		return x.xxx_hidden_ToolChoice
	}
	return ""
}

func (x *TextToTextStreamRequest) GetConfiguration() *TextToTextConfiguration {
	if x != nil {
		return x.xxx_hidden_Configuration
	}
	return nil
}

func (x *TextToTextStreamRequest) SetModel(v string) {
	x.xxx_hidden_Model = v
}

func (x *TextToTextStreamRequest) SetMessages(v []*v1.Message) {
	x.xxx_hidden_Messages = &v
}

func (x *TextToTextStreamRequest) SetTools(v []*v1.Tool) {
	x.xxx_hidden_Tools = &v
}

func (x *TextToTextStreamRequest) SetToolChoice(v string) {
	x.xxx_hidden_ToolChoice = v
}

func (x *TextToTextStreamRequest) SetConfiguration(v *TextToTextConfiguration) {
	x.xxx_hidden_Configuration = v
}

func (x *TextToTextStreamRequest) HasConfiguration() bool {
	if x == nil {
		return false
	}
	return x.xxx_hidden_Configuration != nil
}

func (x *TextToTextStreamRequest) ClearConfiguration() {
	x.xxx_hidden_Configuration = nil
}

type TextToTextStreamRequest_builder struct {
	_ [0]func() // Prevents comparability and use of unkeyed literals for the builder.

	// The resource name of the model used.
	// Format: providers/{provider}/models/{model}
	Model string
	// The conversation messages.
	Messages []*v1.Message
	// Tools available for the model to call.
	Tools []*v1.Tool
	// For the model to use a tool.
	ToolChoice string
	// Additional configuration.
	Configuration *TextToTextConfiguration
}

func (b0 TextToTextStreamRequest_builder) Build() *TextToTextStreamRequest {
	m0 := &TextToTextStreamRequest{}
	b, x := &b0, m0
	_, _ = b, x
	x.xxx_hidden_Model = b.Model
	x.xxx_hidden_Messages = &b.Messages
	x.xxx_hidden_Tools = &b.Tools
	x.xxx_hidden_ToolChoice = b.ToolChoice
	x.xxx_hidden_Configuration = b.Configuration
	return m0
}

// Response message for AiService.TextToTextStream.
type TextToTextStreamResponse struct {
	state              protoimpl.MessageState             `protogen:"opaque.v1"`
	xxx_hidden_Content isTextToTextStreamResponse_Content `protobuf_oneof:"content"`
	unknownFields      protoimpl.UnknownFields
	sizeCache          protoimpl.SizeCache
}

func (x *TextToTextStreamResponse) Reset() {
	*x = TextToTextStreamResponse{}
	mi := &file_malonaz_ai_ai_service_v1_text_to_text_proto_msgTypes[5]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TextToTextStreamResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TextToTextStreamResponse) ProtoMessage() {}

func (x *TextToTextStreamResponse) ProtoReflect() protoreflect.Message {
	mi := &file_malonaz_ai_ai_service_v1_text_to_text_proto_msgTypes[5]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

func (x *TextToTextStreamResponse) GetBlock() *v1.Block {
	if x != nil {
		if x, ok := x.xxx_hidden_Content.(*textToTextStreamResponse_Block); ok {
			return x.Block
		}
	}
	return nil
}

func (x *TextToTextStreamResponse) GetStopReason() TextToTextStopReason {
	if x != nil {
		if x, ok := x.xxx_hidden_Content.(*textToTextStreamResponse_StopReason); ok {
			return x.StopReason
		}
	}
	return TextToTextStopReason_TEXT_TO_TEXT_STOP_REASON_UNSPECIFIED
}

func (x *TextToTextStreamResponse) GetModelUsage() *v1.ModelUsage {
	if x != nil {
		if x, ok := x.xxx_hidden_Content.(*textToTextStreamResponse_ModelUsage); ok {
			return x.ModelUsage
		}
	}
	return nil
}

func (x *TextToTextStreamResponse) GetGenerationMetrics() *v1.GenerationMetrics {
	if x != nil {
		if x, ok := x.xxx_hidden_Content.(*textToTextStreamResponse_GenerationMetrics); ok {
			return x.GenerationMetrics
		}
	}
	return nil
}

func (x *TextToTextStreamResponse) SetBlock(v *v1.Block) {
	if v == nil {
		x.xxx_hidden_Content = nil
		return
	}
	x.xxx_hidden_Content = &textToTextStreamResponse_Block{v}
}

func (x *TextToTextStreamResponse) SetStopReason(v TextToTextStopReason) {
	x.xxx_hidden_Content = &textToTextStreamResponse_StopReason{v}
}

func (x *TextToTextStreamResponse) SetModelUsage(v *v1.ModelUsage) {
	if v == nil {
		x.xxx_hidden_Content = nil
		return
	}
	x.xxx_hidden_Content = &textToTextStreamResponse_ModelUsage{v}
}

func (x *TextToTextStreamResponse) SetGenerationMetrics(v *v1.GenerationMetrics) {
	if v == nil {
		x.xxx_hidden_Content = nil
		return
	}
	x.xxx_hidden_Content = &textToTextStreamResponse_GenerationMetrics{v}
}

func (x *TextToTextStreamResponse) HasContent() bool {
	if x == nil {
		return false
	}
	return x.xxx_hidden_Content != nil
}

func (x *TextToTextStreamResponse) HasBlock() bool {
	if x == nil {
		return false
	}
	_, ok := x.xxx_hidden_Content.(*textToTextStreamResponse_Block)
	return ok
}

func (x *TextToTextStreamResponse) HasStopReason() bool {
	if x == nil {
		return false
	}
	_, ok := x.xxx_hidden_Content.(*textToTextStreamResponse_StopReason)
	return ok
}

func (x *TextToTextStreamResponse) HasModelUsage() bool {
	if x == nil {
		return false
	}
	_, ok := x.xxx_hidden_Content.(*textToTextStreamResponse_ModelUsage)
	return ok
}

func (x *TextToTextStreamResponse) HasGenerationMetrics() bool {
	if x == nil {
		return false
	}
	_, ok := x.xxx_hidden_Content.(*textToTextStreamResponse_GenerationMetrics)
	return ok
}

func (x *TextToTextStreamResponse) ClearContent() {
	x.xxx_hidden_Content = nil
}

func (x *TextToTextStreamResponse) ClearBlock() {
	if _, ok := x.xxx_hidden_Content.(*textToTextStreamResponse_Block); ok {
		x.xxx_hidden_Content = nil
	}
}

func (x *TextToTextStreamResponse) ClearStopReason() {
	if _, ok := x.xxx_hidden_Content.(*textToTextStreamResponse_StopReason); ok {
		x.xxx_hidden_Content = nil
	}
}

func (x *TextToTextStreamResponse) ClearModelUsage() {
	if _, ok := x.xxx_hidden_Content.(*textToTextStreamResponse_ModelUsage); ok {
		x.xxx_hidden_Content = nil
	}
}

func (x *TextToTextStreamResponse) ClearGenerationMetrics() {
	if _, ok := x.xxx_hidden_Content.(*textToTextStreamResponse_GenerationMetrics); ok {
		x.xxx_hidden_Content = nil
	}
}

const TextToTextStreamResponse_Content_not_set_case case_TextToTextStreamResponse_Content = 0
const TextToTextStreamResponse_Block_case case_TextToTextStreamResponse_Content = 1
const TextToTextStreamResponse_StopReason_case case_TextToTextStreamResponse_Content = 2
const TextToTextStreamResponse_ModelUsage_case case_TextToTextStreamResponse_Content = 3
const TextToTextStreamResponse_GenerationMetrics_case case_TextToTextStreamResponse_Content = 4

func (x *TextToTextStreamResponse) WhichContent() case_TextToTextStreamResponse_Content {
	if x == nil {
		return TextToTextStreamResponse_Content_not_set_case
	}
	switch x.xxx_hidden_Content.(type) {
	case *textToTextStreamResponse_Block:
		return TextToTextStreamResponse_Block_case
	case *textToTextStreamResponse_StopReason:
		return TextToTextStreamResponse_StopReason_case
	case *textToTextStreamResponse_ModelUsage:
		return TextToTextStreamResponse_ModelUsage_case
	case *textToTextStreamResponse_GenerationMetrics:
		return TextToTextStreamResponse_GenerationMetrics_case
	default:
		return TextToTextStreamResponse_Content_not_set_case
	}
}

type TextToTextStreamResponse_builder struct {
	_ [0]func() // Prevents comparability and use of unkeyed literals for the builder.

	// Content of this response.

	// Fields of oneof xxx_hidden_Content:
	// A generated block.
	Block *v1.Block
	// Reason why generation stopped.
	StopReason *TextToTextStopReason
	// Model usage event (sent last).
	ModelUsage *v1.ModelUsage
	// Generation metrics (sent last).
	GenerationMetrics *v1.GenerationMetrics
	// -- end of xxx_hidden_Content
}

func (b0 TextToTextStreamResponse_builder) Build() *TextToTextStreamResponse {
	m0 := &TextToTextStreamResponse{}
	b, x := &b0, m0
	_, _ = b, x
	if b.Block != nil {
		x.xxx_hidden_Content = &textToTextStreamResponse_Block{b.Block}
	}
	if b.StopReason != nil {
		x.xxx_hidden_Content = &textToTextStreamResponse_StopReason{*b.StopReason}
	}
	if b.ModelUsage != nil {
		x.xxx_hidden_Content = &textToTextStreamResponse_ModelUsage{b.ModelUsage}
	}
	if b.GenerationMetrics != nil {
		x.xxx_hidden_Content = &textToTextStreamResponse_GenerationMetrics{b.GenerationMetrics}
	}
	return m0
}

type case_TextToTextStreamResponse_Content protoreflect.FieldNumber

func (x case_TextToTextStreamResponse_Content) String() string {
	md := file_malonaz_ai_ai_service_v1_text_to_text_proto_msgTypes[5].Descriptor()
	if x == 0 {
		return "not set"
	}
	return protoimpl.X.MessageFieldStringOf(md, protoreflect.FieldNumber(x))
}

type isTextToTextStreamResponse_Content interface {
	isTextToTextStreamResponse_Content()
}

type textToTextStreamResponse_Block struct {
	// A generated block.
	Block *v1.Block `protobuf:"bytes,1,opt,name=block,proto3,oneof"`
}

type textToTextStreamResponse_StopReason struct {
	// Reason why generation stopped.
	StopReason TextToTextStopReason `protobuf:"varint,2,opt,name=stop_reason,json=stopReason,proto3,enum=malonaz.ai.ai_service.v1.TextToTextStopReason,oneof"`
}

type textToTextStreamResponse_ModelUsage struct {
	// Model usage event (sent last).
	ModelUsage *v1.ModelUsage `protobuf:"bytes,3,opt,name=model_usage,json=modelUsage,proto3,oneof"`
}

type textToTextStreamResponse_GenerationMetrics struct {
	// Generation metrics (sent last).
	GenerationMetrics *v1.GenerationMetrics `protobuf:"bytes,4,opt,name=generation_metrics,json=generationMetrics,proto3,oneof"`
}

func (*textToTextStreamResponse_Block) isTextToTextStreamResponse_Content() {}

func (*textToTextStreamResponse_StopReason) isTextToTextStreamResponse_Content() {}

func (*textToTextStreamResponse_ModelUsage) isTextToTextStreamResponse_Content() {}

func (*textToTextStreamResponse_GenerationMetrics) isTextToTextStreamResponse_Content() {}

var File_malonaz_ai_ai_service_v1_text_to_text_proto protoreflect.FileDescriptor

const file_malonaz_ai_ai_service_v1_text_to_text_proto_rawDesc = "" +
	"\n" +
	"+malonaz/ai/ai_service/v1/text_to_text.proto\x12\x18malonaz.ai.ai_service.v1\x1a\x1bbuf/validate/validate.proto\x1a\x19google/api/resource.proto\x1a\x1bmalonaz/ai/v1/message.proto\x1a\x1bmalonaz/ai/v1/metrics.proto\x1a\x18malonaz/ai/v1/tool.proto\"\xc2\x03\n" +
	"\x17TextToTextConfiguration\x12&\n" +
	"\n" +
	"max_tokens\x18\x01 \x01(\x05B\a\xbaH\x04\x1a\x02(\x00R\tmaxTokens\x129\n" +
	"\vtemperature\x18\x02 \x01(\x01B\x17\xbaH\x14\x12\x12\x19\x00\x00\x00\x00\x00\x00\x00@)\x00\x00\x00\x00\x00\x00\x00\x00R\vtemperature\x12:\n" +
	"\vtool_choice\x18\x03 \x01(\v2\x19.malonaz.ai.v1.ToolChoiceR\n" +
	"toolChoice\x12I\n" +
	"\x10reasoning_effort\x18\x04 \x01(\x0e2\x1e.malonaz.ai.v1.ReasoningEffortR\x0freasoningEffort\x12.\n" +
	"\x13extract_json_object\x18\x05 \x01(\bR\x11extractJsonObject\x129\n" +
	"\x19stream_partial_tool_calls\x18\x06 \x01(\bR\x16streamPartialToolCalls\x12R\n" +
	"\fimage_config\x18\a \x01(\v2/.malonaz.ai.ai_service.v1.ImageGenerationConfigR\vimageConfig\"\xac\x01\n" +
	"\x15ImageGenerationConfig\x12_\n" +
	"\faspect_ratio\x18\x01 \x01(\tB<\xbaH9r7R\x00R\x031:1R\x032:3R\x033:2R\x033:4R\x034:3R\x034:5R\x035:4R\x049:16R\x0416:9R\x0421:9R\vaspectRatio\x122\n" +
	"\n" +
	"image_size\x18\x02 \x01(\tB\x13\xbaH\x10r\x0eR\x00R\x021KR\x022KR\x024KR\timageSize\"\x8c\x02\n" +
	"\x11TextToTextRequest\x125\n" +
	"\x05model\x18\x01 \x01(\tB\x1f\xfaA\x16\n" +
	"\x14ai.malonaz.com/Model\xbaH\x03\xc8\x01\x01R\x05model\x12<\n" +
	"\bmessages\x18\x02 \x03(\v2\x16.malonaz.ai.v1.MessageB\b\xbaH\x05\x92\x01\x02\b\x01R\bmessages\x12)\n" +
	"\x05tools\x18\x03 \x03(\v2\x13.malonaz.ai.v1.ToolR\x05tools\x12W\n" +
	"\rconfiguration\x18\x04 \x01(\v21.malonaz.ai.ai_service.v1.TextToTextConfigurationR\rconfiguration\"\xa4\x02\n" +
	"\x12TextToTextResponse\x120\n" +
	"\amessage\x18\x01 \x01(\v2\x16.malonaz.ai.v1.MessageR\amessage\x12O\n" +
	"\vstop_reason\x18\x02 \x01(\x0e2..malonaz.ai.ai_service.v1.TextToTextStopReasonR\n" +
	"stopReason\x12:\n" +
	"\vmodel_usage\x18\x03 \x01(\v2\x19.malonaz.ai.v1.ModelUsageR\n" +
	"modelUsage\x12O\n" +
	"\x12generation_metrics\x18\x04 \x01(\v2 .malonaz.ai.v1.GenerationMetricsR\x11generationMetrics\"\xb3\x02\n" +
	"\x17TextToTextStreamRequest\x125\n" +
	"\x05model\x18\x01 \x01(\tB\x1f\xfaA\x16\n" +
	"\x14ai.malonaz.com/Model\xbaH\x03\xc8\x01\x01R\x05model\x12<\n" +
	"\bmessages\x18\x02 \x03(\v2\x16.malonaz.ai.v1.MessageB\b\xbaH\x05\x92\x01\x02\b\x01R\bmessages\x12)\n" +
	"\x05tools\x18\x03 \x03(\v2\x13.malonaz.ai.v1.ToolR\x05tools\x12\x1f\n" +
	"\vtool_choice\x18\x04 \x01(\tR\n" +
	"toolChoice\x12W\n" +
	"\rconfiguration\x18\x05 \x01(\v21.malonaz.ai.ai_service.v1.TextToTextConfigurationR\rconfiguration\"\xbe\x02\n" +
	"\x18TextToTextStreamResponse\x12,\n" +
	"\x05block\x18\x01 \x01(\v2\x14.malonaz.ai.v1.BlockH\x00R\x05block\x12Q\n" +
	"\vstop_reason\x18\x02 \x01(\x0e2..malonaz.ai.ai_service.v1.TextToTextStopReasonH\x00R\n" +
	"stopReason\x12<\n" +
	"\vmodel_usage\x18\x03 \x01(\v2\x19.malonaz.ai.v1.ModelUsageH\x00R\n" +
	"modelUsage\x12Q\n" +
	"\x12generation_metrics\x18\x04 \x01(\v2 .malonaz.ai.v1.GenerationMetricsH\x00R\x11generationMetricsB\x10\n" +
	"\acontent\x12\x05\xbaH\x02\b\x01*\xb3\x02\n" +
	"\x14TextToTextStopReason\x12(\n" +
	"$TEXT_TO_TEXT_STOP_REASON_UNSPECIFIED\x10\x00\x12%\n" +
	"!TEXT_TO_TEXT_STOP_REASON_END_TURN\x10\x01\x12'\n" +
	"#TEXT_TO_TEXT_STOP_REASON_MAX_TOKENS\x10\x02\x12&\n" +
	"\"TEXT_TO_TEXT_STOP_REASON_TOOL_CALL\x10\x03\x12*\n" +
	"&TEXT_TO_TEXT_STOP_REASON_STOP_SEQUENCE\x10\x04\x12'\n" +
	"#TEXT_TO_TEXT_STOP_REASON_PAUSE_TURN\x10\x05\x12$\n" +
	" TEXT_TO_TEXT_STOP_REASON_REFUSAL\x10\x06B3Z1github.com/malonaz/core/genproto/ai/ai_service/v1b\x06proto3"

var file_malonaz_ai_ai_service_v1_text_to_text_proto_enumTypes = make([]protoimpl.EnumInfo, 1)
var file_malonaz_ai_ai_service_v1_text_to_text_proto_msgTypes = make([]protoimpl.MessageInfo, 6)
var file_malonaz_ai_ai_service_v1_text_to_text_proto_goTypes = []any{
	(TextToTextStopReason)(0),        // 0: malonaz.ai.ai_service.v1.TextToTextStopReason
	(*TextToTextConfiguration)(nil),  // 1: malonaz.ai.ai_service.v1.TextToTextConfiguration
	(*ImageGenerationConfig)(nil),    // 2: malonaz.ai.ai_service.v1.ImageGenerationConfig
	(*TextToTextRequest)(nil),        // 3: malonaz.ai.ai_service.v1.TextToTextRequest
	(*TextToTextResponse)(nil),       // 4: malonaz.ai.ai_service.v1.TextToTextResponse
	(*TextToTextStreamRequest)(nil),  // 5: malonaz.ai.ai_service.v1.TextToTextStreamRequest
	(*TextToTextStreamResponse)(nil), // 6: malonaz.ai.ai_service.v1.TextToTextStreamResponse
	(*v1.ToolChoice)(nil),            // 7: malonaz.ai.v1.ToolChoice
	(v1.ReasoningEffort)(0),          // 8: malonaz.ai.v1.ReasoningEffort
	(*v1.Message)(nil),               // 9: malonaz.ai.v1.Message
	(*v1.Tool)(nil),                  // 10: malonaz.ai.v1.Tool
	(*v1.ModelUsage)(nil),            // 11: malonaz.ai.v1.ModelUsage
	(*v1.GenerationMetrics)(nil),     // 12: malonaz.ai.v1.GenerationMetrics
	(*v1.Block)(nil),                 // 13: malonaz.ai.v1.Block
}
var file_malonaz_ai_ai_service_v1_text_to_text_proto_depIdxs = []int32{
	7,  // 0: malonaz.ai.ai_service.v1.TextToTextConfiguration.tool_choice:type_name -> malonaz.ai.v1.ToolChoice
	8,  // 1: malonaz.ai.ai_service.v1.TextToTextConfiguration.reasoning_effort:type_name -> malonaz.ai.v1.ReasoningEffort
	2,  // 2: malonaz.ai.ai_service.v1.TextToTextConfiguration.image_config:type_name -> malonaz.ai.ai_service.v1.ImageGenerationConfig
	9,  // 3: malonaz.ai.ai_service.v1.TextToTextRequest.messages:type_name -> malonaz.ai.v1.Message
	10, // 4: malonaz.ai.ai_service.v1.TextToTextRequest.tools:type_name -> malonaz.ai.v1.Tool
	1,  // 5: malonaz.ai.ai_service.v1.TextToTextRequest.configuration:type_name -> malonaz.ai.ai_service.v1.TextToTextConfiguration
	9,  // 6: malonaz.ai.ai_service.v1.TextToTextResponse.message:type_name -> malonaz.ai.v1.Message
	0,  // 7: malonaz.ai.ai_service.v1.TextToTextResponse.stop_reason:type_name -> malonaz.ai.ai_service.v1.TextToTextStopReason
	11, // 8: malonaz.ai.ai_service.v1.TextToTextResponse.model_usage:type_name -> malonaz.ai.v1.ModelUsage
	12, // 9: malonaz.ai.ai_service.v1.TextToTextResponse.generation_metrics:type_name -> malonaz.ai.v1.GenerationMetrics
	9,  // 10: malonaz.ai.ai_service.v1.TextToTextStreamRequest.messages:type_name -> malonaz.ai.v1.Message
	10, // 11: malonaz.ai.ai_service.v1.TextToTextStreamRequest.tools:type_name -> malonaz.ai.v1.Tool
	1,  // 12: malonaz.ai.ai_service.v1.TextToTextStreamRequest.configuration:type_name -> malonaz.ai.ai_service.v1.TextToTextConfiguration
	13, // 13: malonaz.ai.ai_service.v1.TextToTextStreamResponse.block:type_name -> malonaz.ai.v1.Block
	0,  // 14: malonaz.ai.ai_service.v1.TextToTextStreamResponse.stop_reason:type_name -> malonaz.ai.ai_service.v1.TextToTextStopReason
	11, // 15: malonaz.ai.ai_service.v1.TextToTextStreamResponse.model_usage:type_name -> malonaz.ai.v1.ModelUsage
	12, // 16: malonaz.ai.ai_service.v1.TextToTextStreamResponse.generation_metrics:type_name -> malonaz.ai.v1.GenerationMetrics
	17, // [17:17] is the sub-list for method output_type
	17, // [17:17] is the sub-list for method input_type
	17, // [17:17] is the sub-list for extension type_name
	17, // [17:17] is the sub-list for extension extendee
	0,  // [0:17] is the sub-list for field type_name
}

func init() { file_malonaz_ai_ai_service_v1_text_to_text_proto_init() }
func file_malonaz_ai_ai_service_v1_text_to_text_proto_init() {
	if File_malonaz_ai_ai_service_v1_text_to_text_proto != nil {
		return
	}
	file_malonaz_ai_ai_service_v1_text_to_text_proto_msgTypes[5].OneofWrappers = []any{
		(*textToTextStreamResponse_Block)(nil),
		(*textToTextStreamResponse_StopReason)(nil),
		(*textToTextStreamResponse_ModelUsage)(nil),
		(*textToTextStreamResponse_GenerationMetrics)(nil),
	}
	type x struct{}
	out := protoimpl.TypeBuilder{
		File: protoimpl.DescBuilder{
			GoPackagePath: reflect.TypeOf(x{}).PkgPath(),
			RawDescriptor: unsafe.Slice(unsafe.StringData(file_malonaz_ai_ai_service_v1_text_to_text_proto_rawDesc), len(file_malonaz_ai_ai_service_v1_text_to_text_proto_rawDesc)),
			NumEnums:      1,
			NumMessages:   6,
			NumExtensions: 0,
			NumServices:   0,
		},
		GoTypes:           file_malonaz_ai_ai_service_v1_text_to_text_proto_goTypes,
		DependencyIndexes: file_malonaz_ai_ai_service_v1_text_to_text_proto_depIdxs,
		EnumInfos:         file_malonaz_ai_ai_service_v1_text_to_text_proto_enumTypes,
		MessageInfos:      file_malonaz_ai_ai_service_v1_text_to_text_proto_msgTypes,
	}.Build()
	File_malonaz_ai_ai_service_v1_text_to_text_proto = out.File
	file_malonaz_ai_ai_service_v1_text_to_text_proto_goTypes = nil
	file_malonaz_ai_ai_service_v1_text_to_text_proto_depIdxs = nil
}
