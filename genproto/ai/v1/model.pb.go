// Code generated by protoc-gen-go. DO NOT EDIT.
// versions:
// 	protoc-gen-go v1.36.9
// 	protoc        v6.30.0
// source: ai/v1/model.proto

package v1

import (
	v1 "github.com/malonaz/core/genproto/audio/v1"
	protoreflect "google.golang.org/protobuf/reflect/protoreflect"
	protoimpl "google.golang.org/protobuf/runtime/protoimpl"
	descriptorpb "google.golang.org/protobuf/types/descriptorpb"
	reflect "reflect"
	sync "sync"
	unsafe "unsafe"
)

const (
	// Verify that this generated code is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(20 - protoimpl.MinVersion)
	// Verify that runtime/protoimpl is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(protoimpl.MaxVersion - 20)
)

// Represents an AI model provider.
type Provider int32

const (
	// Used to detect an unset field.
	Provider_PROVIDER_UNSPECIFIED Provider = 0
	// Open ai.
	Provider_PROVIDER_OPENAI Provider = 1
	// Anthropic.
	Provider_PROVIDER_ANTHROPIC Provider = 2
	// Eleven labs.
	Provider_PROVIDER_ELEVENLABS Provider = 3
	// Groq.
	Provider_PROVIDER_GROQ Provider = 4
	// Google.
	Provider_PROVIDER_GOOGLE Provider = 5
	// Cartesia.
	Provider_PROVIDER_CARTESIA Provider = 6
)

// Enum value maps for Provider.
var (
	Provider_name = map[int32]string{
		0: "PROVIDER_UNSPECIFIED",
		1: "PROVIDER_OPENAI",
		2: "PROVIDER_ANTHROPIC",
		3: "PROVIDER_ELEVENLABS",
		4: "PROVIDER_GROQ",
		5: "PROVIDER_GOOGLE",
		6: "PROVIDER_CARTESIA",
	}
	Provider_value = map[string]int32{
		"PROVIDER_UNSPECIFIED": 0,
		"PROVIDER_OPENAI":      1,
		"PROVIDER_ANTHROPIC":   2,
		"PROVIDER_ELEVENLABS":  3,
		"PROVIDER_GROQ":        4,
		"PROVIDER_GOOGLE":      5,
		"PROVIDER_CARTESIA":    6,
	}
)

func (x Provider) Enum() *Provider {
	p := new(Provider)
	*p = x
	return p
}

func (x Provider) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (Provider) Descriptor() protoreflect.EnumDescriptor {
	return file_ai_v1_model_proto_enumTypes[0].Descriptor()
}

func (Provider) Type() protoreflect.EnumType {
	return &file_ai_v1_model_proto_enumTypes[0]
}

func (x Provider) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use Provider.Descriptor instead.
func (Provider) EnumDescriptor() ([]byte, []int) {
	return file_ai_v1_model_proto_rawDescGZIP(), []int{0}
}

// Represents a model.
type ModelType int32

const (
	// Used to detect an unset field.
	ModelType_MODEL_TYPE_UNSPECIFIED ModelType = 0
	// Model can be used for STT.
	ModelType_MODEL_TYPE_STT ModelType = 1
	// Model can be used for TTT.
	ModelType_MODEL_TYPE_TTT ModelType = 2
	// Model can be used for TTS.
	ModelType_MODEL_TYPE_TTS ModelType = 3
)

// Enum value maps for ModelType.
var (
	ModelType_name = map[int32]string{
		0: "MODEL_TYPE_UNSPECIFIED",
		1: "MODEL_TYPE_STT",
		2: "MODEL_TYPE_TTT",
		3: "MODEL_TYPE_TTS",
	}
	ModelType_value = map[string]int32{
		"MODEL_TYPE_UNSPECIFIED": 0,
		"MODEL_TYPE_STT":         1,
		"MODEL_TYPE_TTT":         2,
		"MODEL_TYPE_TTS":         3,
	}
)

func (x ModelType) Enum() *ModelType {
	p := new(ModelType)
	*p = x
	return p
}

func (x ModelType) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (ModelType) Descriptor() protoreflect.EnumDescriptor {
	return file_ai_v1_model_proto_enumTypes[1].Descriptor()
}

func (ModelType) Type() protoreflect.EnumType {
	return &file_ai_v1_model_proto_enumTypes[1]
}

func (x ModelType) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use ModelType.Descriptor instead.
func (ModelType) EnumDescriptor() ([]byte, []int) {
	return file_ai_v1_model_proto_rawDescGZIP(), []int{1}
}

// Represents a model.
type Model int32

const (
	// Used to detect an unset field.
	Model_MODEL_UNSPECIFIED Model = 0
	// STT Models.
	// Whisper model.
	Model_MODEL_WHISPER_1 Model = 1
	// Whisper large.
	Model_MODEL_WHISPER_LARGE_V3_TURBO Model = 2
	// TTT Models.
	// GPT 4o.
	Model_MODEL_GPT_4O Model = 101
	// GPT 4o turbo.
	Model_MODEL_GPT_4_TURBO Model = 102
	// GPT 4o turbo pinned version.
	Model_MODEL_GPT_4_TURBO_2024_04_09 Model = 103
	// GPT 4.1.
	Model_MODEL_GPT_4_1 Model = 104
	// GPT 5.
	Model_MODEL_GPT_5 Model = 105
	// Sonnet 3.7.
	Model_MODEL_CLAUDE_SONNET_3_7 Model = 106
	// Sonnet 4.
	Model_MODEL_CLAUDE_SONNET_4 Model = 107
	// Gemini flash 2.0.
	Model_MODEL_GEMINI_FLASH_2 Model = 108
	// Gemini flash 2.5.
	Model_MODEL_GEMINI_FLASH_2_5 Model = 109
	// An open source model that performs well.
	Model_MODEL_KIMI_K2_INSTRUCT Model = 110
	// An open source model that performs well.
	Model_MODEL_QWEN3_32B Model = 111
	// TTS Models.
	// Fast & cheap but not that good.
	Model_MODEL_TTS_1 Model = 201
	// Need to try.
	Model_MODEL_GPT_4O_MINI_TTS Model = 202
	// Fast endpoint for eleven labs. (Currently used by Agents platform).
	Model_MODEL_ELEVEN_FLASH_2_5 Model = 203
	// Very fast model by groq. Not as good as eleven labs.
	Model_MODEL_PLAYAI_TTS Model = 204
	// Best-in-class for now.
	Model_MODEL_SONIC_3 Model = 205
)

// Enum value maps for Model.
var (
	Model_name = map[int32]string{
		0:   "MODEL_UNSPECIFIED",
		1:   "MODEL_WHISPER_1",
		2:   "MODEL_WHISPER_LARGE_V3_TURBO",
		101: "MODEL_GPT_4O",
		102: "MODEL_GPT_4_TURBO",
		103: "MODEL_GPT_4_TURBO_2024_04_09",
		104: "MODEL_GPT_4_1",
		105: "MODEL_GPT_5",
		106: "MODEL_CLAUDE_SONNET_3_7",
		107: "MODEL_CLAUDE_SONNET_4",
		108: "MODEL_GEMINI_FLASH_2",
		109: "MODEL_GEMINI_FLASH_2_5",
		110: "MODEL_KIMI_K2_INSTRUCT",
		111: "MODEL_QWEN3_32B",
		201: "MODEL_TTS_1",
		202: "MODEL_GPT_4O_MINI_TTS",
		203: "MODEL_ELEVEN_FLASH_2_5",
		204: "MODEL_PLAYAI_TTS",
		205: "MODEL_SONIC_3",
	}
	Model_value = map[string]int32{
		"MODEL_UNSPECIFIED":            0,
		"MODEL_WHISPER_1":              1,
		"MODEL_WHISPER_LARGE_V3_TURBO": 2,
		"MODEL_GPT_4O":                 101,
		"MODEL_GPT_4_TURBO":            102,
		"MODEL_GPT_4_TURBO_2024_04_09": 103,
		"MODEL_GPT_4_1":                104,
		"MODEL_GPT_5":                  105,
		"MODEL_CLAUDE_SONNET_3_7":      106,
		"MODEL_CLAUDE_SONNET_4":        107,
		"MODEL_GEMINI_FLASH_2":         108,
		"MODEL_GEMINI_FLASH_2_5":       109,
		"MODEL_KIMI_K2_INSTRUCT":       110,
		"MODEL_QWEN3_32B":              111,
		"MODEL_TTS_1":                  201,
		"MODEL_GPT_4O_MINI_TTS":        202,
		"MODEL_ELEVEN_FLASH_2_5":       203,
		"MODEL_PLAYAI_TTS":             204,
		"MODEL_SONIC_3":                205,
	}
)

func (x Model) Enum() *Model {
	p := new(Model)
	*p = x
	return p
}

func (x Model) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (Model) Descriptor() protoreflect.EnumDescriptor {
	return file_ai_v1_model_proto_enumTypes[2].Descriptor()
}

func (Model) Type() protoreflect.EnumType {
	return &file_ai_v1_model_proto_enumTypes[2]
}

func (x Model) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use Model.Descriptor instead.
func (Model) EnumDescriptor() ([]byte, []int) {
	return file_ai_v1_model_proto_rawDescGZIP(), []int{2}
}

// Represents the level of reasoning effort for AI model responses.
// The reasoning effort parameter guides the model on how many reasoning tokens
// to generate before creating a response to the prompt. Higher effort levels
// result in more thorough reasoning at the cost of speed and token usage.
type ReasoningEffort int32

const (
	// Used to detect an unset field.
	ReasoningEffort_REASONING_EFFORT_UNSPECIFIED ReasoningEffort = 0
	// Default reasoning effort set by platform.
	ReasoningEffort_REASONING_EFFORT_DEFAULT ReasoningEffort = 1
	// Low reasoning effort.
	// Favors speed and economical token usage with minimal reasoning tokens.
	ReasoningEffort_REASONING_EFFORT_LOW ReasoningEffort = 2
	// Medium reasoning effort (default).
	// Provides a balance between speed and reasoning accuracy.
	ReasoningEffort_REASONING_EFFORT_MEDIUM ReasoningEffort = 3
	// High reasoning effort.
	// Favors more complete and thorough reasoning, generating more reasoning
	// tokens before responding. May result in slower responses and higher
	// token usage.
	ReasoningEffort_REASONING_EFFORT_HIGH ReasoningEffort = 4
)

// Enum value maps for ReasoningEffort.
var (
	ReasoningEffort_name = map[int32]string{
		0: "REASONING_EFFORT_UNSPECIFIED",
		1: "REASONING_EFFORT_DEFAULT",
		2: "REASONING_EFFORT_LOW",
		3: "REASONING_EFFORT_MEDIUM",
		4: "REASONING_EFFORT_HIGH",
	}
	ReasoningEffort_value = map[string]int32{
		"REASONING_EFFORT_UNSPECIFIED": 0,
		"REASONING_EFFORT_DEFAULT":     1,
		"REASONING_EFFORT_LOW":         2,
		"REASONING_EFFORT_MEDIUM":      3,
		"REASONING_EFFORT_HIGH":        4,
	}
)

func (x ReasoningEffort) Enum() *ReasoningEffort {
	p := new(ReasoningEffort)
	*p = x
	return p
}

func (x ReasoningEffort) String() string {
	return protoimpl.X.EnumStringOf(x.Descriptor(), protoreflect.EnumNumber(x))
}

func (ReasoningEffort) Descriptor() protoreflect.EnumDescriptor {
	return file_ai_v1_model_proto_enumTypes[3].Descriptor()
}

func (ReasoningEffort) Type() protoreflect.EnumType {
	return &file_ai_v1_model_proto_enumTypes[3]
}

func (x ReasoningEffort) Number() protoreflect.EnumNumber {
	return protoreflect.EnumNumber(x)
}

// Deprecated: Use ReasoningEffort.Descriptor instead.
func (ReasoningEffort) EnumDescriptor() ([]byte, []int) {
	return file_ai_v1_model_proto_rawDescGZIP(), []int{3}
}

// Contains configuration for a  model.
type ModelConfig struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// The provider for this model.
	Provider Provider `protobuf:"varint,1,opt,name=provider,proto3,enum=malonaz.core.ai.v1.Provider" json:"provider,omitempty"`
	// Id of this model for this provider.
	ModelId string `protobuf:"bytes,2,opt,name=model_id,json=modelId,proto3" json:"model_id,omitempty"`
	// Indicates the type of a model.
	ModelType ModelType `protobuf:"varint,3,opt,name=model_type,json=modelType,proto3,enum=malonaz.core.ai.v1.ModelType" json:"model_type,omitempty"`
	// Configuration for TTT model.
	Ttt *TttModelConfig `protobuf:"bytes,4,opt,name=ttt,proto3" json:"ttt,omitempty"`
	// Configuration for TTS model.
	Tts           *TtsModelConfig `protobuf:"bytes,5,opt,name=tts,proto3" json:"tts,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ModelConfig) Reset() {
	*x = ModelConfig{}
	mi := &file_ai_v1_model_proto_msgTypes[0]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ModelConfig) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ModelConfig) ProtoMessage() {}

func (x *ModelConfig) ProtoReflect() protoreflect.Message {
	mi := &file_ai_v1_model_proto_msgTypes[0]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ModelConfig.ProtoReflect.Descriptor instead.
func (*ModelConfig) Descriptor() ([]byte, []int) {
	return file_ai_v1_model_proto_rawDescGZIP(), []int{0}
}

func (x *ModelConfig) GetProvider() Provider {
	if x != nil {
		return x.Provider
	}
	return Provider_PROVIDER_UNSPECIFIED
}

func (x *ModelConfig) GetModelId() string {
	if x != nil {
		return x.ModelId
	}
	return ""
}

func (x *ModelConfig) GetModelType() ModelType {
	if x != nil {
		return x.ModelType
	}
	return ModelType_MODEL_TYPE_UNSPECIFIED
}

func (x *ModelConfig) GetTtt() *TttModelConfig {
	if x != nil {
		return x.Ttt
	}
	return nil
}

func (x *ModelConfig) GetTts() *TtsModelConfig {
	if x != nil {
		return x.Tts
	}
	return nil
}

// Configuration for a ttt model.
type TttModelConfig struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// True if the model supports reasoning.
	Reasoning bool `protobuf:"varint,1,opt,name=reasoning,proto3" json:"reasoning,omitempty"`
	// True if the model support tool calling.
	ToolCall      bool `protobuf:"varint,2,opt,name=tool_call,json=toolCall,proto3" json:"tool_call,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *TttModelConfig) Reset() {
	*x = TttModelConfig{}
	mi := &file_ai_v1_model_proto_msgTypes[1]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TttModelConfig) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TttModelConfig) ProtoMessage() {}

func (x *TttModelConfig) ProtoReflect() protoreflect.Message {
	mi := &file_ai_v1_model_proto_msgTypes[1]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use TttModelConfig.ProtoReflect.Descriptor instead.
func (*TttModelConfig) Descriptor() ([]byte, []int) {
	return file_ai_v1_model_proto_rawDescGZIP(), []int{1}
}

func (x *TttModelConfig) GetReasoning() bool {
	if x != nil {
		return x.Reasoning
	}
	return false
}

func (x *TttModelConfig) GetToolCall() bool {
	if x != nil {
		return x.ToolCall
	}
	return false
}

// Configuration for a tts model.
type TtsModelConfig struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// Audio format of the output of this model.
	AudioFormat   *v1.Format `protobuf:"bytes,1,opt,name=audio_format,json=audioFormat,proto3" json:"audio_format,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *TtsModelConfig) Reset() {
	*x = TtsModelConfig{}
	mi := &file_ai_v1_model_proto_msgTypes[2]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TtsModelConfig) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TtsModelConfig) ProtoMessage() {}

func (x *TtsModelConfig) ProtoReflect() protoreflect.Message {
	mi := &file_ai_v1_model_proto_msgTypes[2]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use TtsModelConfig.ProtoReflect.Descriptor instead.
func (*TtsModelConfig) Descriptor() ([]byte, []int) {
	return file_ai_v1_model_proto_rawDescGZIP(), []int{2}
}

func (x *TtsModelConfig) GetAudioFormat() *v1.Format {
	if x != nil {
		return x.AudioFormat
	}
	return nil
}

var file_ai_v1_model_proto_extTypes = []protoimpl.ExtensionInfo{
	{
		ExtendedType:  (*descriptorpb.EnumValueOptions)(nil),
		ExtensionType: (*ModelConfig)(nil),
		Field:         6544,
		Name:          "malonaz.core.ai.v1.config",
		Tag:           "bytes,6544,opt,name=config",
		Filename:      "ai/v1/model.proto",
	},
}

// Extension fields to descriptorpb.EnumValueOptions.
var (
	// Config for a model.
	//
	// optional malonaz.core.ai.v1.ModelConfig config = 6544;
	E_Config = &file_ai_v1_model_proto_extTypes[0]
)

var File_ai_v1_model_proto protoreflect.FileDescriptor

const file_ai_v1_model_proto_rawDesc = "" +
	"\n" +
	"\x11ai/v1/model.proto\x12\x12malonaz.core.ai.v1\x1a\x14audio/v1/audio.proto\x1a google/protobuf/descriptor.proto\"\x8c\x02\n" +
	"\vModelConfig\x128\n" +
	"\bprovider\x18\x01 \x01(\x0e2\x1c.malonaz.core.ai.v1.ProviderR\bprovider\x12\x19\n" +
	"\bmodel_id\x18\x02 \x01(\tR\amodelId\x12<\n" +
	"\n" +
	"model_type\x18\x03 \x01(\x0e2\x1d.malonaz.core.ai.v1.ModelTypeR\tmodelType\x124\n" +
	"\x03ttt\x18\x04 \x01(\v2\".malonaz.core.ai.v1.TttModelConfigR\x03ttt\x124\n" +
	"\x03tts\x18\x05 \x01(\v2\".malonaz.core.ai.v1.TtsModelConfigR\x03tts\"K\n" +
	"\x0eTttModelConfig\x12\x1c\n" +
	"\treasoning\x18\x01 \x01(\bR\treasoning\x12\x1b\n" +
	"\ttool_call\x18\x02 \x01(\bR\btoolCall\"R\n" +
	"\x0eTtsModelConfig\x12@\n" +
	"\faudio_format\x18\x01 \x01(\v2\x1d.malonaz.core.audio.v1.FormatR\vaudioFormat*\xa9\x01\n" +
	"\bProvider\x12\x18\n" +
	"\x14PROVIDER_UNSPECIFIED\x10\x00\x12\x13\n" +
	"\x0fPROVIDER_OPENAI\x10\x01\x12\x16\n" +
	"\x12PROVIDER_ANTHROPIC\x10\x02\x12\x17\n" +
	"\x13PROVIDER_ELEVENLABS\x10\x03\x12\x11\n" +
	"\rPROVIDER_GROQ\x10\x04\x12\x13\n" +
	"\x0fPROVIDER_GOOGLE\x10\x05\x12\x15\n" +
	"\x11PROVIDER_CARTESIA\x10\x06*c\n" +
	"\tModelType\x12\x1a\n" +
	"\x16MODEL_TYPE_UNSPECIFIED\x10\x00\x12\x12\n" +
	"\x0eMODEL_TYPE_STT\x10\x01\x12\x12\n" +
	"\x0eMODEL_TYPE_TTT\x10\x02\x12\x12\n" +
	"\x0eMODEL_TYPE_TTS\x10\x03*\xa0\b\n" +
	"\x05Model\x12\x15\n" +
	"\x11MODEL_UNSPECIFIED\x10\x00\x12(\n" +
	"\x0fMODEL_WHISPER_1\x10\x01\x1a\x13\x82\x99\x03\x0f\b\x01\x12\twhisper-1\x18\x01\x12B\n" +
	"\x1cMODEL_WHISPER_LARGE_V3_TURBO\x10\x02\x1a \x82\x99\x03\x1c\b\x04\x12\x16whisper-large-v3-turbo\x18\x01\x12&\n" +
	"\fMODEL_GPT_4O\x10e\x1a\x14\x82\x99\x03\x10\b\x01\x12\x06gpt-4o\x18\x02\"\x02\x10\x01\x120\n" +
	"\x11MODEL_GPT_4_TURBO\x10f\x1a\x19\x82\x99\x03\x15\b\x01\x12\vgpt-4-turbo\x18\x02\"\x02\x10\x01\x12F\n" +
	"\x1cMODEL_GPT_4_TURBO_2024_04_09\x10g\x1a$\x82\x99\x03 \b\x01\x12\x16gpt-4-turbo-2024-04-09\x18\x02\"\x02\x10\x01\x12(\n" +
	"\rMODEL_GPT_4_1\x10h\x1a\x15\x82\x99\x03\x11\b\x01\x12\agpt-4.1\x18\x02\"\x02\x10\x01\x12&\n" +
	"\vMODEL_GPT_5\x10i\x1a\x15\x82\x99\x03\x11\b\x01\x12\x05gpt-5\x18\x02\"\x04\b\x01\x10\x01\x12G\n" +
	"\x17MODEL_CLAUDE_SONNET_3_7\x10j\x1a*\x82\x99\x03&\b\x02\x12\x1aclaude-3-7-sonnet-20250219\x18\x02\"\x04\b\x01\x10\x01\x12:\n" +
	"\x15MODEL_CLAUDE_SONNET_4\x10k\x1a\x1f\x82\x99\x03\x1b\b\x02\x12\x0fclaude-sonnet-4\x18\x02\"\x04\b\x01\x10\x01\x128\n" +
	"\x14MODEL_GEMINI_FLASH_2\x10l\x1a\x1e\x82\x99\x03\x1a\b\x05\x12\x0egemini-flash-2\x18\x02\"\x04\b\x01\x10\x01\x12<\n" +
	"\x16MODEL_GEMINI_FLASH_2_5\x10m\x1a \x82\x99\x03\x1c\b\x05\x12\x10gemini-flash-2.5\x18\x02\"\x04\b\x01\x10\x01\x12J\n" +
	"\x16MODEL_KIMI_K2_INSTRUCT\x10n\x1a.\x82\x99\x03*\b\x04\x12 moonshotai/kimi-k2-instruct-0905\x18\x02\"\x02\x10\x01\x123\n" +
	"\x0fMODEL_QWEN3_32B\x10o\x1a\x1e\x82\x99\x03\x1a\b\x04\x12\x0eqwen/qwen3-32b\x18\x02\"\x04\b\x01\x10\x01\x12-\n" +
	"\vMODEL_TTS_1\x10\xc9\x01\x1a\x1b\x82\x99\x03\x17\b\x01\x12\x05tts-1\x18\x03*\n" +
	"\n" +
	"\b\b\xc0\xbb\x01\x10\x01\x18\x10\x12A\n" +
	"\x15MODEL_GPT_4O_MINI_TTS\x10\xca\x01\x1a%\x82\x99\x03!\b\x01\x12\x0fgpt-4o-mini-tts\x18\x03*\n" +
	"\n" +
	"\b\b\xc0\xbb\x01\x10\x01\x18\x10\x12C\n" +
	"\x16MODEL_ELEVEN_FLASH_2_5\x10\xcb\x01\x1a&\x82\x99\x03\"\b\x03\x12\x11eleven_flash_v2_5\x18\x03*\t\n" +
	"\a\b\x80}\x10\x01\x18\x10\x127\n" +
	"\x10MODEL_PLAYAI_TTS\x10\xcc\x01\x1a \x82\x99\x03\x1c\b\x04\x12\n" +
	"playai-tts\x18\x03*\n" +
	"\n" +
	"\b\b\x80\xf7\x02\x10\x01\x18\x10\x120\n" +
	"\rMODEL_SONIC_3\x10\xcd\x01\x1a\x1c\x82\x99\x03\x18\b\x06\x12\asonic-3\x18\x03*\t\n" +
	"\a\b\xc0>\x10\x01\x18\x10*\xa3\x01\n" +
	"\x0fReasoningEffort\x12 \n" +
	"\x1cREASONING_EFFORT_UNSPECIFIED\x10\x00\x12\x1c\n" +
	"\x18REASONING_EFFORT_DEFAULT\x10\x01\x12\x18\n" +
	"\x14REASONING_EFFORT_LOW\x10\x02\x12\x1b\n" +
	"\x17REASONING_EFFORT_MEDIUM\x10\x03\x12\x19\n" +
	"\x15REASONING_EFFORT_HIGH\x10\x04:[\n" +
	"\x06config\x12!.google.protobuf.EnumValueOptions\x18\x903 \x01(\v2\x1f.malonaz.core.ai.v1.ModelConfigR\x06configB(Z&github.com/malonaz/core/genproto/ai/v1b\x06proto3"

var (
	file_ai_v1_model_proto_rawDescOnce sync.Once
	file_ai_v1_model_proto_rawDescData []byte
)

func file_ai_v1_model_proto_rawDescGZIP() []byte {
	file_ai_v1_model_proto_rawDescOnce.Do(func() {
		file_ai_v1_model_proto_rawDescData = protoimpl.X.CompressGZIP(unsafe.Slice(unsafe.StringData(file_ai_v1_model_proto_rawDesc), len(file_ai_v1_model_proto_rawDesc)))
	})
	return file_ai_v1_model_proto_rawDescData
}

var file_ai_v1_model_proto_enumTypes = make([]protoimpl.EnumInfo, 4)
var file_ai_v1_model_proto_msgTypes = make([]protoimpl.MessageInfo, 3)
var file_ai_v1_model_proto_goTypes = []any{
	(Provider)(0),                         // 0: malonaz.core.ai.v1.Provider
	(ModelType)(0),                        // 1: malonaz.core.ai.v1.ModelType
	(Model)(0),                            // 2: malonaz.core.ai.v1.Model
	(ReasoningEffort)(0),                  // 3: malonaz.core.ai.v1.ReasoningEffort
	(*ModelConfig)(nil),                   // 4: malonaz.core.ai.v1.ModelConfig
	(*TttModelConfig)(nil),                // 5: malonaz.core.ai.v1.TttModelConfig
	(*TtsModelConfig)(nil),                // 6: malonaz.core.ai.v1.TtsModelConfig
	(*v1.Format)(nil),                     // 7: malonaz.core.audio.v1.Format
	(*descriptorpb.EnumValueOptions)(nil), // 8: google.protobuf.EnumValueOptions
}
var file_ai_v1_model_proto_depIdxs = []int32{
	0, // 0: malonaz.core.ai.v1.ModelConfig.provider:type_name -> malonaz.core.ai.v1.Provider
	1, // 1: malonaz.core.ai.v1.ModelConfig.model_type:type_name -> malonaz.core.ai.v1.ModelType
	5, // 2: malonaz.core.ai.v1.ModelConfig.ttt:type_name -> malonaz.core.ai.v1.TttModelConfig
	6, // 3: malonaz.core.ai.v1.ModelConfig.tts:type_name -> malonaz.core.ai.v1.TtsModelConfig
	7, // 4: malonaz.core.ai.v1.TtsModelConfig.audio_format:type_name -> malonaz.core.audio.v1.Format
	8, // 5: malonaz.core.ai.v1.config:extendee -> google.protobuf.EnumValueOptions
	4, // 6: malonaz.core.ai.v1.config:type_name -> malonaz.core.ai.v1.ModelConfig
	7, // [7:7] is the sub-list for method output_type
	7, // [7:7] is the sub-list for method input_type
	6, // [6:7] is the sub-list for extension type_name
	5, // [5:6] is the sub-list for extension extendee
	0, // [0:5] is the sub-list for field type_name
}

func init() { file_ai_v1_model_proto_init() }
func file_ai_v1_model_proto_init() {
	if File_ai_v1_model_proto != nil {
		return
	}
	type x struct{}
	out := protoimpl.TypeBuilder{
		File: protoimpl.DescBuilder{
			GoPackagePath: reflect.TypeOf(x{}).PkgPath(),
			RawDescriptor: unsafe.Slice(unsafe.StringData(file_ai_v1_model_proto_rawDesc), len(file_ai_v1_model_proto_rawDesc)),
			NumEnums:      4,
			NumMessages:   3,
			NumExtensions: 1,
			NumServices:   0,
		},
		GoTypes:           file_ai_v1_model_proto_goTypes,
		DependencyIndexes: file_ai_v1_model_proto_depIdxs,
		EnumInfos:         file_ai_v1_model_proto_enumTypes,
		MessageInfos:      file_ai_v1_model_proto_msgTypes,
		ExtensionInfos:    file_ai_v1_model_proto_extTypes,
	}.Build()
	File_ai_v1_model_proto = out.File
	file_ai_v1_model_proto_goTypes = nil
	file_ai_v1_model_proto_depIdxs = nil
}
